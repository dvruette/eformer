nav:
  - Bit Quantization:
    - Calibration: generated-bit_quantization-calibration.md
    - Config: generated-bit_quantization-config.md
    - Int Numerics: generated-bit_quantization-int_numerics.md
    - No Numerics: generated-bit_quantization-no_numerics.md
    - Numerics: generated-bit_quantization-numerics.md
    - Q Dot General: generated-bit_quantization-q_dot_general.md
    - Q Flax: generated-bit_quantization-q_flax.md
    - Stochastic Rounding: generated-bit_quantization-stochastic_rounding.md
  - Checkpoint:
    - Load: generated-checkpoint-_load.md
    - Streamer: generated-checkpoint-streamer.md
  - Functions:
    - Func: generated-functions-_func.md
    - Loss Func: generated-functions-loss_func.md
  - Home: index.md
  - Linen:
    - Linen: generated-linen-linen.md
  - Monitor:
    - Tracker: generated-monitor-tracker.md
  - Optimizers:
    - Adafactor: generated-optimizers-adafactor.md
    - Adamw: generated-optimizers-adamw.md
    - Lion: generated-optimizers-lion.md
    - Optimizer Utils: generated-optimizers-optimizer_utils.md
    - Rmsprop: generated-optimizers-rmsprop.md
  - Pallas Operations:
    - Efficient Attention:
      - Efficient Attention: generated-pallas_operations-efficient_attention-efficient_attention.md
    - Gpu:
      - Flash Attention:
        - Mha: generated-pallas_operations-gpu-flash_attention-mha.md
      - Layer Norm:
        - Layer Norm: generated-pallas_operations-gpu-layer_norm-layer_norm.md
      - Rms Norm:
        - Rms Norm: generated-pallas_operations-gpu-rms_norm-rms_norm.md
      - Softmax:
        - Softmax: generated-pallas_operations-gpu-softmax-softmax.md
    - Pallas Attention:
      - Attention: generated-pallas_operations-pallas_attention-attention.md
    - Tpu:
      - Flash Attention:
        - Flash Attention: generated-pallas_operations-tpu-flash_attention-flash_attention.md
      - Paged Attention:
        - Paged Attention: generated-pallas_operations-tpu-paged_attention-paged_attention.md
      - Ring Attention:
        - Ring Attention: generated-pallas_operations-tpu-ring_attention-ring_attention.md
      - Splash Attention:
        - Splash Attention Kernel: generated-pallas_operations-tpu-splash_attention-splash_attention_kernel.md
        - Splash Attention Mask: generated-pallas_operations-tpu-splash_attention-splash_attention_mask.md
        - Splash Attention Mask Info: generated-pallas_operations-tpu-splash_attention-splash_attention_mask_info.md
  - Sharding:
    - Sharding: generated-sharding-sharding.md
    - T5x Partitioning: generated-sharding-t5x_partitioning.md
  - Utils: generated-utils.md
  - Xrapture:
    - Implicit Array: generated-xrapture-implicit_array.md
    - Tracer: generated-xrapture-tracer.md
    - Xrapture: generated-xrapture-xrapture.md

plugins:
  - search
  - mkdocstrings:
      handlers:
        python:
          options:
            docstring_style: sphinx

repo_url: https://github.com/erfanzar/FJFormer
site_author: Erfan Zare Chavoshi
site_name: FJFormer
copyright: Erfan Zare Chavoshi-FJFormer

theme:
  highlightjs: true
  hljs_languages:
    - yaml
    - python
  name: material
