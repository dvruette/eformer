[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "abc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "abc",
        "description": "abc",
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "ABC",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "typing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "typing",
        "description": "typing",
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "ClassVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Mapping",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Mapping",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Hashable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Protocol",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generic",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Literal",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "overload",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "NamedTuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequence",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Set",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "flax.struct",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "flax.struct",
        "description": "flax.struct",
        "detail": "flax.struct",
        "documentation": {}
    },
    {
        "label": "jax.numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jax.numpy",
        "description": "jax.numpy",
        "detail": "jax.numpy",
        "documentation": {}
    },
    {
        "label": "dataclasses",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "dataclasses",
        "description": "dataclasses",
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "fields",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "is_dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "calibration",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "stochastic_rounding",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "int_numerics",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "no_numerics",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "numerics",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "stochastic_rounding",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "numerics",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "stochastic_rounding",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "numerics",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "no_numerics",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "q_dot_general",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "calibration",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "config",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "int_numerics",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "no_numerics",
        "importPath": "fjformer.bit_quantization",
        "description": "fjformer.bit_quantization",
        "isExtraImport": true,
        "detail": "fjformer.bit_quantization",
        "documentation": {}
    },
    {
        "label": "jax",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jax",
        "description": "jax",
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "lax",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "lax",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "numpy",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "numpy",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "core",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "tree_util",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "lax",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "numpy",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "tree_util",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "numpy",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "Array",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "numpy",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "eval_shape",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "lax",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "numpy",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "numpy",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "lax",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "lax",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "lax",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "lax",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "lax",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "lax",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "ad_checkpoint",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "lax",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "tree_util",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "util",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "numpy",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "random",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "random",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "random",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "numpy",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "numpy",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "numpy",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "Array",
        "importPath": "jax",
        "description": "jax",
        "isExtraImport": true,
        "detail": "jax",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "wraps",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "wraps",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "reduce",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "enum",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "enum",
        "description": "enum",
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "flax",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "flax",
        "description": "flax",
        "detail": "flax",
        "documentation": {}
    },
    {
        "label": "struct",
        "importPath": "flax",
        "description": "flax",
        "isExtraImport": true,
        "detail": "flax",
        "documentation": {}
    },
    {
        "label": "traverse_util",
        "importPath": "flax",
        "description": "flax",
        "isExtraImport": true,
        "detail": "flax",
        "documentation": {}
    },
    {
        "label": "flax.linen",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "flax.linen",
        "description": "flax.linen",
        "detail": "flax.linen",
        "documentation": {}
    },
    {
        "label": "initializers",
        "importPath": "flax.linen",
        "description": "flax.linen",
        "isExtraImport": true,
        "detail": "flax.linen",
        "documentation": {}
    },
    {
        "label": "dtypes",
        "importPath": "flax.linen",
        "description": "flax.linen",
        "isExtraImport": true,
        "detail": "flax.linen",
        "documentation": {}
    },
    {
        "label": "module",
        "importPath": "flax.linen",
        "description": "flax.linen",
        "isExtraImport": true,
        "detail": "flax.linen",
        "documentation": {}
    },
    {
        "label": "transforms",
        "importPath": "flax.linen",
        "description": "flax.linen",
        "isExtraImport": true,
        "detail": "flax.linen",
        "documentation": {}
    },
    {
        "label": "partitioning",
        "importPath": "flax.linen",
        "description": "flax.linen",
        "isExtraImport": true,
        "detail": "flax.linen",
        "documentation": {}
    },
    {
        "label": "lax_numpy",
        "importPath": "jax._src.numpy",
        "description": "jax._src.numpy",
        "isExtraImport": true,
        "detail": "jax._src.numpy",
        "documentation": {}
    },
    {
        "label": "msgpack",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "msgpack",
        "description": "msgpack",
        "detail": "msgpack",
        "documentation": {}
    },
    {
        "label": "from_bytes",
        "importPath": "flax.serialization",
        "description": "flax.serialization",
        "isExtraImport": true,
        "detail": "flax.serialization",
        "documentation": {}
    },
    {
        "label": "to_bytes",
        "importPath": "flax.serialization",
        "description": "flax.serialization",
        "isExtraImport": true,
        "detail": "flax.serialization",
        "documentation": {}
    },
    {
        "label": "to_state_dict",
        "importPath": "flax.serialization",
        "description": "flax.serialization",
        "isExtraImport": true,
        "detail": "flax.serialization",
        "documentation": {}
    },
    {
        "label": "from_bytes",
        "importPath": "flax.serialization",
        "description": "flax.serialization",
        "isExtraImport": true,
        "detail": "flax.serialization",
        "documentation": {}
    },
    {
        "label": "to_bytes",
        "importPath": "flax.serialization",
        "description": "flax.serialization",
        "isExtraImport": true,
        "detail": "flax.serialization",
        "documentation": {}
    },
    {
        "label": "to_state_dict",
        "importPath": "flax.serialization",
        "description": "flax.serialization",
        "isExtraImport": true,
        "detail": "flax.serialization",
        "documentation": {}
    },
    {
        "label": "from_state_dict",
        "importPath": "flax.serialization",
        "description": "flax.serialization",
        "isExtraImport": true,
        "detail": "flax.serialization",
        "documentation": {}
    },
    {
        "label": "flax.traverse_util",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "flax.traverse_util",
        "description": "flax.traverse_util",
        "detail": "flax.traverse_util",
        "documentation": {}
    },
    {
        "label": "flatten_dict",
        "importPath": "flax.traverse_util",
        "description": "flax.traverse_util",
        "isExtraImport": true,
        "detail": "flax.traverse_util",
        "documentation": {}
    },
    {
        "label": "flatten_dict",
        "importPath": "flax.traverse_util",
        "description": "flax.traverse_util",
        "isExtraImport": true,
        "detail": "flax.traverse_util",
        "documentation": {}
    },
    {
        "label": "unflatten_dict",
        "importPath": "flax.traverse_util",
        "description": "flax.traverse_util",
        "isExtraImport": true,
        "detail": "flax.traverse_util",
        "documentation": {}
    },
    {
        "label": "empty_node",
        "importPath": "flax.traverse_util",
        "description": "flax.traverse_util",
        "isExtraImport": true,
        "detail": "flax.traverse_util",
        "documentation": {}
    },
    {
        "label": "CheckpointManager",
        "importPath": "fjformer.checkpoint.streamer",
        "description": "fjformer.checkpoint.streamer",
        "isExtraImport": true,
        "detail": "fjformer.checkpoint.streamer",
        "documentation": {}
    },
    {
        "label": "safetensors.flax",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "safetensors.flax",
        "description": "safetensors.flax",
        "detail": "safetensors.flax",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "ContextVar",
        "importPath": "contextvars",
        "description": "contextvars",
        "isExtraImport": true,
        "detail": "contextvars",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "count",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "UninitializedAval",
        "importPath": "fjformer.core.errors",
        "description": "fjformer.core.errors",
        "isExtraImport": true,
        "detail": "fjformer.core.errors",
        "documentation": {}
    },
    {
        "label": "MaterializationError",
        "importPath": "fjformer.core.errors",
        "description": "fjformer.core.errors",
        "isExtraImport": true,
        "detail": "fjformer.core.errors",
        "documentation": {}
    },
    {
        "label": "OperationError",
        "importPath": "fjformer.core.errors",
        "description": "fjformer.core.errors",
        "isExtraImport": true,
        "detail": "fjformer.core.errors",
        "documentation": {}
    },
    {
        "label": "ShapeDtypeError",
        "importPath": "fjformer.core.errors",
        "description": "fjformer.core.errors",
        "isExtraImport": true,
        "detail": "fjformer.core.errors",
        "documentation": {}
    },
    {
        "label": "UnsupportedPrimitiveError",
        "importPath": "fjformer.core.errors",
        "description": "fjformer.core.errors",
        "isExtraImport": true,
        "detail": "fjformer.core.errors",
        "documentation": {}
    },
    {
        "label": "jax.extend.linear_util",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jax.extend.linear_util",
        "description": "jax.extend.linear_util",
        "detail": "jax.extend.linear_util",
        "documentation": {}
    },
    {
        "label": "jax.interpreters.partial_eval",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jax.interpreters.partial_eval",
        "description": "jax.interpreters.partial_eval",
        "detail": "jax.interpreters.partial_eval",
        "documentation": {}
    },
    {
        "label": "DTypeLike",
        "importPath": "jax._src.typing",
        "description": "jax._src.typing",
        "isExtraImport": true,
        "detail": "jax._src.typing",
        "documentation": {}
    },
    {
        "label": "Shape",
        "importPath": "jax._src.typing",
        "description": "jax._src.typing",
        "isExtraImport": true,
        "detail": "jax._src.typing",
        "documentation": {}
    },
    {
        "label": "flatten_fun",
        "importPath": "jax.api_util",
        "description": "jax.api_util",
        "isExtraImport": true,
        "detail": "jax.api_util",
        "documentation": {}
    },
    {
        "label": "flatten_fun_nokwargs",
        "importPath": "jax.api_util",
        "description": "jax.api_util",
        "isExtraImport": true,
        "detail": "jax.api_util",
        "documentation": {}
    },
    {
        "label": "jax.tree_util",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jax.tree_util",
        "description": "jax.tree_util",
        "detail": "jax.tree_util",
        "documentation": {}
    },
    {
        "label": "register_pytree_with_keys_class",
        "importPath": "jax.tree_util",
        "description": "jax.tree_util",
        "isExtraImport": true,
        "detail": "jax.tree_util",
        "documentation": {}
    },
    {
        "label": "Dispatcher",
        "importPath": "plum",
        "description": "plum",
        "isExtraImport": true,
        "detail": "plum",
        "documentation": {}
    },
    {
        "label": "Function",
        "importPath": "plum",
        "description": "plum",
        "isExtraImport": true,
        "detail": "plum",
        "documentation": {}
    },
    {
        "label": "dispatch",
        "importPath": "plum",
        "description": "plum",
        "isExtraImport": true,
        "detail": "plum",
        "documentation": {}
    },
    {
        "label": "parametric",
        "importPath": "plum",
        "description": "plum",
        "isExtraImport": true,
        "detail": "plum",
        "documentation": {}
    },
    {
        "label": "ELEMENTWISE_BINOPS",
        "importPath": "fjformer.core.implicit_array",
        "description": "fjformer.core.implicit_array",
        "isExtraImport": true,
        "detail": "fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "ELEMENTWISE_UNOPS",
        "importPath": "fjformer.core.implicit_array",
        "description": "fjformer.core.implicit_array",
        "isExtraImport": true,
        "detail": "fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "ArrayValue",
        "importPath": "fjformer.core.implicit_array",
        "description": "fjformer.core.implicit_array",
        "isExtraImport": true,
        "detail": "fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "ImplicitArray",
        "importPath": "fjformer.core.implicit_array",
        "description": "fjformer.core.implicit_array",
        "isExtraImport": true,
        "detail": "fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "aux_field",
        "importPath": "fjformer.core.implicit_array",
        "description": "fjformer.core.implicit_array",
        "isExtraImport": true,
        "detail": "fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "default_handler",
        "importPath": "fjformer.core.implicit_array",
        "description": "fjformer.core.implicit_array",
        "isExtraImport": true,
        "detail": "fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "primitive_handler",
        "importPath": "fjformer.core.implicit_array",
        "description": "fjformer.core.implicit_array",
        "isExtraImport": true,
        "detail": "fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "use_implicit_args",
        "importPath": "fjformer.core.implicit_array",
        "description": "fjformer.core.implicit_array",
        "isExtraImport": true,
        "detail": "fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "use_implicit_args",
        "importPath": "fjformer.core.implicit_array",
        "description": "fjformer.core.implicit_array",
        "isExtraImport": true,
        "detail": "fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "Complement",
        "importPath": "fjformer.core.types",
        "description": "fjformer.core.types",
        "isExtraImport": true,
        "detail": "fjformer.core.types",
        "documentation": {}
    },
    {
        "label": "optax",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "optax",
        "description": "optax",
        "detail": "optax",
        "documentation": {}
    },
    {
        "label": "softmax_cross_entropy",
        "importPath": "optax",
        "description": "optax",
        "isExtraImport": true,
        "detail": "optax",
        "documentation": {}
    },
    {
        "label": "float0",
        "importPath": "jax.dtypes",
        "description": "jax.dtypes",
        "isExtraImport": true,
        "detail": "jax.dtypes",
        "documentation": {}
    },
    {
        "label": "SymbolicConstant",
        "importPath": "fjformer.core.symbols",
        "description": "fjformer.core.symbols",
        "isExtraImport": true,
        "detail": "fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "chex",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "chex",
        "description": "chex",
        "detail": "chex",
        "documentation": {}
    },
    {
        "label": "ravel_pytree",
        "importPath": "jax._src.flatten_util",
        "description": "jax._src.flatten_util",
        "isExtraImport": true,
        "detail": "jax._src.flatten_util",
        "documentation": {}
    },
    {
        "label": "mul",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "common_utils",
        "importPath": "flax.training",
        "description": "flax.training",
        "isExtraImport": true,
        "detail": "flax.training",
        "documentation": {}
    },
    {
        "label": "logsumexp",
        "importPath": "jax.scipy.special",
        "description": "jax.scipy.special",
        "isExtraImport": true,
        "detail": "jax.scipy.special",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "flax.core",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "flax.core",
        "description": "flax.core",
        "detail": "flax.core",
        "documentation": {}
    },
    {
        "label": "FrozenDict",
        "importPath": "flax.core",
        "description": "flax.core",
        "isExtraImport": true,
        "detail": "flax.core",
        "documentation": {}
    },
    {
        "label": "promote_dtype",
        "importPath": "flax.linen.dtypes",
        "description": "flax.linen.dtypes",
        "isExtraImport": true,
        "detail": "flax.linen.dtypes",
        "documentation": {}
    },
    {
        "label": "compact",
        "importPath": "flax.linen.module",
        "description": "flax.linen.module",
        "isExtraImport": true,
        "detail": "flax.linen.module",
        "documentation": {}
    },
    {
        "label": "Module",
        "importPath": "flax.linen.module",
        "description": "flax.linen.module",
        "isExtraImport": true,
        "detail": "flax.linen.module",
        "documentation": {}
    },
    {
        "label": "ShapedArray",
        "importPath": "jax.core",
        "description": "jax.core",
        "isExtraImport": true,
        "detail": "jax.core",
        "documentation": {}
    },
    {
        "label": "core",
        "importPath": "fjformer",
        "description": "fjformer",
        "isExtraImport": true,
        "detail": "fjformer",
        "documentation": {}
    },
    {
        "label": "EmptyNode",
        "importPath": "fjformer.core",
        "description": "fjformer.core",
        "isExtraImport": true,
        "detail": "fjformer.core",
        "documentation": {}
    },
    {
        "label": "materialize_nested",
        "importPath": "fjformer.core",
        "description": "fjformer.core",
        "isExtraImport": true,
        "detail": "fjformer.core",
        "documentation": {}
    },
    {
        "label": "tree_map_with_implicit",
        "importPath": "fjformer.core",
        "description": "fjformer.core",
        "isExtraImport": true,
        "detail": "fjformer.core",
        "documentation": {}
    },
    {
        "label": "use_implicit_args",
        "importPath": "fjformer.core",
        "description": "fjformer.core",
        "isExtraImport": true,
        "detail": "fjformer.core",
        "documentation": {}
    },
    {
        "label": "ImplicitArray",
        "importPath": "fjformer.core",
        "description": "fjformer.core",
        "isExtraImport": true,
        "detail": "fjformer.core",
        "documentation": {}
    },
    {
        "label": "primitive_handler",
        "importPath": "fjformer.core",
        "description": "fjformer.core",
        "isExtraImport": true,
        "detail": "fjformer.core",
        "documentation": {}
    },
    {
        "label": "use_implicit_args",
        "importPath": "fjformer.core",
        "description": "fjformer.core",
        "isExtraImport": true,
        "detail": "fjformer.core",
        "documentation": {}
    },
    {
        "label": "ArrayValue",
        "importPath": "fjformer.core",
        "description": "fjformer.core",
        "isExtraImport": true,
        "detail": "fjformer.core",
        "documentation": {}
    },
    {
        "label": "freeze_subtrees",
        "importPath": "fjformer.core.utilities",
        "description": "fjformer.core.utilities",
        "isExtraImport": true,
        "detail": "fjformer.core.utilities",
        "documentation": {}
    },
    {
        "label": "freeze_keys",
        "importPath": "fjformer.core.utilities",
        "description": "fjformer.core.utilities",
        "isExtraImport": true,
        "detail": "fjformer.core.utilities",
        "documentation": {}
    },
    {
        "label": "LoraWeight",
        "importPath": "fjformer.lora.lora_core",
        "description": "fjformer.lora.lora_core",
        "isExtraImport": true,
        "detail": "fjformer.lora.lora_core",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "importPath": "fjformer.utils",
        "description": "fjformer.utils",
        "isExtraImport": true,
        "detail": "fjformer.utils",
        "documentation": {}
    },
    {
        "label": "curses",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "curses",
        "description": "curses",
        "detail": "curses",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "threading",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "threading",
        "description": "threading",
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "IPython.display",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "IPython.display",
        "description": "IPython.display",
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "jax.profiler",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jax.profiler",
        "description": "jax.profiler",
        "detail": "jax.profiler",
        "documentation": {}
    },
    {
        "label": "optax_add_scheduled_weight_decay",
        "importPath": "fjformer.optimizers.optimizer_utils",
        "description": "fjformer.optimizers.optimizer_utils",
        "isExtraImport": true,
        "detail": "fjformer.optimizers.optimizer_utils",
        "documentation": {}
    },
    {
        "label": "jax.lax",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jax.lax",
        "description": "jax.lax",
        "detail": "jax.lax",
        "documentation": {}
    },
    {
        "label": "with_sharding_constraint",
        "importPath": "jax.lax",
        "description": "jax.lax",
        "isExtraImport": true,
        "detail": "jax.lax",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "annotations",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "pallas",
        "importPath": "jax.experimental",
        "description": "jax.experimental",
        "isExtraImport": true,
        "detail": "jax.experimental",
        "documentation": {}
    },
    {
        "label": "pallas",
        "importPath": "jax.experimental",
        "description": "jax.experimental",
        "isExtraImport": true,
        "detail": "jax.experimental",
        "documentation": {}
    },
    {
        "label": "pallas",
        "importPath": "jax.experimental",
        "description": "jax.experimental",
        "isExtraImport": true,
        "detail": "jax.experimental",
        "documentation": {}
    },
    {
        "label": "pallas",
        "importPath": "jax.experimental",
        "description": "jax.experimental",
        "isExtraImport": true,
        "detail": "jax.experimental",
        "documentation": {}
    },
    {
        "label": "pallas",
        "importPath": "jax.experimental",
        "description": "jax.experimental",
        "isExtraImport": true,
        "detail": "jax.experimental",
        "documentation": {}
    },
    {
        "label": "pallas",
        "importPath": "jax.experimental",
        "description": "jax.experimental",
        "isExtraImport": true,
        "detail": "jax.experimental",
        "documentation": {}
    },
    {
        "label": "pallas",
        "importPath": "jax.experimental",
        "description": "jax.experimental",
        "isExtraImport": true,
        "detail": "jax.experimental",
        "documentation": {}
    },
    {
        "label": "pallas",
        "importPath": "jax.experimental",
        "description": "jax.experimental",
        "isExtraImport": true,
        "detail": "jax.experimental",
        "documentation": {}
    },
    {
        "label": "pallas",
        "importPath": "jax.experimental",
        "description": "jax.experimental",
        "isExtraImport": true,
        "detail": "jax.experimental",
        "documentation": {}
    },
    {
        "label": "mesh_utils",
        "importPath": "jax.experimental",
        "description": "jax.experimental",
        "isExtraImport": true,
        "detail": "jax.experimental",
        "documentation": {}
    },
    {
        "label": "multihost_utils",
        "importPath": "jax.experimental",
        "description": "jax.experimental",
        "isExtraImport": true,
        "detail": "jax.experimental",
        "documentation": {}
    },
    {
        "label": "for_loop",
        "importPath": "jax._src.lax.control_flow.for_loop",
        "description": "jax._src.lax.control_flow.for_loop",
        "isExtraImport": true,
        "detail": "jax._src.lax.control_flow.for_loop",
        "documentation": {}
    },
    {
        "label": "for_loop",
        "importPath": "jax._src.lax.control_flow.for_loop",
        "description": "jax._src.lax.control_flow.for_loop",
        "isExtraImport": true,
        "detail": "jax._src.lax.control_flow.for_loop",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "tpu",
        "importPath": "jax.experimental.pallas",
        "description": "jax.experimental.pallas",
        "isExtraImport": true,
        "detail": "jax.experimental.pallas",
        "documentation": {}
    },
    {
        "label": "tpu",
        "importPath": "jax.experimental.pallas",
        "description": "jax.experimental.pallas",
        "isExtraImport": true,
        "detail": "jax.experimental.pallas",
        "documentation": {}
    },
    {
        "label": "tpu",
        "importPath": "jax.experimental.pallas",
        "description": "jax.experimental.pallas",
        "isExtraImport": true,
        "detail": "jax.experimental.pallas",
        "documentation": {}
    },
    {
        "label": "tpu",
        "importPath": "jax.experimental.pallas",
        "description": "jax.experimental.pallas",
        "isExtraImport": true,
        "detail": "jax.experimental.pallas",
        "documentation": {}
    },
    {
        "label": "Mapping",
        "importPath": "collections.abc",
        "description": "collections.abc",
        "isExtraImport": true,
        "detail": "collections.abc",
        "documentation": {}
    },
    {
        "label": "splash_attention_mask",
        "importPath": "fjformer.pallas_operations.tpu.splash_attention",
        "description": "fjformer.pallas_operations.tpu.splash_attention",
        "isExtraImport": true,
        "detail": "fjformer.pallas_operations.tpu.splash_attention",
        "documentation": {}
    },
    {
        "label": "splash_attention_mask_info",
        "importPath": "fjformer.pallas_operations.tpu.splash_attention",
        "description": "fjformer.pallas_operations.tpu.splash_attention",
        "isExtraImport": true,
        "detail": "fjformer.pallas_operations.tpu.splash_attention",
        "documentation": {}
    },
    {
        "label": "splash_attention_mask",
        "importPath": "fjformer.pallas_operations.tpu.splash_attention",
        "description": "fjformer.pallas_operations.tpu.splash_attention",
        "isExtraImport": true,
        "detail": "fjformer.pallas_operations.tpu.splash_attention",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "create_device_mesh",
        "importPath": "jax.experimental.mesh_utils",
        "description": "jax.experimental.mesh_utils",
        "isExtraImport": true,
        "detail": "jax.experimental.mesh_utils",
        "documentation": {}
    },
    {
        "label": "create_hybrid_device_mesh",
        "importPath": "jax.experimental.mesh_utils",
        "description": "jax.experimental.mesh_utils",
        "isExtraImport": true,
        "detail": "jax.experimental.mesh_utils",
        "documentation": {}
    },
    {
        "label": "pxla",
        "importPath": "jax.interpreters",
        "description": "jax.interpreters",
        "isExtraImport": true,
        "detail": "jax.interpreters",
        "documentation": {}
    },
    {
        "label": "Mesh",
        "importPath": "jax.sharding",
        "description": "jax.sharding",
        "isExtraImport": true,
        "detail": "jax.sharding",
        "documentation": {}
    },
    {
        "label": "NamedSharding",
        "importPath": "jax.sharding",
        "description": "jax.sharding",
        "isExtraImport": true,
        "detail": "jax.sharding",
        "documentation": {}
    },
    {
        "label": "PartitionSpec",
        "importPath": "jax.sharding",
        "description": "jax.sharding",
        "isExtraImport": true,
        "detail": "jax.sharding",
        "documentation": {}
    },
    {
        "label": "Mesh",
        "importPath": "jax.sharding",
        "description": "jax.sharding",
        "isExtraImport": true,
        "detail": "jax.sharding",
        "documentation": {}
    },
    {
        "label": "PartitionSpec",
        "importPath": "jax.sharding",
        "description": "jax.sharding",
        "isExtraImport": true,
        "detail": "jax.sharding",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "pjit",
        "importPath": "jax.experimental.pjit",
        "description": "jax.experimental.pjit",
        "isExtraImport": true,
        "detail": "jax.experimental.pjit",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "importlib.util",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib.util",
        "description": "importlib.util",
        "detail": "importlib.util",
        "documentation": {}
    },
    {
        "label": "dot_product_attention",
        "importPath": "flax.linen.attention",
        "description": "flax.linen.attention",
        "isExtraImport": true,
        "detail": "flax.linen.attention",
        "documentation": {}
    },
    {
        "label": "make_attention_mask",
        "importPath": "flax.linen.attention",
        "description": "flax.linen.attention",
        "isExtraImport": true,
        "detail": "flax.linen.attention",
        "documentation": {}
    },
    {
        "label": "make_causal_mask",
        "importPath": "flax.linen.attention",
        "description": "flax.linen.attention",
        "isExtraImport": true,
        "detail": "flax.linen.attention",
        "documentation": {}
    },
    {
        "label": "combine_masks",
        "importPath": "flax.linen.attention",
        "description": "flax.linen.attention",
        "isExtraImport": true,
        "detail": "flax.linen.attention",
        "documentation": {}
    },
    {
        "label": "flash_attention",
        "importPath": "src.fjformer.pallas_operations.pallas_attention",
        "description": "src.fjformer.pallas_operations.pallas_attention",
        "isExtraImport": true,
        "detail": "src.fjformer.pallas_operations.pallas_attention",
        "documentation": {}
    },
    {
        "label": "jax.random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jax.random",
        "description": "jax.random",
        "detail": "jax.random",
        "documentation": {}
    },
    {
        "label": "src.fjformer.linen",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "src.fjformer.linen",
        "description": "src.fjformer.linen",
        "detail": "src.fjformer.linen",
        "documentation": {}
    },
    {
        "label": "GenerateRNG",
        "importPath": "src.fjformer",
        "description": "src.fjformer",
        "isExtraImport": true,
        "detail": "src.fjformer",
        "documentation": {}
    },
    {
        "label": "compute_weighted_cross_entropy_and_accuracy",
        "importPath": "fjformer.functions.loss_functions",
        "description": "fjformer.functions.loss_functions",
        "isExtraImport": true,
        "detail": "fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "cross_entropy_loss_and_accuracy",
        "importPath": "fjformer.functions.loss_functions",
        "description": "fjformer.functions.loss_functions",
        "isExtraImport": true,
        "detail": "fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "cross_entropy_with_logits",
        "importPath": "fjformer.functions.loss_functions",
        "description": "fjformer.functions.loss_functions",
        "isExtraImport": true,
        "detail": "fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "get_loss_normalizing_factor_and_weights",
        "importPath": "fjformer.functions.loss_functions",
        "description": "fjformer.functions.loss_functions",
        "isExtraImport": true,
        "detail": "fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "fused_cross_entropy_loss_and_accuracy",
        "importPath": "fjformer.functions.loss_functions",
        "description": "fjformer.functions.loss_functions",
        "isExtraImport": true,
        "detail": "fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "SpecialLossNormalizingFactor",
        "importPath": "fjformer.functions.loss_functions",
        "description": "fjformer.functions.loss_functions",
        "isExtraImport": true,
        "detail": "fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "project",
        "kind": 5,
        "importPath": "docs.conf",
        "description": "docs.conf",
        "peekOfCode": "project = \"FJFormer\"\ncopyright = \"2023, Erfan Zare Chavoshi - FJFormer\"\nauthor = \"Erfan Zare Chavoshi\"\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.autosectionlabel\",\n    \"sphinx.ext.doctest\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx.ext.mathjax\",",
        "detail": "docs.conf",
        "documentation": {}
    },
    {
        "label": "copyright",
        "kind": 5,
        "importPath": "docs.conf",
        "description": "docs.conf",
        "peekOfCode": "copyright = \"2023, Erfan Zare Chavoshi - FJFormer\"\nauthor = \"Erfan Zare Chavoshi\"\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.autosectionlabel\",\n    \"sphinx.ext.doctest\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx.ext.mathjax\",\n    \"sphinx.ext.napoleon\",",
        "detail": "docs.conf",
        "documentation": {}
    },
    {
        "label": "author",
        "kind": 5,
        "importPath": "docs.conf",
        "description": "docs.conf",
        "peekOfCode": "author = \"Erfan Zare Chavoshi\"\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.autosectionlabel\",\n    \"sphinx.ext.doctest\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx.ext.mathjax\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.viewcode\",",
        "detail": "docs.conf",
        "documentation": {}
    },
    {
        "label": "extensions",
        "kind": 5,
        "importPath": "docs.conf",
        "description": "docs.conf",
        "peekOfCode": "extensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.autosectionlabel\",\n    \"sphinx.ext.doctest\",\n    \"sphinx.ext.intersphinx\",\n    \"sphinx.ext.mathjax\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.viewcode\",\n    \"myst_nb\",",
        "detail": "docs.conf",
        "documentation": {}
    },
    {
        "label": "templates_path",
        "kind": 5,
        "importPath": "docs.conf",
        "description": "docs.conf",
        "peekOfCode": "templates_path = [\"_templates\"]\nexclude_patterns = [\"_build\", \"Thumbs.db\", \".DS_Store\"]\nintersphinx_mapping = {\n    \"jax\": (\"https://jax.readthedocs.io/en/latest/\", None),\n    \"numpy\": (\"https://numpy.org/doc/stable/\", None),\n    \"flax\": (\"https://flax.readthedocs.io/en/latest/\", None),\n}\nhtml_theme = \"sphinx_book_theme\"\nhtml_static_path = [\"_static\"]\nhtml_css_files = [",
        "detail": "docs.conf",
        "documentation": {}
    },
    {
        "label": "exclude_patterns",
        "kind": 5,
        "importPath": "docs.conf",
        "description": "docs.conf",
        "peekOfCode": "exclude_patterns = [\"_build\", \"Thumbs.db\", \".DS_Store\"]\nintersphinx_mapping = {\n    \"jax\": (\"https://jax.readthedocs.io/en/latest/\", None),\n    \"numpy\": (\"https://numpy.org/doc/stable/\", None),\n    \"flax\": (\"https://flax.readthedocs.io/en/latest/\", None),\n}\nhtml_theme = \"sphinx_book_theme\"\nhtml_static_path = [\"_static\"]\nhtml_css_files = [\n    \"style.css\",",
        "detail": "docs.conf",
        "documentation": {}
    },
    {
        "label": "intersphinx_mapping",
        "kind": 5,
        "importPath": "docs.conf",
        "description": "docs.conf",
        "peekOfCode": "intersphinx_mapping = {\n    \"jax\": (\"https://jax.readthedocs.io/en/latest/\", None),\n    \"numpy\": (\"https://numpy.org/doc/stable/\", None),\n    \"flax\": (\"https://flax.readthedocs.io/en/latest/\", None),\n}\nhtml_theme = \"sphinx_book_theme\"\nhtml_static_path = [\"_static\"]\nhtml_css_files = [\n    \"style.css\",\n]",
        "detail": "docs.conf",
        "documentation": {}
    },
    {
        "label": "html_theme",
        "kind": 5,
        "importPath": "docs.conf",
        "description": "docs.conf",
        "peekOfCode": "html_theme = \"sphinx_book_theme\"\nhtml_static_path = [\"_static\"]\nhtml_css_files = [\n    \"style.css\",\n]\nsource_suffix = [\".rst\", \".md\", \".ipynb\"]\nautosummary_generate = True",
        "detail": "docs.conf",
        "documentation": {}
    },
    {
        "label": "html_static_path",
        "kind": 5,
        "importPath": "docs.conf",
        "description": "docs.conf",
        "peekOfCode": "html_static_path = [\"_static\"]\nhtml_css_files = [\n    \"style.css\",\n]\nsource_suffix = [\".rst\", \".md\", \".ipynb\"]\nautosummary_generate = True",
        "detail": "docs.conf",
        "documentation": {}
    },
    {
        "label": "html_css_files",
        "kind": 5,
        "importPath": "docs.conf",
        "description": "docs.conf",
        "peekOfCode": "html_css_files = [\n    \"style.css\",\n]\nsource_suffix = [\".rst\", \".md\", \".ipynb\"]\nautosummary_generate = True",
        "detail": "docs.conf",
        "documentation": {}
    },
    {
        "label": "source_suffix",
        "kind": 5,
        "importPath": "docs.conf",
        "description": "docs.conf",
        "peekOfCode": "source_suffix = [\".rst\", \".md\", \".ipynb\"]\nautosummary_generate = True",
        "detail": "docs.conf",
        "documentation": {}
    },
    {
        "label": "autosummary_generate",
        "kind": 5,
        "importPath": "docs.conf",
        "description": "docs.conf",
        "peekOfCode": "autosummary_generate = True",
        "detail": "docs.conf",
        "documentation": {}
    },
    {
        "label": "Calibration",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.calibration",
        "description": "src.fjformer.bit_quantization.calibration",
        "peekOfCode": "class Calibration(abc.ABC):\n    @abc.abstractmethod\n    def get_bound(self, x, shared_axes) -> jnp.ndarray:\n        pass\n@flax.struct.dataclass\nclass ConstantCalibration(Calibration):\n    bound: Union[jnp.ndarray, float]\n    def get_bound(self, x, shared_axes) -> jnp.ndarray:\n        \"\"\"Calibration.\"\"\"\n        del shared_axes",
        "detail": "src.fjformer.bit_quantization.calibration",
        "documentation": {}
    },
    {
        "label": "ConstantCalibration",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.calibration",
        "description": "src.fjformer.bit_quantization.calibration",
        "peekOfCode": "class ConstantCalibration(Calibration):\n    bound: Union[jnp.ndarray, float]\n    def get_bound(self, x, shared_axes) -> jnp.ndarray:\n        \"\"\"Calibration.\"\"\"\n        del shared_axes\n        assert self.bound > 0, 'Bound should be positive.'\n        return jnp.asarray(self.bound).reshape((1,) * len(x.shape))\n@flax.struct.dataclass\nclass AbsMaxCalibration(Calibration):\n    \"\"\"Simple max(abs(x)) calibration.\"\"\"",
        "detail": "src.fjformer.bit_quantization.calibration",
        "documentation": {}
    },
    {
        "label": "AbsMaxCalibration",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.calibration",
        "description": "src.fjformer.bit_quantization.calibration",
        "peekOfCode": "class AbsMaxCalibration(Calibration):\n    \"\"\"Simple max(abs(x)) calibration.\"\"\"\n    def get_bound(self, x, shared_axes) -> jnp.ndarray:\n        \"\"\"Calibration.\"\"\"\n        msg = 'Perhaps you are using fake_quant and forgot to set them.'\n        assert shared_axes is not None, msg\n        # NOTE: If you want to clip, consider using clip and clip_gradient in\n        # int_numerics.IntNumerics.\n        abs_max = jnp.max(jnp.abs(x), axis=shared_axes, keepdims=True)\n        abs_max = jnp.where(abs_max == 0.0, jnp.ones_like(abs_max), abs_max)",
        "detail": "src.fjformer.bit_quantization.calibration",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "class Tensor:\n    \"\"\"Configuration of quantization of one tensor or one side of tensor op.\"\"\"\n    numerics: numerics.QNumerics\n    calib_shared_axes: Optional[list[int]]\n    scale_stop_grad: bool\n    # noise+clip+round\n    # We apply gradient of clip_and_round in bwd pass.\n    calibration: calibration.Calibration\n    # Round up the calibration to power of 2 (po2).\n    po2_scale: bool",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "LocalQ",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "class LocalQ:\n    contraction_axis_shard_count: int\n@dataclasses.dataclass(slots=True)\nclass DotGeneralRaw:\n    \"\"\"Configuration of quantization of one dot_general without gradient.\"\"\"\n    lhs: Tensor\n    rhs: Tensor\n    dg_accumulator_dtype: Optional[DType]\n    local_aqt: Optional[LocalQ]\n    @classmethod",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "DotGeneralRaw",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "class DotGeneralRaw:\n    \"\"\"Configuration of quantization of one dot_general without gradient.\"\"\"\n    lhs: Tensor\n    rhs: Tensor\n    dg_accumulator_dtype: Optional[DType]\n    local_aqt: Optional[LocalQ]\n    @classmethod\n    def make(cls, *args, **kwargs) -> 'DotGeneralRaw':\n        \"\"\"\n        The make function is a factory function that creates an instance of the DotGeneralRaw class.",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "DotGeneral",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "class DotGeneral:\n    \"\"\"Configuration of quantization of dot_general and its gradients.\"\"\"\n    fwd: DotGeneralRaw\n    dlhs: DotGeneralRaw\n    drhs: DotGeneralRaw\n    @classmethod\n    def make(cls, *args, **kwargs) -> 'DotGeneral':\n        return dot_general_make(*args, **kwargs)\ndef set_fwd_numerics(cfg, fwd_numerics: numerics.QNumerics):\n    \"\"\"",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "set_fwd_numerics",
        "kind": 2,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "def set_fwd_numerics(cfg, fwd_numerics: numerics.QNumerics):\n    \"\"\"\n    The set_fwd_numerics function sets the numerics of the forward problem.\n    :param cfg: Store the configuration of the simulation\n    :param fwd_numerics: numerics.QNumerics: Set the numerical\n    :return: The configuration object with the numerics for the forward problem set\n    \"\"\"\n    cfg.fwd.lhs.numerics = fwd_numerics\n    cfg.fwd.rhs.numerics = fwd_numerics\ndef set_accumulator_dtype(",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "set_accumulator_dtype",
        "kind": 2,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "def set_accumulator_dtype(\n        cfg: DotGeneral,\n        fwd_dtype: Optional[DType],\n        dlhs_dtype: Optional[DType],\n        drhs_dtype: Optional[DType],\n):\n    \"\"\"\n    The set_accumulator_dtype function sets the accumulator dtype for each of the three\n    differentiable functions.  The accumulator dtype is used to store intermediate results\n    during forward and backward passes.  It is also used to store gradients during backward pass.",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "set_stochastic_rounding",
        "kind": 2,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "def set_stochastic_rounding(\n        cfg: DotGeneral,\n        vjp_lhs_stochastic_rounding: bool,\n        vjp_rhs_stochastic_rounding: bool,\n        implementation: str,\n):\n    \"\"\"Configure stochastic rounding implementation.\"\"\"\n    noise_implementations = {\n        'jax.uniform': lambda shape, key: jax.random.uniform(key, shape) - 0.5,\n        'custom-1': stochastic_rounding.random_centered_uniform,",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "set_static_bound",
        "kind": 2,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "def set_static_bound(cfg: DotGeneral, bound: float = 1.0):\n    \"\"\"\n    The set_static_bound function sets the calibration of all the forward and backward\n    differentiation operators to a constant value. This is useful for testing purposes, as it\n    allows us to check that our implementation is correct by comparing against known values.\n    :param cfg: DotGeneral: Set the bounds for each of the six functions in a dotgeneral object\n    :param bound: float: Set the bound of the calibration\n    \"\"\"\n    cfg.fwd.lhs.calibration = calibration.ConstantCalibration(bound)\n    cfg.fwd.rhs.calibration = calibration.ConstantCalibration(bound)",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "tensor_make",
        "kind": 2,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "def tensor_make(bits: Optional[int]) -> 'Tensor':\n    \"\"\"\n    The tensor_make function is a helper function that creates a Tensor object.\n    :param bits: Optional[int]: Set the number of bits for quantization\n    :return: A tensor object\n    \"\"\"\n    if bits is None:\n        effective_numerics = no_numerics.NoNumerics()\n    else:\n        pz = False if bits == 1 else True",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "dot_general_raw_make",
        "kind": 2,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "def dot_general_raw_make(\n        lhs_bits=None,\n        rhs_bits=None,\n        local_aqt=None,\n) -> 'DotGeneralRaw':\n    \"\"\"\n    The dot_general_raw_make function is a helper function that creates a DotGeneralRaw object.\n    :param lhs_bits: Determine the dtype of the lhs tensor\n    :param rhs_bits: Determine the dtype of the accumulator\n    :param local_aqt: Determine the type of accumulator used",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "conv_general_dilated_make",
        "kind": 2,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "def conv_general_dilated_make(\n        spatial_dimensions=2,\n        lhs_bits: Optional[int] = None,\n        rhs_bits: Optional[int] = None,\n) -> 'DotGeneralRaw':\n    \"\"\"Create quantization config conv_general_dilated.\"\"\"\n    config = dot_general_raw_make(lhs_bits, rhs_bits)\n    # Hardcoding flax assumptions.\n    if config.lhs:\n        config.lhs.calib_shared_axes = list(range(1, spatial_dimensions + 2))",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "dot_general_make",
        "kind": 2,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "def dot_general_make(\n        lhs_bits: Optional[int] = None,\n        rhs_bits: Optional[int] = None,\n        bwd_bits: Optional[int] = None,\n        use_fwd_quant: bool = True,\n        dlhs_local_aqt=None,\n        drhs_local_aqt=None,\n) -> 'DotGeneral':\n    \"\"\"Create quantization configs for input matrices to a matmul.\"\"\"\n    fwd = dot_general_raw_make(lhs_bits, rhs_bits)",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "fully_quantized",
        "kind": 2,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "def fully_quantized(\n        *,\n        fwd_bits: Optional[int] = 8,\n        bwd_bits: Optional[int] = 8,\n        use_fwd_quant: bool = True,\n        use_stochastic_rounding: Optional[bool] = True,\n        # Typically we have (but it's a caller's responsibility to check):\n        # - vjp_lhs_stochastic_rounding is referring to the gradient and\n        # - vjp_rhs_stochastic_rounding is referring to the activations/weights.\n        vjp_lhs_stochastic_rounding: Optional[bool] = None,",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "config_v3",
        "kind": 2,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "def config_v3(\n        *,\n        fwd_bits: Optional[int] = 8,\n        dlhs_bits: Optional[int] = 8,\n        drhs_bits: Optional[int] = None,\n        use_dummy_static_bound: bool = False,\n        rng_type: str = 'jax.uniform',  # 'custom-1'\n        dlhs_local_aqt: Optional[LocalQ] = None,\n        drhs_local_aqt: Optional[LocalQ] = None,\n        fwd_accumulator_dtype: ... = jnp.int32,",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "DType",
        "kind": 5,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "DType = Any\nContext = Any\nClipAndRoundFn = Callable[[jnp.ndarray, Context], jnp.ndarray]\n@dataclasses.dataclass(slots=True)\nclass Tensor:\n    \"\"\"Configuration of quantization of one tensor or one side of tensor op.\"\"\"\n    numerics: numerics.QNumerics\n    calib_shared_axes: Optional[list[int]]\n    scale_stop_grad: bool\n    # noise+clip+round",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "Context",
        "kind": 5,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "Context = Any\nClipAndRoundFn = Callable[[jnp.ndarray, Context], jnp.ndarray]\n@dataclasses.dataclass(slots=True)\nclass Tensor:\n    \"\"\"Configuration of quantization of one tensor or one side of tensor op.\"\"\"\n    numerics: numerics.QNumerics\n    calib_shared_axes: Optional[list[int]]\n    scale_stop_grad: bool\n    # noise+clip+round\n    # We apply gradient of clip_and_round in bwd pass.",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "ClipAndRoundFn",
        "kind": 5,
        "importPath": "src.fjformer.bit_quantization.config",
        "description": "src.fjformer.bit_quantization.config",
        "peekOfCode": "ClipAndRoundFn = Callable[[jnp.ndarray, Context], jnp.ndarray]\n@dataclasses.dataclass(slots=True)\nclass Tensor:\n    \"\"\"Configuration of quantization of one tensor or one side of tensor op.\"\"\"\n    numerics: numerics.QNumerics\n    calib_shared_axes: Optional[list[int]]\n    scale_stop_grad: bool\n    # noise+clip+round\n    # We apply gradient of clip_and_round in bwd pass.\n    calibration: calibration.Calibration",
        "detail": "src.fjformer.bit_quantization.config",
        "documentation": {}
    },
    {
        "label": "IntNumerics",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.int_numerics",
        "description": "src.fjformer.bit_quantization.int_numerics",
        "peekOfCode": "class IntNumerics(numerics.QNumerics, flax.struct.PyTreeNode):\n    \"\"\"Numerics for int8, int4, binary, etc.\"\"\"\n    bits: int\n    preserve_zero: bool\n    preserve_max_val: bool\n    clip: bool\n    clip_gradient: bool\n    round: bool\n    noise_fn: Optional[stochastic_rounding.NoiseFn]\n    dtype: Optional[Any] = None",
        "detail": "src.fjformer.bit_quantization.int_numerics",
        "documentation": {}
    },
    {
        "label": "NoNumerics",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.no_numerics",
        "description": "src.fjformer.bit_quantization.no_numerics",
        "peekOfCode": "class NoNumerics(numerics.QNumerics, flax.struct.PyTreeNode):\n    \"\"\"No quantization, use a native type such as bf16.\"\"\"\n    # noise_fn has no effect in NoNumerics.\n    noise_fn: Optional[stochastic_rounding.NoiseFn] = None\n    dtype: Optional[Any] = None\n    # it in a special way right now. These functions are never called\n    def get_dtype(self):\n        return None\n    def fwd(self, x, context):\n        pass",
        "detail": "src.fjformer.bit_quantization.no_numerics",
        "documentation": {}
    },
    {
        "label": "QNumerics",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.numerics",
        "description": "src.fjformer.bit_quantization.numerics",
        "peekOfCode": "class QNumerics(flax.struct.PyTreeNode, abc.ABC):\n    \"\"\"Numerics for int8, int4, binary, etc.\"\"\"\n    # it in test. Remove and leave only get_dtype(\n    @abc.abstractmethod\n    def get_dtype(self):\n        pass\n    @abc.abstractmethod\n    def fwd(self, x, context):\n        \"\"\"Forward pass.\"\"\"\n        pass",
        "detail": "src.fjformer.bit_quantization.numerics",
        "documentation": {}
    },
    {
        "label": "Context",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.q_dot_general",
        "description": "src.fjformer.bit_quantization.q_dot_general",
        "peekOfCode": "class Context:\n    key: Optional[jax.Array]\n    train_step: Optional[int]\ndef _context_split(context: Context) -> tuple[Context, Context]:\n    def mk_ctx(key):\n        return Context(key=key, train_step=context.train_step)\n    if context.key is not None:\n        key1, key2 = jax.random.split(context.key)\n        return mk_ctx(key1), mk_ctx(key2)\n    return mk_ctx(None), mk_ctx(None)",
        "detail": "src.fjformer.bit_quantization.q_dot_general",
        "documentation": {}
    },
    {
        "label": "QTensor",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.q_dot_general",
        "description": "src.fjformer.bit_quantization.q_dot_general",
        "peekOfCode": "class QTensor:\n    qvalue: jnp.ndarray\n    qvalue_scale_t: jnp.ndarray\n@flax.struct.dataclass\nclass MultiTensor:\n    x: jnp.ndarray\n    qx: Optional[QTensor]\n@flax.struct.dataclass\nclass TensorRes:\n    \"\"\"All the things we pass from the forward pass to the backward pass.\"\"\"",
        "detail": "src.fjformer.bit_quantization.q_dot_general",
        "documentation": {}
    },
    {
        "label": "MultiTensor",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.q_dot_general",
        "description": "src.fjformer.bit_quantization.q_dot_general",
        "peekOfCode": "class MultiTensor:\n    x: jnp.ndarray\n    qx: Optional[QTensor]\n@flax.struct.dataclass\nclass TensorRes:\n    \"\"\"All the things we pass from the forward pass to the backward pass.\"\"\"\n    mt: MultiTensor\n    quant_grad: Union[Callable[[jnp.ndarray], tuple[jnp.ndarray]], None]\n@flax.struct.dataclass\nclass DotGeneralRes:",
        "detail": "src.fjformer.bit_quantization.q_dot_general",
        "documentation": {}
    },
    {
        "label": "TensorRes",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.q_dot_general",
        "description": "src.fjformer.bit_quantization.q_dot_general",
        "peekOfCode": "class TensorRes:\n    \"\"\"All the things we pass from the forward pass to the backward pass.\"\"\"\n    mt: MultiTensor\n    quant_grad: Union[Callable[[jnp.ndarray], tuple[jnp.ndarray]], None]\n@flax.struct.dataclass\nclass DotGeneralRes:\n    context_bwd: Context\n    lhs: TensorRes\n    rhs: TensorRes\ndef _scale_trans(x, ca, ba):",
        "detail": "src.fjformer.bit_quantization.q_dot_general",
        "documentation": {}
    },
    {
        "label": "DotGeneralRes",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.q_dot_general",
        "description": "src.fjformer.bit_quantization.q_dot_general",
        "peekOfCode": "class DotGeneralRes:\n    context_bwd: Context\n    lhs: TensorRes\n    rhs: TensorRes\ndef _scale_trans(x, ca, ba):\n    \"\"\"Transposes x to output dimension order.\"\"\"\n    ca = list(ca)\n    ba = list(ba)\n    for i in ca:\n        assert x.shape[i] == 1",
        "detail": "src.fjformer.bit_quantization.q_dot_general",
        "documentation": {}
    },
    {
        "label": "make_fake_quant",
        "kind": 2,
        "importPath": "src.fjformer.bit_quantization.q_dot_general",
        "description": "src.fjformer.bit_quantization.q_dot_general",
        "peekOfCode": "def make_fake_quant(cfg: config.Tensor, ca=None):\n    def fake_quant(x, context):\n        x_q, inv_scale, _ = _scale_quant(x, cfg=cfg, ca=ca, context=context)\n        return _maybe_mul(x_q, inv_scale)\n    return fake_quant\n@flax.struct.dataclass\n# It is used only when use_fwd_quant = True\nclass QTensor:\n    qvalue: jnp.ndarray\n    qvalue_scale_t: jnp.ndarray",
        "detail": "src.fjformer.bit_quantization.q_dot_general",
        "documentation": {}
    },
    {
        "label": "make_dot_general",
        "kind": 2,
        "importPath": "src.fjformer.bit_quantization.q_dot_general",
        "description": "src.fjformer.bit_quantization.q_dot_general",
        "peekOfCode": "def make_dot_general(cfg: Optional[config.DotGeneral]):\n    \"\"\"\n    The make_dot_general function is a wrapper around the dot_general function.\n    It takes in two QTensors, lhs and rhs, and returns a QTensor out.\n    The make_dot_general function also handles preprocessing of the inputs to dot_general (lhs and rhs)\n    and postprocessing of the output from dot_general (out).  The pre-/post-processing steps are:\n    :param cfg: Optional[config.DotGeneral]: Specify the configuration of the dot_general operation\n    :return: A function that returns a function\n    \"\"\"\n    if cfg is None:",
        "detail": "src.fjformer.bit_quantization.q_dot_general",
        "documentation": {}
    },
    {
        "label": "QuantMode",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.q_flax",
        "description": "src.fjformer.bit_quantization.q_flax",
        "peekOfCode": "class QuantMode(enum.Enum):\n    TRAIN = 1\n    CONVERT = 2\n    SERVE = 3\nclass Freezer(nn.Module):\n    \"\"\"Identity function that can freeze its input.\n    On default it is an identity function that saves the input in a variable.\n    In 'use_frozen=True' mode, ignores the input and returns the frozen value. It\n    is usefult to implement 'constant folding' and put quantized weights and\n    scales in the checkpoint for serving.",
        "detail": "src.fjformer.bit_quantization.q_flax",
        "documentation": {}
    },
    {
        "label": "Freezer",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.q_flax",
        "description": "src.fjformer.bit_quantization.q_flax",
        "peekOfCode": "class Freezer(nn.Module):\n    \"\"\"Identity function that can freeze its input.\n    On default it is an identity function that saves the input in a variable.\n    In 'use_frozen=True' mode, ignores the input and returns the frozen value. It\n    is usefult to implement 'constant folding' and put quantized weights and\n    scales in the checkpoint for serving.\n    \"\"\"\n    quant_collection: str\n    quant_mode: QuantMode\n    q_shape: Iterable[int]",
        "detail": "src.fjformer.bit_quantization.q_flax",
        "documentation": {}
    },
    {
        "label": "QDotGeneral",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.q_flax",
        "description": "src.fjformer.bit_quantization.q_flax",
        "peekOfCode": "class QDotGeneral(nn.Module):\n    \"\"\"A layer that can be injected into flax.nn.Dense, etc.\"\"\"\n    cfg: Optional[config.DotGeneral] = None\n    prng_name: Optional[str] = 'params'\n    lhs_quant_mode: QuantMode = QuantMode.TRAIN\n    lhs_init: nn.initializers.Initializer = jnp.zeros\n    lhs_scale_init: nn.initializers.Initializer = jnp.zeros\n    lhs_var_name: str = 'qlhs'\n    rhs_quant_mode: QuantMode = QuantMode.TRAIN\n    rhs_init: nn.initializers.Initializer = jnp.zeros",
        "detail": "src.fjformer.bit_quantization.q_flax",
        "documentation": {}
    },
    {
        "label": "QEinsum",
        "kind": 6,
        "importPath": "src.fjformer.bit_quantization.q_flax",
        "description": "src.fjformer.bit_quantization.q_flax",
        "peekOfCode": "class QEinsum(flax.struct.PyTreeNode):\n    \"\"\"Quantized Einsum class for model injection.\"\"\"\n    cfg: Optional[config.DotGeneral] = None\n    prng_name: Optional[str] = 'params'\n    lhs_quant_mode: QuantMode = QuantMode.TRAIN\n    lhs_init: nn.initializers.Initializer = jnp.zeros\n    lhs_scale_init: nn.initializers.Initializer = jnp.zeros\n    lhs_var_name: str = 'qlhs'\n    rhs_quant_mode: QuantMode = QuantMode.TRAIN\n    rhs_init: nn.initializers.Initializer = jnp.zeros",
        "detail": "src.fjformer.bit_quantization.q_flax",
        "documentation": {}
    },
    {
        "label": "config_v4",
        "kind": 2,
        "importPath": "src.fjformer.bit_quantization.q_flax",
        "description": "src.fjformer.bit_quantization.q_flax",
        "peekOfCode": "def config_v4(\n        *,\n        fwd_bits: Optional[int] = 8,\n        dlhs_bits: Optional[int] = 8,\n        drhs_bits: Optional[int] = None,\n        # The dummy static bound flag is for performance benchmarking.\n        use_dummy_static_bound: bool = False,\n        rng_type: str = 'jax.uniform',  # 'custom-1'\n        dlhs_local_aqt: Optional[config.LocalQ] = None,\n        drhs_local_aqt: Optional[config.LocalQ] = None,",
        "detail": "src.fjformer.bit_quantization.q_flax",
        "documentation": {}
    },
    {
        "label": "random_centered_uniform",
        "kind": 2,
        "importPath": "src.fjformer.bit_quantization.stochastic_rounding",
        "description": "src.fjformer.bit_quantization.stochastic_rounding",
        "peekOfCode": "def random_centered_uniform(\n        shape: tuple[int, ...], key: jax.Array\n) -> jnp.ndarray:\n    \"\"\"Generates uniform number in [-0.5, 0.5].\"\"\"\n    dtype = jnp.dtype('uint16')\n    nbits = jnp.iinfo(dtype).bits\n    # Generate random bits.\n    bits = jax.random.bits(key, shape, dtype)\n    # Align bits with the mantissa of f32.\n    nmant = jnp.finfo(jnp.float32).nmant",
        "detail": "src.fjformer.bit_quantization.stochastic_rounding",
        "documentation": {}
    },
    {
        "label": "NoiseFn",
        "kind": 5,
        "importPath": "src.fjformer.bit_quantization.stochastic_rounding",
        "description": "src.fjformer.bit_quantization.stochastic_rounding",
        "peekOfCode": "NoiseFn = Callable[[tuple[int, ...], jax.Array], jnp.ndarray]\ndef random_centered_uniform(\n        shape: tuple[int, ...], key: jax.Array\n) -> jnp.ndarray:\n    \"\"\"Generates uniform number in [-0.5, 0.5].\"\"\"\n    dtype = jnp.dtype('uint16')\n    nbits = jnp.iinfo(dtype).bits\n    # Generate random bits.\n    bits = jax.random.bits(key, shape, dtype)\n    # Align bits with the mantissa of f32.",
        "detail": "src.fjformer.bit_quantization.stochastic_rounding",
        "documentation": {}
    },
    {
        "label": "get_dtype",
        "kind": 2,
        "importPath": "src.fjformer.checkpoint._load",
        "description": "src.fjformer.checkpoint._load",
        "peekOfCode": "def get_dtype(dtype):\n    if isinstance(dtype, str):\n        dtype = {\n            \"bf16\": jnp.bfloat16,\n            \"bfloat16\": jnp.bfloat16,\n            \"fp16\": jnp.float16,\n            \"float16\": jnp.float16,\n            \"fp32\": jnp.float32,\n            \"float32\": jnp.float32,\n            \"fp64\": jnp.float64,",
        "detail": "src.fjformer.checkpoint._load",
        "documentation": {}
    },
    {
        "label": "float_tensor_to_dtype",
        "kind": 2,
        "importPath": "src.fjformer.checkpoint._load",
        "description": "src.fjformer.checkpoint._load",
        "peekOfCode": "def float_tensor_to_dtype(tensor, dtype):\n    if dtype is None or dtype == \"\":\n        return tensor\n    if isinstance(dtype, str):\n        dtype = get_dtype(dtype)\n    float_dtypes = (jnp.bfloat16, jnp.float16, jnp.float32, jnp.float64)\n    if getattr(tensor, \"dtype\", None) in float_dtypes:\n        tensor = tensor.astype(dtype)\n    return tensor\ndef load_and_convert_checkpoint_to_torch(",
        "detail": "src.fjformer.checkpoint._load",
        "documentation": {}
    },
    {
        "label": "load_and_convert_checkpoint_to_torch",
        "kind": 2,
        "importPath": "src.fjformer.checkpoint._load",
        "description": "src.fjformer.checkpoint._load",
        "peekOfCode": "def load_and_convert_checkpoint_to_torch(\n    path,\n    dtype=jnp.float16,\n    transpose_needed=None,\n    transpose_not_needed=None,\n    select_params_field: bool = True,\n):\n    import torch\n    if transpose_needed is None:\n        transpose_needed = [\"kernel\"]",
        "detail": "src.fjformer.checkpoint._load",
        "documentation": {}
    },
    {
        "label": "read_ckpt",
        "kind": 2,
        "importPath": "src.fjformer.checkpoint._load",
        "description": "src.fjformer.checkpoint._load",
        "peekOfCode": "def read_ckpt(\n    path: Union[str, os.PathLike], shard_fns=None, add_extra_past_fix: list = None\n):\n    tensors = {}\n    with open(path, \"rb\") as stream:\n        unpacker = msgpack.Unpacker(stream, read_size=83886080, max_buffer_size=0)\n        for key, value in unpacker:\n            if add_extra_past_fix is not None:\n                key = add_extra_past_fix + key\n            key = tuple(key)",
        "detail": "src.fjformer.checkpoint._load",
        "documentation": {}
    },
    {
        "label": "save_ckpt",
        "kind": 2,
        "importPath": "src.fjformer.checkpoint._load",
        "description": "src.fjformer.checkpoint._load",
        "peekOfCode": "def save_ckpt(train_state, path, gather_fns=None, float_dtype=None):\n    train_state = to_state_dict(train_state)\n    packer = msgpack.Packer()\n    flatten_train_state = flatten_dict(train_state)\n    if gather_fns is not None:\n        gather_fns = flatten_dict(to_state_dict(gather_fns))\n    with open(path, \"wb\") as stream:\n        for key, value in flatten_train_state.items():\n            if gather_fns is not None:\n                value = gather_fns[key](value)",
        "detail": "src.fjformer.checkpoint._load",
        "documentation": {}
    },
    {
        "label": "CheckpointManager",
        "kind": 6,
        "importPath": "src.fjformer.checkpoint.streamer",
        "description": "src.fjformer.checkpoint.streamer",
        "peekOfCode": "class CheckpointManager(object):\n    \"\"\"\n    A class to manage saving and loading checkpoints.\n    Args:\n        checkpoint_dir: The directory to save checkpoints to.\n        enable: Whether to enable saving and loading checkpoints.\n        float_dtype: The floating-point data type to use for saving checkpoints.\n        save_optimizer_state: Whether to save the optimizer state in the checkpoint.\n        verbose: Whether to print verbose output.\n    \"\"\"",
        "detail": "src.fjformer.checkpoint.streamer",
        "documentation": {}
    },
    {
        "label": "load_file",
        "kind": 2,
        "importPath": "src.fjformer.checkpoint.streamer",
        "description": "src.fjformer.checkpoint.streamer",
        "peekOfCode": "def load_file(filename: Union[str, os.PathLike]) -> Tuple[dict, dict]:\n    \"\"\"\n    Load a checkpoint file from the given filename.\n    Args:\n        filename: The path to the checkpoint file.\n    Returns:\n        A tuple containing the state dictionary and metadata.\n    \"\"\"\n    result = {}\n    with safetensors.safe_open(filename, framework=\"flax\") as f:",
        "detail": "src.fjformer.checkpoint.streamer",
        "documentation": {}
    },
    {
        "label": "is_flatten",
        "kind": 2,
        "importPath": "src.fjformer.checkpoint.streamer",
        "description": "src.fjformer.checkpoint.streamer",
        "peekOfCode": "def is_flatten(pytree: Union[dict, struct.PyTreeNode]) -> bool:\n    \"\"\"\n    Check if the given PyTree is flattened.\n    Args:\n        pytree: The PyTree to check.\n    Returns:\n        True if the PyTree is flattened, False otherwise.\n    \"\"\"\n    return True if isinstance([k for k in pytree.keys()][0], tuple) else False\ndef get_dtype(tensor: jax.Array, dtype: Optional[Union[str, jnp.dtype]]) -> jax.Array:",
        "detail": "src.fjformer.checkpoint.streamer",
        "documentation": {}
    },
    {
        "label": "get_dtype",
        "kind": 2,
        "importPath": "src.fjformer.checkpoint.streamer",
        "description": "src.fjformer.checkpoint.streamer",
        "peekOfCode": "def get_dtype(tensor: jax.Array, dtype: Optional[Union[str, jnp.dtype]]) -> jax.Array:\n    \"\"\"\n    Get the tensor with the specified data type.\n    Args:\n        tensor: The input tensor.\n        dtype: The desired data type.\n    Returns:\n        The tensor with the specified data type.\n    \"\"\"\n    if dtype is None or dtype == \"\":",
        "detail": "src.fjformer.checkpoint.streamer",
        "documentation": {}
    },
    {
        "label": "SymbolicConstantError",
        "kind": 6,
        "importPath": "src.fjformer.core.errors",
        "description": "src.fjformer.core.errors",
        "peekOfCode": "class SymbolicConstantError(Exception):\n    \"\"\"Base exception class for SymbolicConstant-related errors.\"\"\"\n    pass\nclass ShapeDtypeError(SymbolicConstantError):\n    \"\"\"Exception raised for errors in shape or dtype determination.\"\"\"\n    pass\nclass OperationError(SymbolicConstantError):\n    \"\"\"Exception raised when an operation on SymbolicConstant fails.\"\"\"\n    pass\nclass MaterializationError(SymbolicConstantError):",
        "detail": "src.fjformer.core.errors",
        "documentation": {}
    },
    {
        "label": "ShapeDtypeError",
        "kind": 6,
        "importPath": "src.fjformer.core.errors",
        "description": "src.fjformer.core.errors",
        "peekOfCode": "class ShapeDtypeError(SymbolicConstantError):\n    \"\"\"Exception raised for errors in shape or dtype determination.\"\"\"\n    pass\nclass OperationError(SymbolicConstantError):\n    \"\"\"Exception raised when an operation on SymbolicConstant fails.\"\"\"\n    pass\nclass MaterializationError(SymbolicConstantError):\n    \"\"\"Exception raised when materialization of a SymbolicConstant fails.\"\"\"\n    pass\nclass UnsupportedPrimitiveError(SymbolicConstantError):",
        "detail": "src.fjformer.core.errors",
        "documentation": {}
    },
    {
        "label": "OperationError",
        "kind": 6,
        "importPath": "src.fjformer.core.errors",
        "description": "src.fjformer.core.errors",
        "peekOfCode": "class OperationError(SymbolicConstantError):\n    \"\"\"Exception raised when an operation on SymbolicConstant fails.\"\"\"\n    pass\nclass MaterializationError(SymbolicConstantError):\n    \"\"\"Exception raised when materialization of a SymbolicConstant fails.\"\"\"\n    pass\nclass UnsupportedPrimitiveError(SymbolicConstantError):\n    \"\"\"Exception raised when an unsupported JAX primitive is encountered.\"\"\"\n    pass\nclass UninitializedAval(Exception):",
        "detail": "src.fjformer.core.errors",
        "documentation": {}
    },
    {
        "label": "MaterializationError",
        "kind": 6,
        "importPath": "src.fjformer.core.errors",
        "description": "src.fjformer.core.errors",
        "peekOfCode": "class MaterializationError(SymbolicConstantError):\n    \"\"\"Exception raised when materialization of a SymbolicConstant fails.\"\"\"\n    pass\nclass UnsupportedPrimitiveError(SymbolicConstantError):\n    \"\"\"Exception raised when an unsupported JAX primitive is encountered.\"\"\"\n    pass\nclass UninitializedAval(Exception):\n    \"\"\"Exception raised when an aval is accessed before initialization.\"\"\"\n    def __init__(self, kind):\n        super().__init__(",
        "detail": "src.fjformer.core.errors",
        "documentation": {}
    },
    {
        "label": "UnsupportedPrimitiveError",
        "kind": 6,
        "importPath": "src.fjformer.core.errors",
        "description": "src.fjformer.core.errors",
        "peekOfCode": "class UnsupportedPrimitiveError(SymbolicConstantError):\n    \"\"\"Exception raised when an unsupported JAX primitive is encountered.\"\"\"\n    pass\nclass UninitializedAval(Exception):\n    \"\"\"Exception raised when an aval is accessed before initialization.\"\"\"\n    def __init__(self, kind):\n        super().__init__(\n            (\n                f\"{kind} was not set during initialization. Shape and dtype may be set by:\"\n                \"\\n\\t1. Directly passing them as keyword arguments to ImplicitArray instances\"",
        "detail": "src.fjformer.core.errors",
        "documentation": {}
    },
    {
        "label": "UninitializedAval",
        "kind": 6,
        "importPath": "src.fjformer.core.errors",
        "description": "src.fjformer.core.errors",
        "peekOfCode": "class UninitializedAval(Exception):\n    \"\"\"Exception raised when an aval is accessed before initialization.\"\"\"\n    def __init__(self, kind):\n        super().__init__(\n            (\n                f\"{kind} was not set during initialization. Shape and dtype may be set by:\"\n                \"\\n\\t1. Directly passing them as keyword arguments to ImplicitArray instances\"\n                \"\\n\\t2. Overriding the default_shape/default_dtype class attributes\"\n                \"\\n\\t3. Overriding the compute_shape/compute_dtype methods\"\n                \"\\n\\t4. Overriding __post_init__ and setting their values there\"",
        "detail": "src.fjformer.core.errors",
        "documentation": {}
    },
    {
        "label": "ImplicitArrayError",
        "kind": 6,
        "importPath": "src.fjformer.core.errors",
        "description": "src.fjformer.core.errors",
        "peekOfCode": "class ImplicitArrayError(Exception):\n    \"\"\"Base exception for ImplicitArray-related errors.\"\"\"\n    pass",
        "detail": "src.fjformer.core.errors",
        "documentation": {}
    },
    {
        "label": "ArrayValue",
        "kind": 6,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "class ArrayValue(ABC):\n    pass\nArrayValue.register(jax.Array)\ndef get_lax_primitive_by_name(name: str) -> jax.core.Primitive:\n    \"\"\"Get a JAX LAX primitive by its name.\"\"\"\n    return getattr(jax.lax, f\"{name}_p\")\ndef get_primitive_handler(primitive):\n    \"\"\"Get or create a handler for a given primitive.\"\"\"\n    if isinstance(primitive, str):\n        primitive = get_lax_primitive_by_name(primitive)",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "_EmptyNodeCls",
        "kind": 6,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "class _EmptyNodeCls:\n    _instance = None\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n        return cls._instance\nEmptyNode = _EmptyNodeCls()\ntree_util.register_pytree_node(\n    _EmptyNodeCls, lambda node: ((), None), lambda _, __: EmptyNode\n)",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "_AvalDescriptor",
        "kind": 6,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "class _AvalDescriptor:\n    \"\"\"Descriptor for lazy initialization of shape and dtype.\"\"\"\n    def __set_name__(self, owner: Any, name: str) -> None:\n        self._name = f\"_{name}\"\n    def __get__(self, obj: Any, owner: Any = None) -> Any:\n        if obj is None:\n            return None\n        result = getattr(obj, self._name, None)\n        if result is None:\n            raise UninitializedAval(kind=self._name[1:])",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "_ImplicitArrayBase",
        "kind": 6,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "class _ImplicitArrayBase(ArrayValue, ABC):\n    \"\"\"Base class for ImplicitArray with common attributes.\"\"\"\n    commute_ops: ClassVar[bool] = True\n    warn_on_materialize: ClassVar[bool] = True\n    default_shape: ClassVar[Optional[Tuple[int, ...]]] = None\n    default_dtype: ClassVar[Optional[Any]] = None\n    shape: Optional[Tuple[int, ...]] = aux_field(kw_only=True, default=None)\n    dtype: Any = aux_field(kw_only=True, default=None)\n@dataclass\nclass ImplicitArray(_ImplicitArrayBase):",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "ImplicitArray",
        "kind": 6,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "class ImplicitArray(_ImplicitArrayBase):\n    \"\"\"\n    Abstract class for representing an abstract array without instantiation.\n    Subclasses must implement the materialize method, which defines the relationship\n    between the implicit array and the value it represents. Subclasses are valid\n    arguments to functions decorated with qax.use_implicit_args.\n    The represented shape and dtype may be defined in various ways:\n    1. Explicitly passing shape/dtype keyword arguments at initialization\n    2. Overriding the default_shape/default_dtype class variables\n    3. Overriding the compute_shape/compute_dtype methods",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "ImplicitArrayTracer",
        "kind": 6,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "class ImplicitArrayTracer(core.Tracer):\n    def __init__(self, trace, value):\n        super().__init__(trace)\n        self.value = value\n    @property\n    def aval(self):\n        if isinstance(self.value, ImplicitArray):\n            return self.value.aval\n        return core.get_aval(self.value)\n    def full_lower(self):",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "ImplicitArrayTrace",
        "kind": 6,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "class ImplicitArrayTrace(core.Trace):\n    pure = lift = lambda self, val: ImplicitArrayTracer(self, val)\n    def process_primitive(self, primitive, tracers, params):\n        outs = NotImplemented\n        vals = [t.value for t in tracers]\n        implicit_idx = next(\n            i for i, v in enumerate(vals) if isinstance(v, ImplicitArray)\n        )\n        # First try to handle the primitive using custom handlers\n        outs = vals[implicit_idx].handle_primitive(primitive, *vals, params=params)",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "get_lax_primitive_by_name",
        "kind": 2,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "def get_lax_primitive_by_name(name: str) -> jax.core.Primitive:\n    \"\"\"Get a JAX LAX primitive by its name.\"\"\"\n    return getattr(jax.lax, f\"{name}_p\")\ndef get_primitive_handler(primitive):\n    \"\"\"Get or create a handler for a given primitive.\"\"\"\n    if isinstance(primitive, str):\n        primitive = get_lax_primitive_by_name(primitive)\n    handler = _dispatch.functions.get(primitive)\n    if handler is None:\n        def _not_impl_handler(primitive: jax.core.Primitive, *args, **kwargs):",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "get_primitive_handler",
        "kind": 2,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "def get_primitive_handler(primitive):\n    \"\"\"Get or create a handler for a given primitive.\"\"\"\n    if isinstance(primitive, str):\n        primitive = get_lax_primitive_by_name(primitive)\n    handler = _dispatch.functions.get(primitive)\n    if handler is None:\n        def _not_impl_handler(primitive: jax.core.Primitive, *args, **kwargs):\n            return NotImplemented\n        _not_impl_handler.__doc__ = f\"Default handler for {primitive.name}\"\n        handler = Function(_not_impl_handler)",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "primitive_handler",
        "kind": 2,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "def primitive_handler(primitives, precedence=0):\n    \"\"\"Decorator to register a handler for one or more primitives.\"\"\"\n    if isinstance(primitives, (str, jax.core.Primitive)):\n        primitives = [primitives]\n    def decorator(fn):\n        for primitive in primitives:\n            handler = get_primitive_handler(primitive)\n            handler.register(fn, precedence=precedence)\n    return decorator\nCOMMUTATIVE_OPS = frozenset(",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "combine_leaf_predicate",
        "kind": 2,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "def combine_leaf_predicate(base_fn, is_leaf):\n    @wraps(base_fn)\n    def new_fn(*args, new_is_leaf=None):\n        if new_is_leaf is None:\n            combined_is_leaf = is_leaf\n        else:\n            def combined_is_leaf(arg):\n                return is_leaf(arg) or new_is_leaf(arg)\n        return base_fn(*args, is_leaf=combined_is_leaf)\n    return new_fn",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "leaf_predicate",
        "kind": 2,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "def leaf_predicate(x):\n    return isinstance(x, (ImplicitArray, _EmptyNodeCls))\ntree_map_with_implicit = combine_leaf_predicate(jax.tree_map, leaf_predicate)\ntree_map_with_path_with_implicit = combine_leaf_predicate(\n    tree_util.tree_map_with_path, leaf_predicate\n)\ntree_flatten_with_implicit = combine_leaf_predicate(\n    tree_util.tree_flatten, leaf_predicate\n)\ntree_flatten_with_path_with_implicit = combine_leaf_predicate(",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "flatten_one_implicit_layer",
        "kind": 2,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "def flatten_one_implicit_layer(tree):\n    def is_leaf_below_node(node, x):\n        return isinstance(x, ImplicitArray) and x is not node\n    def replace_subtree_implicits(node):\n        return tree_util.tree_map(\n            lambda _: 1, node, is_leaf=partial(is_leaf_below_node, node)\n        )\n    prototype = tree_map_with_implicit(replace_subtree_implicits, tree)\n    struct = tree_util.tree_structure(prototype)\n    leaves = tree_leaves_with_implicit(tree)",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "implicit_depth",
        "kind": 2,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "def implicit_depth(tree):\n    leaves = tree_leaves_with_implicit(tree)\n    depth = 0\n    while True:\n        next_leaves = []\n        any_implicit = False\n        for leaf in leaves:\n            if not isinstance(leaf, ImplicitArray):\n                continue\n            any_implicit = True",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "get_common_prefix_transforms",
        "kind": 2,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "def get_common_prefix_transforms(trees):\n    \"\"\"\n    Given an iterable of pytrees which have the same structure after all\n    ImplicitArray instances are materialized, return a list of callables\n    which will transform each tree into the largest common structure\n    obtainable via materialization of ImplicitArrays.\n    \"\"\"\n    if len(trees) <= 1:\n        return [lambda x: x for _ in trees]\n    all_leaves, structures = zip(*(tree_flatten_with_implicit(tree) for tree in trees))",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "materialize_nested",
        "kind": 2,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "def materialize_nested(implicit_arr, full=False):\n    \"\"\"\n    Materialize an ImplicitArray instance, handling the case where implicit_arr.materialize()\n    involves further ImplicitArray instances.\n    Arguments:\n        implicit_arr: An ImplicitArray instance\n        full: If True, repeatedly materialize until the result is a concrete array\n    Returns:\n        The materialized array\n    \"\"\"",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "use_implicit_args",
        "kind": 2,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "def use_implicit_args(f: Callable) -> Callable:\n    \"\"\"\n    Decorator which allows a function to accept arguments which subclass ImplicitArray, possibly\n    including further ImplicitArray instances as children.\n    Any number of arguments (including 0) may be ImplicitArrays.\n    Args:\n        f: The function to be decorated.\n    Returns:\n        A wrapped function that can handle ImplicitArray arguments.\n    \"\"\"",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "default_handler",
        "kind": 2,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "def default_handler(primitive: Any, *args: Any, **params: Any) -> Any:\n    \"\"\"Default handler for primitives.\"\"\"\n    subfuns, bind_params = primitive.get_bind_params(params)\n    return primitive.bind(*subfuns, *args, **bind_params)\ndef aux_field(metadata: Optional[dict] = None, **kwargs: Any) -> Any:\n    \"\"\"Create an auxiliary field for ImplicitArray subclasses.\"\"\"\n    metadata = dict(metadata) if metadata else {}\n    metadata[\"implicit_array_aux\"] = True\n    return field(metadata=metadata, **kwargs)\nclass _AvalDescriptor:",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "aux_field",
        "kind": 2,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "def aux_field(metadata: Optional[dict] = None, **kwargs: Any) -> Any:\n    \"\"\"Create an auxiliary field for ImplicitArray subclasses.\"\"\"\n    metadata = dict(metadata) if metadata else {}\n    metadata[\"implicit_array_aux\"] = True\n    return field(metadata=metadata, **kwargs)\nclass _AvalDescriptor:\n    \"\"\"Descriptor for lazy initialization of shape and dtype.\"\"\"\n    def __set_name__(self, owner: Any, name: str) -> None:\n        self._name = f\"_{name}\"\n    def __get__(self, obj: Any, owner: Any = None) -> Any:",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "wrap_jaxpr",
        "kind": 2,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "def wrap_jaxpr(jaxpr, vals_with_implicits, return_closed=True):\n    if isinstance(jaxpr, jax.core.ClosedJaxpr):\n        literals = jaxpr.literals\n        jaxpr = jaxpr.jaxpr\n    else:\n        literals = []\n    wrapped_fn = lu.wrap_init(use_implicit_args(partial(core.eval_jaxpr, jaxpr)))\n    flat_args, in_tree = jax.tree_util.tree_flatten((literals, *vals_with_implicits))\n    flat_fn, out_tree = flatten_fun_nokwargs(wrapped_fn, in_tree)\n    new_jaxpr, _, consts = pe.trace_to_jaxpr_dynamic(",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "materialize_handler",
        "kind": 2,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "def materialize_handler(primitive, *vals, params):\n    vals = _materialize_all(vals)\n    subfuns, bind_params = primitive.get_bind_params(params)\n    result = use_implicit_args(primitive.bind)(*subfuns, *vals, **bind_params)\n    return result\ndef _broadcast_tuple(t, trees):\n    if isinstance(trees, jax.tree_util.PyTreeDef):\n        trees = jax.tree_util.tree_unflatten(trees, range(trees.num_leaves))\n    assert len(t) == len(trees)\n    return tuple(",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "_dispatch",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "_dispatch = Dispatcher()\n_primitive_ids = count()\nclass ArrayValue(ABC):\n    pass\nArrayValue.register(jax.Array)\ndef get_lax_primitive_by_name(name: str) -> jax.core.Primitive:\n    \"\"\"Get a JAX LAX primitive by its name.\"\"\"\n    return getattr(jax.lax, f\"{name}_p\")\ndef get_primitive_handler(primitive):\n    \"\"\"Get or create a handler for a given primitive.\"\"\"",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "_primitive_ids",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "_primitive_ids = count()\nclass ArrayValue(ABC):\n    pass\nArrayValue.register(jax.Array)\ndef get_lax_primitive_by_name(name: str) -> jax.core.Primitive:\n    \"\"\"Get a JAX LAX primitive by its name.\"\"\"\n    return getattr(jax.lax, f\"{name}_p\")\ndef get_primitive_handler(primitive):\n    \"\"\"Get or create a handler for a given primitive.\"\"\"\n    if isinstance(primitive, str):",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "COMMUTATIVE_OPS",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "COMMUTATIVE_OPS = frozenset(\n    [\n        \"add\",\n        \"bitwise_and\",\n        \"bitwise_or\",\n        \"bitwise_xor\",\n        \"eq\",\n        \"max\",\n        \"min\",\n        \"mul\",",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "ELEMENTWISE_UNOPS",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "ELEMENTWISE_UNOPS = frozenset(\n    [\n        \"abs\",\n        \"acos\",\n        \"acosh\",\n        \"asin\",\n        \"asinh\",\n        \"atan\",\n        \"atanh\",\n        \"bessel_i0e\",",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "ELEMENTWISE_BINOPS",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "ELEMENTWISE_BINOPS = frozenset(\n    [\n        \"add\",\n        \"and\",\n        \"atan2\",\n        \"complex\",\n        \"div\",\n        \"eq\",\n        \"ge\",\n        \"gt\",",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "REDUCTION_OPS",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "REDUCTION_OPS = frozenset(\n    [\n        \"argmax\",\n        \"argmin\",\n        \"reduce_and\",\n        \"reduce_max\",\n        \"reduce_min\",\n        \"reduce_or\",\n        \"reduce_prod\",\n        \"reduce_sum\",",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "CUMULATIVE_REDUCTION_OPS",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "CUMULATIVE_REDUCTION_OPS = frozenset(\n    [\n        \"cumlogsumexp\",\n        \"cummax\",\n        \"cummin\",\n        \"cumprod\",\n        \"cumsum\",\n    ]\n)\nclass _EmptyNodeCls:",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "EmptyNode",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "EmptyNode = _EmptyNodeCls()\ntree_util.register_pytree_node(\n    _EmptyNodeCls, lambda node: ((), None), lambda _, __: EmptyNode\n)\ndef combine_leaf_predicate(base_fn, is_leaf):\n    @wraps(base_fn)\n    def new_fn(*args, new_is_leaf=None):\n        if new_is_leaf is None:\n            combined_is_leaf = is_leaf\n        else:",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "tree_map_with_implicit",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "tree_map_with_implicit = combine_leaf_predicate(jax.tree_map, leaf_predicate)\ntree_map_with_path_with_implicit = combine_leaf_predicate(\n    tree_util.tree_map_with_path, leaf_predicate\n)\ntree_flatten_with_implicit = combine_leaf_predicate(\n    tree_util.tree_flatten, leaf_predicate\n)\ntree_flatten_with_path_with_implicit = combine_leaf_predicate(\n    tree_util.tree_flatten_with_path, leaf_predicate\n)",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "tree_map_with_path_with_implicit",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "tree_map_with_path_with_implicit = combine_leaf_predicate(\n    tree_util.tree_map_with_path, leaf_predicate\n)\ntree_flatten_with_implicit = combine_leaf_predicate(\n    tree_util.tree_flatten, leaf_predicate\n)\ntree_flatten_with_path_with_implicit = combine_leaf_predicate(\n    tree_util.tree_flatten_with_path, leaf_predicate\n)\ntree_leaves_with_implicit = combine_leaf_predicate(",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "tree_flatten_with_implicit",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "tree_flatten_with_implicit = combine_leaf_predicate(\n    tree_util.tree_flatten, leaf_predicate\n)\ntree_flatten_with_path_with_implicit = combine_leaf_predicate(\n    tree_util.tree_flatten_with_path, leaf_predicate\n)\ntree_leaves_with_implicit = combine_leaf_predicate(\n    tree_util.tree_leaves, leaf_predicate\n)\ntree_structure_with_implicit = combine_leaf_predicate(",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "tree_flatten_with_path_with_implicit",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "tree_flatten_with_path_with_implicit = combine_leaf_predicate(\n    tree_util.tree_flatten_with_path, leaf_predicate\n)\ntree_leaves_with_implicit = combine_leaf_predicate(\n    tree_util.tree_leaves, leaf_predicate\n)\ntree_structure_with_implicit = combine_leaf_predicate(\n    tree_util.tree_structure, leaf_predicate\n)\ndef flatten_one_implicit_layer(tree):",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "tree_leaves_with_implicit",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "tree_leaves_with_implicit = combine_leaf_predicate(\n    tree_util.tree_leaves, leaf_predicate\n)\ntree_structure_with_implicit = combine_leaf_predicate(\n    tree_util.tree_structure, leaf_predicate\n)\ndef flatten_one_implicit_layer(tree):\n    def is_leaf_below_node(node, x):\n        return isinstance(x, ImplicitArray) and x is not node\n    def replace_subtree_implicits(node):",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "tree_structure_with_implicit",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "tree_structure_with_implicit = combine_leaf_predicate(\n    tree_util.tree_structure, leaf_predicate\n)\ndef flatten_one_implicit_layer(tree):\n    def is_leaf_below_node(node, x):\n        return isinstance(x, ImplicitArray) and x is not node\n    def replace_subtree_implicits(node):\n        return tree_util.tree_map(\n            lambda _: 1, node, is_leaf=partial(is_leaf_below_node, node)\n        )",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "_aval_discovery",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "_aval_discovery = ContextVar(\"aval_discovery\", default=False)\n@contextmanager\ndef _aval_discovery_context():\n    \"\"\"Context manager for aval discovery.\"\"\"\n    token = _aval_discovery.set(True)\n    try:\n        yield\n    finally:\n        _aval_discovery.reset(token)\n@dataclass",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "_default_handlers",
        "kind": 5,
        "importPath": "src.fjformer.core.implicit_array",
        "description": "src.fjformer.core.implicit_array",
        "peekOfCode": "_default_handlers = {\n    \"cond\": _handle_cond,\n    \"remat2\": _handle_remat2,\n    \"pjit\": _handle_pjit,\n    \"scan\": _handle_scan,\n}\ndef materialize_handler(primitive, *vals, params):\n    vals = _materialize_all(vals)\n    subfuns, bind_params = primitive.get_bind_params(params)\n    result = use_implicit_args(primitive.bind)(*subfuns, *vals, **bind_params)",
        "detail": "src.fjformer.core.implicit_array",
        "documentation": {}
    },
    {
        "label": "SymbolicConstant",
        "kind": 6,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "class SymbolicConstant(ImplicitArray):\n    \"\"\"\n    Represents a constant array symbolically, allowing for efficient operations at compile time.\n    Attributes:\n        value (Any): The constant value of the array\n        weak_type (bool): Whether the constant has a weak type\n    \"\"\"\n    value: Any = aux_field()\n    weak_type: bool = aux_field(default=False)\n    def __post_init__(self):",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "symbolic_zero_like",
        "kind": 2,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "def symbolic_zero_like(\n    x: Any, shape: Optional[Tuple[int, ...]] = None, dtype: Optional[Any] = None\n) -> \"SymbolicConstant\":\n    \"\"\"\n    Create a SymbolicConstant filled with zeros, similar to the input.\n    Args:\n        x: Input to base the result on\n        shape: Optional shape for the result\n        dtype: Optional dtype for the result\n    Returns:",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "symbolic_full_like",
        "kind": 2,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "def symbolic_full_like(\n    x: Any,\n    fill_value: Any,\n    shape: Optional[Tuple[int, ...]] = None,\n    dtype: Optional[Any] = None,\n) -> \"SymbolicConstant\":\n    \"\"\"\n    Create a SymbolicConstant filled with a specific value, similar to the input.\n    Args:\n        x: Input to base the result on",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "broadcast_to",
        "kind": 2,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "def broadcast_to(val, shape):\n    return jnp.broadcast_to(val, shape)\n@use_implicit_args\ndef astype(val, dtype):\n    return val.astype(dtype)\n@primitive_handler(\n    [\n        \"reshape\",\n        \"broadcast_in_dim\",\n        \"reduce_min\",",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "astype",
        "kind": 2,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "def astype(val, dtype):\n    return val.astype(dtype)\n@primitive_handler(\n    [\n        \"reshape\",\n        \"broadcast_in_dim\",\n        \"reduce_min\",\n        \"reduce_max\",\n        \"reduce_or\",\n        \"reduce_and\",",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "unchanged_value_op",
        "kind": 2,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "def unchanged_value_op(primitive, sym: SymbolicConstant, **kwargs):\n    \"\"\"\n    Handle operations that don't change the constant value.\n    \"\"\"\n    out_shape, out_dtype = _out_shape_dtype(primitive, sym, **kwargs)\n    return SymbolicConstant(sym.value, shape=out_shape, dtype=out_dtype)\ndef _op_and_reshape(primitive, lhs, rhs, flip=False):\n    \"\"\"\n    Close over one arg so we can do math at tracing time, but let the other one get traced.\n    \"\"\"",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "special_case_binop",
        "kind": 2,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "def special_case_binop(\n    name,\n    identity=None,\n    annihilator=None,\n    flip=False,\n):\n    \"\"\"\n    Create a special case handler for binary operations.\n    \"\"\"\n    lhs_type = SymbolicConstant",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "eval_default_handler",
        "kind": 2,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "def eval_default_handler(primitive, *args, **kwargs):\n    \"\"\"\n    Evaluate the default handler for a primitive at compile time.\n    \"\"\"\n    with jax.ensure_compile_time_eval():\n        result = primitive.bind(*args, **kwargs)\n    return result\n@primitive_handler(ELEMENTWISE_UNOPS, precedence=_GENERAL)\ndef handle_unop(primitive, sym: SymbolicConstant, **kwargs):\n    \"\"\"",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "handle_unop",
        "kind": 2,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "def handle_unop(primitive, sym: SymbolicConstant, **kwargs):\n    \"\"\"\n    Handle unary operations on SymbolicConstants.\n    Args:\n        primitive: The JAX primitive to handle\n        sym: The SymbolicConstant to operate on\n        **kwargs: Additional keyword arguments\n    Returns:\n        SymbolicConstant: The result of the unary operation\n    Raises:",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "handle_binop",
        "kind": 2,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "def handle_binop(\n    primitive,\n    lhs: SymbolicConstant,\n    rhs: SymbolicConstant,\n    **kwargs,\n):\n    \"\"\"\n    Handle binary operations on SymbolicConstants.\n    \"\"\"\n    out_shape, out_dtype = _out_shape_dtype(primitive, lhs, rhs, **kwargs)",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "reduce_sum",
        "kind": 2,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "def reduce_sum(\n    primitive,\n    sym: SymbolicConstant,\n    *,\n    axes,\n):\n    \"\"\"\n    Handle reduction operations (sum and product) on SymbolicConstants.\n    \"\"\"\n    out_shape, out_dtype = _out_shape_dtype(primitive, sym, axes=axes)",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "handle_select_n",
        "kind": 2,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "def handle_select_n(\n    primitive,\n    cond_val,\n    *arg_vals: SymbolicConstant,\n):\n    \"\"\"\n    Handle select_n operation on SymbolicConstants.\n    \"\"\"\n    if len(set(val.value.item() for val in arg_vals)) != 1:\n        return NotImplemented",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "INFINITY",
        "kind": 5,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "INFINITY = float(\"inf\")\nNEGATIVE_INFINITY = float(\"-inf\")\n_GENERAL = -2\n_SPECIALIZED = -1\ndef _get_shape_dtype(\n    x: Any, shape: Optional[Tuple[int, ...]] = None, dtype: Optional[Any] = None\n) -> Tuple[Tuple[int, ...], Any]:\n    \"\"\"\n    Determine the shape and dtype for a given input.\n    Args:",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "NEGATIVE_INFINITY",
        "kind": 5,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "NEGATIVE_INFINITY = float(\"-inf\")\n_GENERAL = -2\n_SPECIALIZED = -1\ndef _get_shape_dtype(\n    x: Any, shape: Optional[Tuple[int, ...]] = None, dtype: Optional[Any] = None\n) -> Tuple[Tuple[int, ...], Any]:\n    \"\"\"\n    Determine the shape and dtype for a given input.\n    Args:\n        x: Input array or value",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "_GENERAL",
        "kind": 5,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "_GENERAL = -2\n_SPECIALIZED = -1\ndef _get_shape_dtype(\n    x: Any, shape: Optional[Tuple[int, ...]] = None, dtype: Optional[Any] = None\n) -> Tuple[Tuple[int, ...], Any]:\n    \"\"\"\n    Determine the shape and dtype for a given input.\n    Args:\n        x: Input array or value\n        shape: Optional shape to use",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "_SPECIALIZED",
        "kind": 5,
        "importPath": "src.fjformer.core.symbols",
        "description": "src.fjformer.core.symbols",
        "peekOfCode": "_SPECIALIZED = -1\ndef _get_shape_dtype(\n    x: Any, shape: Optional[Tuple[int, ...]] = None, dtype: Optional[Any] = None\n) -> Tuple[Tuple[int, ...], Any]:\n    \"\"\"\n    Determine the shape and dtype for a given input.\n    Args:\n        x: Input array or value\n        shape: Optional shape to use\n        dtype: Optional dtype to use",
        "detail": "src.fjformer.core.symbols",
        "documentation": {}
    },
    {
        "label": "ComplementError",
        "kind": 6,
        "importPath": "src.fjformer.core.types",
        "description": "src.fjformer.core.types",
        "peekOfCode": "class ComplementError(Exception):\n    \"\"\"Base exception class for Complement-related errors.\"\"\"\n    pass\nclass TypeParameterError(ComplementError):\n    \"\"\"Exception raised for errors in type parameter initialization or comparison.\"\"\"\n    pass\n# Type variables for better type hinting\nA = TypeVar(\"A\")\nB = TypeVar(\"B\")\nclass _ComplementMeta(type):",
        "detail": "src.fjformer.core.types",
        "documentation": {}
    },
    {
        "label": "TypeParameterError",
        "kind": 6,
        "importPath": "src.fjformer.core.types",
        "description": "src.fjformer.core.types",
        "peekOfCode": "class TypeParameterError(ComplementError):\n    \"\"\"Exception raised for errors in type parameter initialization or comparison.\"\"\"\n    pass\n# Type variables for better type hinting\nA = TypeVar(\"A\")\nB = TypeVar(\"B\")\nclass _ComplementMeta(type):\n    def __instancecheck__(self, x: Any) -> bool:\n        \"\"\"\n        Check if an object is an instance of the Complement type.",
        "detail": "src.fjformer.core.types",
        "documentation": {}
    },
    {
        "label": "_ComplementMeta",
        "kind": 6,
        "importPath": "src.fjformer.core.types",
        "description": "src.fjformer.core.types",
        "peekOfCode": "class _ComplementMeta(type):\n    def __instancecheck__(self, x: Any) -> bool:\n        \"\"\"\n        Check if an object is an instance of the Complement type.\n        Args:\n            x: The object to check.\n        Returns:\n            bool: True if x is an instance of Complement, False otherwise.\n        \"\"\"\n        try:",
        "detail": "src.fjformer.core.types",
        "documentation": {}
    },
    {
        "label": "Complement",
        "kind": 6,
        "importPath": "src.fjformer.core.types",
        "description": "src.fjformer.core.types",
        "peekOfCode": "class Complement(metaclass=_ComplementMeta):\n    \"\"\"\n    Represents the relative complement of two types.\n    The Complement[A, B] represents all elements in A that are not in B (A - B).\n    Attributes:\n        type_parameter (Tuple[Optional[Type], Optional[Type]]): A tuple containing the two types A and B.\n    \"\"\"\n    @classmethod\n    @dispatch\n    def __init_type_parameter__(",
        "detail": "src.fjformer.core.types",
        "documentation": {}
    },
    {
        "label": "A",
        "kind": 5,
        "importPath": "src.fjformer.core.types",
        "description": "src.fjformer.core.types",
        "peekOfCode": "A = TypeVar(\"A\")\nB = TypeVar(\"B\")\nclass _ComplementMeta(type):\n    def __instancecheck__(self, x: Any) -> bool:\n        \"\"\"\n        Check if an object is an instance of the Complement type.\n        Args:\n            x: The object to check.\n        Returns:\n            bool: True if x is an instance of Complement, False otherwise.",
        "detail": "src.fjformer.core.types",
        "documentation": {}
    },
    {
        "label": "B",
        "kind": 5,
        "importPath": "src.fjformer.core.types",
        "description": "src.fjformer.core.types",
        "peekOfCode": "B = TypeVar(\"B\")\nclass _ComplementMeta(type):\n    def __instancecheck__(self, x: Any) -> bool:\n        \"\"\"\n        Check if an object is an instance of the Complement type.\n        Args:\n            x: The object to check.\n        Returns:\n            bool: True if x is an instance of Complement, False otherwise.\n        \"\"\"",
        "detail": "src.fjformer.core.types",
        "documentation": {}
    },
    {
        "label": "OptimizerError",
        "kind": 6,
        "importPath": "src.fjformer.core.utilities",
        "description": "src.fjformer.core.utilities",
        "peekOfCode": "class OptimizerError(Exception):\n    \"\"\"Base exception class for optimizer-related errors.\"\"\"\n    pass\nclass AxisError(OptimizerError):\n    \"\"\"Exception raised for errors related to axis operations.\"\"\"\n    pass\nclass FreezingError(OptimizerError):\n    \"\"\"Exception raised for errors during parameter freezing.\"\"\"\n    pass\nclass UpdateError(OptimizerError):",
        "detail": "src.fjformer.core.utilities",
        "documentation": {}
    },
    {
        "label": "AxisError",
        "kind": 6,
        "importPath": "src.fjformer.core.utilities",
        "description": "src.fjformer.core.utilities",
        "peekOfCode": "class AxisError(OptimizerError):\n    \"\"\"Exception raised for errors related to axis operations.\"\"\"\n    pass\nclass FreezingError(OptimizerError):\n    \"\"\"Exception raised for errors during parameter freezing.\"\"\"\n    pass\nclass UpdateError(OptimizerError):\n    \"\"\"Exception raised for errors during parameter updates.\"\"\"\n    pass\ndef vmap_all_but_one(f: Callable, axis: int, out_ndim: int = 0) -> Callable:",
        "detail": "src.fjformer.core.utilities",
        "documentation": {}
    },
    {
        "label": "FreezingError",
        "kind": 6,
        "importPath": "src.fjformer.core.utilities",
        "description": "src.fjformer.core.utilities",
        "peekOfCode": "class FreezingError(OptimizerError):\n    \"\"\"Exception raised for errors during parameter freezing.\"\"\"\n    pass\nclass UpdateError(OptimizerError):\n    \"\"\"Exception raised for errors during parameter updates.\"\"\"\n    pass\ndef vmap_all_but_one(f: Callable, axis: int, out_ndim: int = 0) -> Callable:\n    \"\"\"\n    Repeatedly calls vmap to map over all axes except for `axis`.\n    All args will be mapped on the same dimensions.",
        "detail": "src.fjformer.core.utilities",
        "documentation": {}
    },
    {
        "label": "UpdateError",
        "kind": 6,
        "importPath": "src.fjformer.core.utilities",
        "description": "src.fjformer.core.utilities",
        "peekOfCode": "class UpdateError(OptimizerError):\n    \"\"\"Exception raised for errors during parameter updates.\"\"\"\n    pass\ndef vmap_all_but_one(f: Callable, axis: int, out_ndim: int = 0) -> Callable:\n    \"\"\"\n    Repeatedly calls vmap to map over all axes except for `axis`.\n    All args will be mapped on the same dimensions.\n    Args:\n        f: The function to be mapped.\n        axis: The axis to exclude from mapping.",
        "detail": "src.fjformer.core.utilities",
        "documentation": {}
    },
    {
        "label": "vmap_all_but_one",
        "kind": 2,
        "importPath": "src.fjformer.core.utilities",
        "description": "src.fjformer.core.utilities",
        "peekOfCode": "def vmap_all_but_one(f: Callable, axis: int, out_ndim: int = 0) -> Callable:\n    \"\"\"\n    Repeatedly calls vmap to map over all axes except for `axis`.\n    All args will be mapped on the same dimensions.\n    Args:\n        f: The function to be mapped.\n        axis: The axis to exclude from mapping.\n        out_ndim: The number of dimensions in the output.\n    Returns:\n        A wrapped function that applies vmap to all axes except the specified one.",
        "detail": "src.fjformer.core.utilities",
        "documentation": {}
    },
    {
        "label": "freeze_subtrees",
        "kind": 2,
        "importPath": "src.fjformer.core.utilities",
        "description": "src.fjformer.core.utilities",
        "peekOfCode": "def freeze_subtrees(\n    optimizer: optax.GradientTransformation,\n    label_fn: Callable[[Any], Dict[str, Any]],\n    use_scalar_zeros: bool = False,\n) -> optax.GradientTransformation:\n    \"\"\"\n    Wraps an optimizer such that subtrees specified by label_fn will receive zeros as updates.\n    Args:\n        optimizer: The original optimizer to be wrapped.\n        label_fn: A function that labels subtrees as \"freeze\" or \"train\".",
        "detail": "src.fjformer.core.utilities",
        "documentation": {}
    },
    {
        "label": "freeze_keys",
        "kind": 2,
        "importPath": "src.fjformer.core.utilities",
        "description": "src.fjformer.core.utilities",
        "peekOfCode": "def freeze_keys(\n    optimizer: optax.GradientTransformation,\n    arr_type: type,\n    keys: Union[List[str], Set[str]],\n    use_scalar_zeros: bool = False,\n) -> optax.GradientTransformation:\n    \"\"\"\n    Freezes specific keys in the optimizer.\n    Args:\n        optimizer: The original optimizer to be wrapped.",
        "detail": "src.fjformer.core.utilities",
        "documentation": {}
    },
    {
        "label": "apply_updates",
        "kind": 2,
        "importPath": "src.fjformer.core.utilities",
        "description": "src.fjformer.core.utilities",
        "peekOfCode": "def apply_updates(params: optax.Params, updates: optax.Updates) -> optax.Params:\n    \"\"\"\n    Applies updates to parameters, supporting SymbolicConstant instances.\n    Args:\n        params: The current parameters.\n        updates: The updates to be applied.\n    Returns:\n        The updated parameters.\n    Raises:\n        UpdateError: If there's an error during the update process.",
        "detail": "src.fjformer.core.utilities",
        "documentation": {}
    },
    {
        "label": "set_to_zero_scalar",
        "kind": 2,
        "importPath": "src.fjformer.core.utilities",
        "description": "src.fjformer.core.utilities",
        "peekOfCode": "def set_to_zero_scalar() -> optax.GradientTransformation:\n    \"\"\"\n    Returns a gradient transformation that sets all gradients to 0 to make downstream constant folding cheaper.\n    Returns:\n        A gradient transformation that sets all gradients to scalar zeros.\n    Raises:\n        OptimizerError: If there's an error during the transformation process.\n    \"\"\"\n    try:\n        def init_fn(params: Any) -> optax.EmptyState:",
        "detail": "src.fjformer.core.utilities",
        "documentation": {}
    },
    {
        "label": "global_norm",
        "kind": 2,
        "importPath": "src.fjformer.functions._functions",
        "description": "src.fjformer.functions._functions",
        "peekOfCode": "def global_norm(tree: chex.ArrayTree) -> chex.Array:\n    \"\"\"\n    Computes the global norm of a PyTree of arrays.\n    The global norm is the square root of the sum of squares of all elements in the PyTree.\n    Args:\n        tree: The PyTree of arrays.\n    Returns:\n        The global norm as a scalar JAX array.\n    \"\"\"\n    squared = jax.tree_util.tree_map(lambda x: jnp.sum(jnp.square(x)), tree)",
        "detail": "src.fjformer.functions._functions",
        "documentation": {}
    },
    {
        "label": "average_metrics",
        "kind": 2,
        "importPath": "src.fjformer.functions._functions",
        "description": "src.fjformer.functions._functions",
        "peekOfCode": "def average_metrics(metrics: list[chex.ArrayTree]) -> chex.ArrayTree:\n    \"\"\"\n    Averages a list of metrics, each represented as a PyTree.\n    This function is useful for averaging metrics across multiple devices or steps.\n    Args:\n        metrics: A list of PyTrees representing the metrics.\n    Returns:\n        A PyTree with the same structure as the input metrics, but with each leaf\n        node replaced by the average of the corresponding leaf nodes across the\n        input metrics.",
        "detail": "src.fjformer.functions._functions",
        "documentation": {}
    },
    {
        "label": "transpose",
        "kind": 2,
        "importPath": "src.fjformer.functions._functions",
        "description": "src.fjformer.functions._functions",
        "peekOfCode": "def transpose(array: chex.Array, dim0: int, dim1: int) -> chex.Array:\n    \"\"\"\n    Transposes two dimensions of a JAX array.\n    This function provides a convenient way to swap two dimensions of an array using\n    negative indexing for dimensions.\n    Args:\n        array: The input array.\n        dim0: The first dimension to transpose. Can be negative.\n        dim1: The second dimension to transpose. Can be negative.\n    Returns:",
        "detail": "src.fjformer.functions._functions",
        "documentation": {}
    },
    {
        "label": "fused_softmax",
        "kind": 2,
        "importPath": "src.fjformer.functions._functions",
        "description": "src.fjformer.functions._functions",
        "peekOfCode": "def fused_softmax(x: chex.Array, axis: int = -1) -> chex.Array:\n    \"\"\"\n    Computes a numerically stable softmax using a fused implementation.\n    This function computes the softmax by first calculating the log-softmax and\n    then exponentiating the result. This approach is more numerically stable than\n    directly computing the softmax.\n    Args:\n        x: The input array.\n        axis: The axis along which to compute the softmax.\n    Returns:",
        "detail": "src.fjformer.functions._functions",
        "documentation": {}
    },
    {
        "label": "SpecialLossNormalizingFactor",
        "kind": 6,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "class SpecialLossNormalizingFactor(enum.Enum):\n    \"\"\"\n    Specially calculated loss normalizing factors that are not constant.\n    Attributes:\n        NUM_REAL_TARGET_TOKENS: Divide the loss by the number of real (non-padding) tokens.\n        NUM_TOTAL_TARGET_TOKENS: Divide the loss by the total number of target tokens.\n        AVERAGE_PER_SEQUENCE: Compute the average loss per sequence.\n    \"\"\"\n    NUM_REAL_TARGET_TOKENS = 1\n    NUM_TOTAL_TARGET_TOKENS = 2",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "mse",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def mse(labels: Array, predictions: Array) -> float:\n    \"\"\"\n    Computes the mean squared error between two arrays.\n    Args:\n        labels: The true values.\n        predictions: The predicted values.\n    Returns:\n        The mean squared error (MSE) between the labels and predictions.\n    \"\"\"\n    return np.mean((labels - predictions) ** 2)",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "mae",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def mae(labels: Array, predictions: Array) -> float:\n    \"\"\"\n    Computes the mean absolute error between two arrays.\n    Args:\n        labels: The true values.\n        predictions: The predicted values.\n    Returns:\n        The mean absolute error (MAE) between the labels and predictions.\n    \"\"\"\n    return np.mean(np.abs(labels - predictions))",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "cross_entropy",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def cross_entropy(\n    labels: Array, predictions: Array, ignore_index: Optional[int] = None\n) -> float:\n    \"\"\"\n    Computes the cross-entropy loss between labels and predictions.\n    Args:\n        labels: The true class labels (integers).\n        predictions: The predicted class probabilities.\n        ignore_index: An optional index to ignore when computing the loss.\n    Returns:",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "binary_cross_entropy",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def binary_cross_entropy(labels: Array, predictions: Array) -> float:\n    \"\"\"\n    Computes the binary cross-entropy loss between labels and predictions.\n    Args:\n        labels: The true binary labels (0 or 1).\n        predictions: The predicted probabilities for the positive class.\n    Returns:\n        The binary cross-entropy loss.\n    \"\"\"\n    labels = jax.nn.one_hot(labels, predictions.shape[-1])",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "nll",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def nll(labels: Array, predictions: Array) -> float:\n    \"\"\"\n    Computes the negative log-likelihood loss.\n    Args:\n        labels: The true class labels (integers).\n        predictions: The predicted class probabilities.\n    Returns:\n        The negative log-likelihood loss.\n    \"\"\"\n    return -np.sum(labels * np.log(predictions + 1e-8))",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "l2",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def l2(labels: Array, predictions: Array) -> float:\n    \"\"\"\n    Computes the L2 loss (sum of squared differences).\n    Args:\n        labels: The true values.\n        predictions: The predicted values.\n    Returns:\n        The L2 loss.\n    \"\"\"\n    return np.sum((labels - predictions) ** 2)",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "hinge",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def hinge(labels: Array, predictions: Array) -> float:\n    \"\"\"\n    Computes the hinge loss, commonly used in SVMs.\n    Args:\n        labels: The true class labels (-1 or 1).\n        predictions: The predicted values.\n    Returns:\n        The hinge loss.\n    \"\"\"\n    return np.mean(np.maximum(0, 1 - labels * predictions))",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "log_cosh",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def log_cosh(labels: Array, predictions: Array) -> float:\n    \"\"\"\n    Computes the log-cosh loss, a smooth approximation of the MAE.\n    Args:\n        labels: The true values.\n        predictions: The predicted values.\n    Returns:\n        The log-cosh loss.\n    \"\"\"\n    def cosh(x):",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "binary_cross_entropy_onehot",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def binary_cross_entropy_onehot(labels: Array, predictions: Array) -> float:\n    \"\"\"\n    Computes the binary cross-entropy loss using one-hot encoded labels.\n    Args:\n        labels: The true class labels (one-hot encoded).\n        predictions: The predicted class probabilities.\n    Returns:\n        The binary cross-entropy loss.\n    \"\"\"\n    labels = jax.nn.one_hot(labels, predictions.shape[-1])",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "cross_entropy_onehot",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def cross_entropy_onehot(labels: Array, predictions: Array) -> float:\n    \"\"\"\n    Computes the cross-entropy loss using one-hot encoded labels.\n    Args:\n        labels: The true class labels (integers).\n        predictions: The predicted class probabilities.\n    Returns:\n        The cross-entropy loss.\n    \"\"\"\n    labels = jax.nn.one_hot(labels, predictions.shape[-1])",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "mse_loss",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def mse_loss(val: Array, target: Array, valid: Optional[Array] = None) -> float:\n    \"\"\"\n    Computes the mean squared error loss with optional masking.\n    Args:\n        val: The predicted values.\n        target: The true values.\n        valid: An optional mask to indicate valid positions.\n    Returns:\n        The masked mean squared error loss.\n    \"\"\"",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "cross_entropy_loss_and_accuracy",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def cross_entropy_loss_and_accuracy(\n    logits: chex.Array, tokens: chex.Array, valid: Optional[chex.Array] = None\n) -> Tuple[float, float]:\n    \"\"\"\n    Computes the cross-entropy loss and accuracy with optional masking.\n    Args:\n        logits: The predicted logits.\n        tokens: The true class labels (integers).\n        valid: An optional mask to indicate valid positions.\n    Returns:",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "fused_cross_entropy_loss_and_accuracy",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def fused_cross_entropy_loss_and_accuracy(\n    logits: chex.Array, tokens: chex.Array, valid: Optional[chex.Array] = None\n) -> Tuple[float, float]:\n    \"\"\"\n    Computes the cross-entropy loss and accuracy with optional masking (fused version).\n    Args:\n        logits: The predicted logits.\n        tokens: The true class labels (integers).\n        valid: An optional mask to indicate valid positions.\n    Returns:",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "cross_entropy_with_logits",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def cross_entropy_with_logits(\n    logits: jnp.ndarray, targets: jnp.ndarray, z_loss: float\n) -> Tuple[jnp.ndarray, jnp.ndarray]:\n    \"\"\"\n    Computes cross-entropy loss with a stable custom gradient.\n    This function computes the cross-entropy loss with an optional auxiliary loss\n    term (`z_loss`) to prevent logits from drifting too far from zero.\n    Args:\n        logits: The predicted logits.\n        targets: The one-hot encoded target labels.",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "compute_weighted_cross_entropy",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def compute_weighted_cross_entropy(\n    logits: jnp.ndarray,\n    targets: jnp.ndarray,\n    weights: Optional[jnp.ndarray] = None,\n    label_smoothing: float = 0.0,\n    z_loss: float = 0.0,\n    loss_normalizing_factor: Optional[float] = None,\n) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n    \"\"\"\n    Computes weighted cross-entropy loss, z-loss, and weight sum.",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "compute_weighted_cross_entropy_and_accuracy",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def compute_weighted_cross_entropy_and_accuracy(\n    logits: jnp.ndarray,\n    targets: jnp.ndarray,\n    weights: Optional[jnp.ndarray] = None,\n    label_smoothing: float = 0.0,\n    z_loss: float = 0.0,\n    loss_normalizing_factor: Optional[float] = None,\n) -> Tuple[jnp.ndarray, jnp.ndarray, jnp.ndarray, jnp.ndarray]:\n    \"\"\"\n    Computes weighted cross-entropy loss, z-loss, weight sum, and accuracy.",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "convert_special_loss_normalizing_factor_to_enum",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def convert_special_loss_normalizing_factor_to_enum(\n    x: str,\n) -> SpecialLossNormalizingFactor:\n    \"\"\"\n    Converts a stringified version of SpecialLossNormalizingFactor to an enum.\n    Args:\n        x: Stringified version of the enum value.\n    Returns:\n        The corresponding SpecialLossNormalizingFactor enum value.\n    \"\"\"",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "get_loss_normalizing_factor_and_weights",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def get_loss_normalizing_factor_and_weights(\n    loss_normalizing_factor: Optional[\n        Union[float, int, str, SpecialLossNormalizingFactor]\n    ],\n    batch: Mapping[str, jnp.ndarray],\n) -> Tuple[Optional[float], Optional[jnp.ndarray]]:\n    \"\"\"\n    Gets the loss normalizing factor and weights from a batch of data.\n    Args:\n        loss_normalizing_factor: The loss normalizing factor to use.",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "auxiliary_load_balancing_loss_func",
        "kind": 2,
        "importPath": "src.fjformer.functions.loss_functions",
        "description": "src.fjformer.functions.loss_functions",
        "peekOfCode": "def auxiliary_load_balancing_loss_func(\n    gate_logits: chex.Array,\n    num_experts: int,\n    top_k: int,\n    attention_mask: Optional[chex.Array] = None,\n) -> chex.Array:\n    \"\"\"Computes auxiliary load balancing loss as in Switch Transformer.\n    See Switch Transformer (https://arxiv.org/abs/2101.03961)\n    Args:\n        gate_logits: The logits for the gating network.",
        "detail": "src.fjformer.functions.loss_functions",
        "documentation": {}
    },
    {
        "label": "Key",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class Key(Hashable, Protocol):\n    def __lt__(self: K, value: K, /) -> bool:\n        ...\nPath = str\nPathParts = Tuple[Key, ...]\nLeaf = Any\n# Dense\nPrecisionLike = Union[\n    None,\n    str,",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "In",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class In(Generic[T]):\n    \"\"\"Specifies a variable collection should only be lifted as input.\"\"\"\n    axis: T\n@dataclasses.dataclass(frozen=True)\nclass Out(Generic[T]):\n    \"\"\"Specifies a variable collection should only be lifted as output.\"\"\"\n    axis: T\nAxis = Optional[int]\nInOutAxis = Union[Axis, In[Axis], Out[Axis]]\nScanAxis = int",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "Out",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class Out(Generic[T]):\n    \"\"\"Specifies a variable collection should only be lifted as output.\"\"\"\n    axis: T\nAxis = Optional[int]\nInOutAxis = Union[Axis, In[Axis], Out[Axis]]\nScanAxis = int\nInOutScanAxis = Union[ScanAxis, In[ScanAxis], Out[ScanAxis]]\nAxes = Union[int, Sequence[int]]\n# SPMD\nLogicalNames = Tuple[Union[str, None], ...]",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "Int8Params",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class Int8Params:\n    kernel: Array\n    scale: Array\n    min_vals: Array\n    def tree_flatten(self):\n        return (self.kernel, self.scale, self.min_vals), {}\n    @classmethod\n    def tree_unflatten(cls, aux, data):\n        kernel, scale, min_vals = data\n        return cls(",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "Dense",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class Dense(Module):\n    \"\"\"A linear transformation applied over the last dimension of the input.\n    Attributes:\n      features: the number of output features.\n      use_bias: whether to add a bias to the output (default: True).\n      dtype: the dtype of the computation (default: infer from input and params).\n      param_dtype: the dtype passed to parameter initializers (default: float32).\n      precision: numerical precision of the computation see `jax.lax.Precision`\n        for details.\n      kernel_init: initializer function for the weight matrix.",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "_Conv",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class _Conv(Module):\n    \"\"\"Convolution Module wrapping `lax.conv_general_dilated[_local]`.\n    Attributes:\n      features: number of convolution filters.\n      kernel_size: shape of the convolutional kernel.\n      strides: an integer or a sequence of `n` integers, representing the\n        inter-window strides (default: 1).\n      padding: either the string `\"SAME\"`, the string `\"VALID\"`, the string\n        `\"CIRCULAR\"` (periodic boundary conditions), or a sequence of `n` `(low,\n        high)` integer pairs that give the padding to apply before and after each",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "Conv",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class Conv(_Conv):\n    \"\"\"Convolution Module wrapping `lax.conv_general_dilated`.\n    Attributes:\n      features: number of convolution filters.\n      kernel_size: shape of the convolutional kernel.\n      strides: an integer or a sequence of `n` integers, representing the\n        inter-window strides (default: 1).\n      padding: either the string `\"SAME\"`, the string `\"VALID\"`, the string\n        `\"CIRCULAR\"` (periodic boundary conditions), or a sequence of `n` `(low,\n        high)` integer pairs that give the padding to apply before and after each",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "ConvLocal",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class ConvLocal(_Conv):\n    \"\"\"Local convolution Module wrapping `lax.conv_general_dilated_local`.\n    Attributes:\n      features: number of convolution filters.\n      kernel_size: shape of the convolutional kernel.\n      strides: an integer or a sequence of `n` integers, representing the\n        inter-window strides (default: 1).\n      padding: either the string `\"SAME\"`, the string `\"VALID\"`, the string\n        `\"CIRCULAR\"` (periodic boundary conditions), or a sequence of `n` `(low,\n        high)` integer pairs that give the padding to apply before and after each",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "ConvTranspose",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class ConvTranspose(Module):\n    \"\"\"Convolution Module wrapping lax.conv_transpose.\n    Attributes:\n      features: number of convolution filters.\n      kernel_size: shape of the convolutional kernel. For 1D convolution,\n        the kernel size can be passed as an integer. For all other cases, it must\n        be a sequence of integers.\n      strides: a sequence of `n` integers, representing the inter-window strides.\n      padding: either the string `\"SAME\"`, the string `\"VALID\"`, the string\n        `\"CIRCULAR\"` (periodic boundary conditions), or a sequence of `n` `(low,",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "Embed",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class Embed(Module):\n    \"\"\"Embedding Module.\n    A parameterized function from integers [0, n) to d-dimensional vectors.\n    Attributes:\n      num_embeddings: number of embeddings.\n      features: number of feature dimensions for each embedding.\n      dtype: the dtype of the embedding vectors (default: same as embedding).\n      param_dtype: the dtype passed to parameter initializers (default: float32).\n      embedding_init: embedding initializer.\n    \"\"\"",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "BatchNorm",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class BatchNorm(Module):\n    \"\"\"BatchNorm Module.\n    Usage Note:\n    If we define a model with BatchNorm, for example::\n      >>> import flax.linen as nn\n      >>> import jax, jax.numpy as jnp\n      >>> BN = nn.BatchNorm(momentum=0.9, epsilon=1e-5, dtype=jnp.float32)\n    The initialized variables dict will contain, in addition to a \"params\"\n    collection, a separate \"batch_stats\" collection that will contain all the\n    running statistics for all the BatchNorm layers in a model::",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class LayerNorm(Module):\n    epsilon: float = 1e-6\n    dtype: Optional[Dtype] = None\n    param_dtype: Dtype = jnp.float32\n    use_bias: bool = True\n    use_scale: bool = True\n    bias_init: Initializer = initializers.zeros\n    scale_init: Initializer = initializers.ones\n    reduction_axes: Axes = -1\n    feature_axes: Axes = -1",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class RMSNorm(Module):\n    \"\"\"RMS Layer normalization (https://arxiv.org/abs/1910.07467).\n    RMSNorm normalizes the activations of the layer for each given example in a\n    batch independently, rather than across a batch like Batch Normalization.\n    Unlike LayerNorm which re-centers the mean to be 0 and normalizes by the\n    standard deviation of the activations, RMSNorm does not re-center at all\n    and instead normalizes by the root mean square of the activations.\n    Example usage::\n      >>> import flax.linen as nn\n      >>> import jax",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "GroupNorm",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class GroupNorm(Module):\n    num_groups: Optional[int] = 32\n    group_size: Optional[int] = None\n    epsilon: float = 1e-6\n    dtype: Optional[Dtype] = None\n    param_dtype: Dtype = jnp.float32\n    use_bias: bool = True\n    use_scale: bool = True\n    bias_init: Initializer = initializers.zeros\n    scale_init: Initializer = initializers.ones",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "InstanceNorm",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class InstanceNorm(Module):\n    \"\"\"Instance normalization (https://arxiv.org/abs/1607.08022v3).\n    InstanceNorm normalizes the activations of the layer for each channel (rather\n    than across all channels like Layer Normalization), and for each given example\n    in a batch independently (rather than across an entire batch like Batch\n    Normalization). i.e. applies a transformation that maintains the mean activation\n    within each channel within each example close to 0 and the activation standard\n    deviation close to 1.\n    NOTE: This normalization operation is identical to LayerNorm and GroupNorm; the\n    difference is simply which axes are reduced and the shape of the feature axes",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "SpectralNorm",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class SpectralNorm(Module):\n    layer_instance: Module\n    n_steps: int = 1\n    epsilon: float = 1e-12\n    dtype: Optional[Dtype] = None\n    param_dtype: Dtype = jnp.float32\n    error_on_non_matrix: bool = False\n    collection_name: str = \"batch_stats\"\n    @compact\n    def __call__(self, *args, update_stats: bool, **kwargs):",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "WeightNorm",
        "kind": 6,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "class WeightNorm(Module):\n    layer_instance: Module\n    epsilon: float = 1e-12\n    dtype: Optional[Dtype] = None\n    param_dtype: Dtype = jnp.float32\n    use_scale: bool = True\n    scale_init: Initializer = initializers.ones\n    feature_axes: Optional[Axes] = -1\n    variable_filter: Optional[Iterable] = dataclasses.field(\n        default_factory=lambda: {\"kernel\"}",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "quantize",
        "kind": 2,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "def quantize(\n        array: Array,\n        axis: int = -1\n) -> Tuple[Array, Array, Array]:\n    min_vals = jnp.min(array, axis=axis, keepdims=True)\n    max_vals = jnp.max(array, axis=axis, keepdims=True)\n    # Compute the scaling factors\n    scale = (max_vals - min_vals) / (2 ** 7 - 1)\n    # Quantize the data\n    quantized_data = jnp.round((array - min_vals) / scale)",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "dequantize",
        "kind": 2,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "def dequantize(\n        array_quant: Array,\n        scale: Array,\n        min_vals: Array,\n        float_dtype: jnp.dtype = jnp.float16,\n):\n    return array_quant * scale + min_vals\n@jax.tree_util.register_pytree_node_class\n@dataclasses.dataclass\nclass Int8Params:",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "quantize_int8_parameters",
        "kind": 2,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "def quantize_int8_parameters(\n        filter_list_quantization: list,\n        params: dict,\n):\n    pattern = re.compile(\"({})\".format(\"|\".join(filter_list_quantization)))\n    def lam_func(path, array: Array):\n        if pattern.search(\"/\".join(p.key for p in path)):\n            array = Int8Params(*quantize(array))\n            return array\n        return array",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "dequantize_int8_parameters",
        "kind": 2,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "def dequantize_int8_parameters(\n        params: jax.tree_util.PyTreeDef,\n        dtype: jnp.dtype = jnp.float32,\n        shard_funcs: Optional[Mapping[str, Callable[[chex.Array], chex.Array]]] = None\n):\n    def _q(pr):\n        if isinstance(pr, Int8Params):\n            return jnp.array(dequantize(pr.kernel, pr.scale, pr.min_vals, dtype))\n        return pr\n    prm = flax.traverse_util.flatten_dict(params)",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "control_quantization",
        "kind": 2,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "def control_quantization(array, param_dtype):\n    if isinstance(array, Int8Params):\n        array = dequantize(\n            array.kernel,\n            array.scale,\n            array.min_vals,\n            param_dtype,\n        )\n    return array\nclass Dense(Module):",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "canonicalize_padding",
        "kind": 2,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "def canonicalize_padding(padding: PaddingLike, rank: int) -> LaxPadding:\n    \"\"\" \"Canonicalizes conv padding to a jax.lax supported format.\"\"\"\n    if isinstance(padding, str):\n        return padding\n    if isinstance(padding, int):\n        return [(padding, padding)] * rank\n    if isinstance(padding, Sequence) and len(padding) == rank:\n        new_pad = []\n        for p in padding:\n            if isinstance(p, int):",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "Array",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "Array = Union[jax.Array, Any]\nPRNGKey = jax.Array\nRNGSequences = Dict[str, PRNGKey]\nDtype = Union[jax.typing.DTypeLike, Any]\nShape = Sequence[int]\nK = TypeVar('K')\nclass Key(Hashable, Protocol):\n    def __lt__(self: K, value: K, /) -> bool:\n        ...\nPath = str",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "PRNGKey",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "PRNGKey = jax.Array\nRNGSequences = Dict[str, PRNGKey]\nDtype = Union[jax.typing.DTypeLike, Any]\nShape = Sequence[int]\nK = TypeVar('K')\nclass Key(Hashable, Protocol):\n    def __lt__(self: K, value: K, /) -> bool:\n        ...\nPath = str\nPathParts = Tuple[Key, ...]",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "RNGSequences",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "RNGSequences = Dict[str, PRNGKey]\nDtype = Union[jax.typing.DTypeLike, Any]\nShape = Sequence[int]\nK = TypeVar('K')\nclass Key(Hashable, Protocol):\n    def __lt__(self: K, value: K, /) -> bool:\n        ...\nPath = str\nPathParts = Tuple[Key, ...]\nLeaf = Any",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "Dtype",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "Dtype = Union[jax.typing.DTypeLike, Any]\nShape = Sequence[int]\nK = TypeVar('K')\nclass Key(Hashable, Protocol):\n    def __lt__(self: K, value: K, /) -> bool:\n        ...\nPath = str\nPathParts = Tuple[Key, ...]\nLeaf = Any\n# Dense",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "Shape",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "Shape = Sequence[int]\nK = TypeVar('K')\nclass Key(Hashable, Protocol):\n    def __lt__(self: K, value: K, /) -> bool:\n        ...\nPath = str\nPathParts = Tuple[Key, ...]\nLeaf = Any\n# Dense\nPrecisionLike = Union[",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "K",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "K = TypeVar('K')\nclass Key(Hashable, Protocol):\n    def __lt__(self: K, value: K, /) -> bool:\n        ...\nPath = str\nPathParts = Tuple[Key, ...]\nLeaf = Any\n# Dense\nPrecisionLike = Union[\n    None,",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "Path",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "Path = str\nPathParts = Tuple[Key, ...]\nLeaf = Any\n# Dense\nPrecisionLike = Union[\n    None,\n    str,\n    jax.lax.Precision,\n    Tuple[str, str],\n    Tuple[jax.lax.Precision, jax.lax.Precision],",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "PathParts",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "PathParts = Tuple[Key, ...]\nLeaf = Any\n# Dense\nPrecisionLike = Union[\n    None,\n    str,\n    jax.lax.Precision,\n    Tuple[str, str],\n    Tuple[jax.lax.Precision, jax.lax.Precision],\n]",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "Leaf",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "Leaf = Any\n# Dense\nPrecisionLike = Union[\n    None,\n    str,\n    jax.lax.Precision,\n    Tuple[str, str],\n    Tuple[jax.lax.Precision, jax.lax.Precision],\n]\nDotGeneralT = Callable[..., Array]",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "PrecisionLike",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "PrecisionLike = Union[\n    None,\n    str,\n    jax.lax.Precision,\n    Tuple[str, str],\n    Tuple[jax.lax.Precision, jax.lax.Precision],\n]\nDotGeneralT = Callable[..., Array]\nConvGeneralDilatedT = Callable[..., Array]\nPaddingLike = Union[str, int, Sequence[Union[int, Tuple[int, int]]]]",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "DotGeneralT",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "DotGeneralT = Callable[..., Array]\nConvGeneralDilatedT = Callable[..., Array]\nPaddingLike = Union[str, int, Sequence[Union[int, Tuple[int, int]]]]\nLaxPadding = Union[str, Sequence[Tuple[int, int]]]\n# Initializers\nInitializer = Union[jax.nn.initializers.Initializer, Callable[..., Any]]\n# Collections\nCollection = Mapping[str, Any]\nMutableCollection = Dict[str, Any]\n# Dicts",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "ConvGeneralDilatedT",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "ConvGeneralDilatedT = Callable[..., Array]\nPaddingLike = Union[str, int, Sequence[Union[int, Tuple[int, int]]]]\nLaxPadding = Union[str, Sequence[Tuple[int, int]]]\n# Initializers\nInitializer = Union[jax.nn.initializers.Initializer, Callable[..., Any]]\n# Collections\nCollection = Mapping[str, Any]\nMutableCollection = Dict[str, Any]\n# Dicts\nVariableDict = Mapping[str, Collection]",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "PaddingLike",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "PaddingLike = Union[str, int, Sequence[Union[int, Tuple[int, int]]]]\nLaxPadding = Union[str, Sequence[Tuple[int, int]]]\n# Initializers\nInitializer = Union[jax.nn.initializers.Initializer, Callable[..., Any]]\n# Collections\nCollection = Mapping[str, Any]\nMutableCollection = Dict[str, Any]\n# Dicts\nVariableDict = Mapping[str, Collection]\nFrozenVariableDict = FrozenDict[str, Collection]",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "LaxPadding",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "LaxPadding = Union[str, Sequence[Tuple[int, int]]]\n# Initializers\nInitializer = Union[jax.nn.initializers.Initializer, Callable[..., Any]]\n# Collections\nCollection = Mapping[str, Any]\nMutableCollection = Dict[str, Any]\n# Dicts\nVariableDict = Mapping[str, Collection]\nFrozenVariableDict = FrozenDict[str, Collection]\nMutableVariableDict = Dict[str, MutableCollection]",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "Initializer",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "Initializer = Union[jax.nn.initializers.Initializer, Callable[..., Any]]\n# Collections\nCollection = Mapping[str, Any]\nMutableCollection = Dict[str, Any]\n# Dicts\nVariableDict = Mapping[str, Collection]\nFrozenVariableDict = FrozenDict[str, Collection]\nMutableVariableDict = Dict[str, MutableCollection]\nPRNGFoldable = Union[int, str]\n# Axes",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "Collection",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "Collection = Mapping[str, Any]\nMutableCollection = Dict[str, Any]\n# Dicts\nVariableDict = Mapping[str, Collection]\nFrozenVariableDict = FrozenDict[str, Collection]\nMutableVariableDict = Dict[str, MutableCollection]\nPRNGFoldable = Union[int, str]\n# Axes\nT = TypeVar('T')\n@dataclasses.dataclass(frozen=True)",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "MutableCollection",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "MutableCollection = Dict[str, Any]\n# Dicts\nVariableDict = Mapping[str, Collection]\nFrozenVariableDict = FrozenDict[str, Collection]\nMutableVariableDict = Dict[str, MutableCollection]\nPRNGFoldable = Union[int, str]\n# Axes\nT = TypeVar('T')\n@dataclasses.dataclass(frozen=True)\nclass In(Generic[T]):",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "VariableDict",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "VariableDict = Mapping[str, Collection]\nFrozenVariableDict = FrozenDict[str, Collection]\nMutableVariableDict = Dict[str, MutableCollection]\nPRNGFoldable = Union[int, str]\n# Axes\nT = TypeVar('T')\n@dataclasses.dataclass(frozen=True)\nclass In(Generic[T]):\n    \"\"\"Specifies a variable collection should only be lifted as input.\"\"\"\n    axis: T",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "FrozenVariableDict",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "FrozenVariableDict = FrozenDict[str, Collection]\nMutableVariableDict = Dict[str, MutableCollection]\nPRNGFoldable = Union[int, str]\n# Axes\nT = TypeVar('T')\n@dataclasses.dataclass(frozen=True)\nclass In(Generic[T]):\n    \"\"\"Specifies a variable collection should only be lifted as input.\"\"\"\n    axis: T\n@dataclasses.dataclass(frozen=True)",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "MutableVariableDict",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "MutableVariableDict = Dict[str, MutableCollection]\nPRNGFoldable = Union[int, str]\n# Axes\nT = TypeVar('T')\n@dataclasses.dataclass(frozen=True)\nclass In(Generic[T]):\n    \"\"\"Specifies a variable collection should only be lifted as input.\"\"\"\n    axis: T\n@dataclasses.dataclass(frozen=True)\nclass Out(Generic[T]):",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "PRNGFoldable",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "PRNGFoldable = Union[int, str]\n# Axes\nT = TypeVar('T')\n@dataclasses.dataclass(frozen=True)\nclass In(Generic[T]):\n    \"\"\"Specifies a variable collection should only be lifted as input.\"\"\"\n    axis: T\n@dataclasses.dataclass(frozen=True)\nclass Out(Generic[T]):\n    \"\"\"Specifies a variable collection should only be lifted as output.\"\"\"",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "T",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "T = TypeVar('T')\n@dataclasses.dataclass(frozen=True)\nclass In(Generic[T]):\n    \"\"\"Specifies a variable collection should only be lifted as input.\"\"\"\n    axis: T\n@dataclasses.dataclass(frozen=True)\nclass Out(Generic[T]):\n    \"\"\"Specifies a variable collection should only be lifted as output.\"\"\"\n    axis: T\nAxis = Optional[int]",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "Axis",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "Axis = Optional[int]\nInOutAxis = Union[Axis, In[Axis], Out[Axis]]\nScanAxis = int\nInOutScanAxis = Union[ScanAxis, In[ScanAxis], Out[ScanAxis]]\nAxes = Union[int, Sequence[int]]\n# SPMD\nLogicalNames = Tuple[Union[str, None], ...]\n# Maps each logical axis  to physical mesh, can be either None (replicated),\n# one physical axis or a tuple of physical axes.\nLogicalRules = Sequence[Tuple[str, Union[str, Tuple[str, ...], None]]]",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "InOutAxis",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "InOutAxis = Union[Axis, In[Axis], Out[Axis]]\nScanAxis = int\nInOutScanAxis = Union[ScanAxis, In[ScanAxis], Out[ScanAxis]]\nAxes = Union[int, Sequence[int]]\n# SPMD\nLogicalNames = Tuple[Union[str, None], ...]\n# Maps each logical axis  to physical mesh, can be either None (replicated),\n# one physical axis or a tuple of physical axes.\nLogicalRules = Sequence[Tuple[str, Union[str, Tuple[str, ...], None]]]\nArrayPytree = Any  # pylint: disable=invalid-name",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "ScanAxis",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "ScanAxis = int\nInOutScanAxis = Union[ScanAxis, In[ScanAxis], Out[ScanAxis]]\nAxes = Union[int, Sequence[int]]\n# SPMD\nLogicalNames = Tuple[Union[str, None], ...]\n# Maps each logical axis  to physical mesh, can be either None (replicated),\n# one physical axis or a tuple of physical axes.\nLogicalRules = Sequence[Tuple[str, Union[str, Tuple[str, ...], None]]]\nArrayPytree = Any  # pylint: disable=invalid-name\nLogicalPartitionSpec = Any  # pylint: disable=invalid-name",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "InOutScanAxis",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "InOutScanAxis = Union[ScanAxis, In[ScanAxis], Out[ScanAxis]]\nAxes = Union[int, Sequence[int]]\n# SPMD\nLogicalNames = Tuple[Union[str, None], ...]\n# Maps each logical axis  to physical mesh, can be either None (replicated),\n# one physical axis or a tuple of physical axes.\nLogicalRules = Sequence[Tuple[str, Union[str, Tuple[str, ...], None]]]\nArrayPytree = Any  # pylint: disable=invalid-name\nLogicalPartitionSpec = Any  # pylint: disable=invalid-name\nLogicalPartitionSpecPytree = Any  # pylint: disable=invalid-name",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "Axes",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "Axes = Union[int, Sequence[int]]\n# SPMD\nLogicalNames = Tuple[Union[str, None], ...]\n# Maps each logical axis  to physical mesh, can be either None (replicated),\n# one physical axis or a tuple of physical axes.\nLogicalRules = Sequence[Tuple[str, Union[str, Tuple[str, ...], None]]]\nArrayPytree = Any  # pylint: disable=invalid-name\nLogicalPartitionSpec = Any  # pylint: disable=invalid-name\nLogicalPartitionSpecPytree = Any  # pylint: disable=invalid-name\nPartitionSpecPytree = Any  # pylint: disable=invalid-name",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "LogicalNames",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "LogicalNames = Tuple[Union[str, None], ...]\n# Maps each logical axis  to physical mesh, can be either None (replicated),\n# one physical axis or a tuple of physical axes.\nLogicalRules = Sequence[Tuple[str, Union[str, Tuple[str, ...], None]]]\nArrayPytree = Any  # pylint: disable=invalid-name\nLogicalPartitionSpec = Any  # pylint: disable=invalid-name\nLogicalPartitionSpecPytree = Any  # pylint: disable=invalid-name\nPartitionSpecPytree = Any  # pylint: disable=invalid-name\nSharding = Tuple[Optional[str], ...]\nfield = dataclasses.field",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "LogicalRules",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "LogicalRules = Sequence[Tuple[str, Union[str, Tuple[str, ...], None]]]\nArrayPytree = Any  # pylint: disable=invalid-name\nLogicalPartitionSpec = Any  # pylint: disable=invalid-name\nLogicalPartitionSpecPytree = Any  # pylint: disable=invalid-name\nPartitionSpecPytree = Any  # pylint: disable=invalid-name\nSharding = Tuple[Optional[str], ...]\nfield = dataclasses.field\ncanonicalize_dtype = dtypes.canonicalize_dtype\nmerge_param = module.merge_param\nmap_variables = transforms.map_variables",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "ArrayPytree",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "ArrayPytree = Any  # pylint: disable=invalid-name\nLogicalPartitionSpec = Any  # pylint: disable=invalid-name\nLogicalPartitionSpecPytree = Any  # pylint: disable=invalid-name\nPartitionSpecPytree = Any  # pylint: disable=invalid-name\nSharding = Tuple[Optional[str], ...]\nfield = dataclasses.field\ncanonicalize_dtype = dtypes.canonicalize_dtype\nmerge_param = module.merge_param\nmap_variables = transforms.map_variables\ndefault_kernel_init = initializers.lecun_normal()",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "LogicalPartitionSpec",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "LogicalPartitionSpec = Any  # pylint: disable=invalid-name\nLogicalPartitionSpecPytree = Any  # pylint: disable=invalid-name\nPartitionSpecPytree = Any  # pylint: disable=invalid-name\nSharding = Tuple[Optional[str], ...]\nfield = dataclasses.field\ncanonicalize_dtype = dtypes.canonicalize_dtype\nmerge_param = module.merge_param\nmap_variables = transforms.map_variables\ndefault_kernel_init = initializers.lecun_normal()\ndef quantize(",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "LogicalPartitionSpecPytree",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "LogicalPartitionSpecPytree = Any  # pylint: disable=invalid-name\nPartitionSpecPytree = Any  # pylint: disable=invalid-name\nSharding = Tuple[Optional[str], ...]\nfield = dataclasses.field\ncanonicalize_dtype = dtypes.canonicalize_dtype\nmerge_param = module.merge_param\nmap_variables = transforms.map_variables\ndefault_kernel_init = initializers.lecun_normal()\ndef quantize(\n        array: Array,",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "PartitionSpecPytree",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "PartitionSpecPytree = Any  # pylint: disable=invalid-name\nSharding = Tuple[Optional[str], ...]\nfield = dataclasses.field\ncanonicalize_dtype = dtypes.canonicalize_dtype\nmerge_param = module.merge_param\nmap_variables = transforms.map_variables\ndefault_kernel_init = initializers.lecun_normal()\ndef quantize(\n        array: Array,\n        axis: int = -1",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "Sharding",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "Sharding = Tuple[Optional[str], ...]\nfield = dataclasses.field\ncanonicalize_dtype = dtypes.canonicalize_dtype\nmerge_param = module.merge_param\nmap_variables = transforms.map_variables\ndefault_kernel_init = initializers.lecun_normal()\ndef quantize(\n        array: Array,\n        axis: int = -1\n) -> Tuple[Array, Array, Array]:",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "field",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "field = dataclasses.field\ncanonicalize_dtype = dtypes.canonicalize_dtype\nmerge_param = module.merge_param\nmap_variables = transforms.map_variables\ndefault_kernel_init = initializers.lecun_normal()\ndef quantize(\n        array: Array,\n        axis: int = -1\n) -> Tuple[Array, Array, Array]:\n    min_vals = jnp.min(array, axis=axis, keepdims=True)",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "canonicalize_dtype",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "canonicalize_dtype = dtypes.canonicalize_dtype\nmerge_param = module.merge_param\nmap_variables = transforms.map_variables\ndefault_kernel_init = initializers.lecun_normal()\ndef quantize(\n        array: Array,\n        axis: int = -1\n) -> Tuple[Array, Array, Array]:\n    min_vals = jnp.min(array, axis=axis, keepdims=True)\n    max_vals = jnp.max(array, axis=axis, keepdims=True)",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "merge_param",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "merge_param = module.merge_param\nmap_variables = transforms.map_variables\ndefault_kernel_init = initializers.lecun_normal()\ndef quantize(\n        array: Array,\n        axis: int = -1\n) -> Tuple[Array, Array, Array]:\n    min_vals = jnp.min(array, axis=axis, keepdims=True)\n    max_vals = jnp.max(array, axis=axis, keepdims=True)\n    # Compute the scaling factors",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "map_variables",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "map_variables = transforms.map_variables\ndefault_kernel_init = initializers.lecun_normal()\ndef quantize(\n        array: Array,\n        axis: int = -1\n) -> Tuple[Array, Array, Array]:\n    min_vals = jnp.min(array, axis=axis, keepdims=True)\n    max_vals = jnp.max(array, axis=axis, keepdims=True)\n    # Compute the scaling factors\n    scale = (max_vals - min_vals) / (2 ** 7 - 1)",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "default_kernel_init",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "default_kernel_init = initializers.lecun_normal()\ndef quantize(\n        array: Array,\n        axis: int = -1\n) -> Tuple[Array, Array, Array]:\n    min_vals = jnp.min(array, axis=axis, keepdims=True)\n    max_vals = jnp.max(array, axis=axis, keepdims=True)\n    # Compute the scaling factors\n    scale = (max_vals - min_vals) / (2 ** 7 - 1)\n    # Quantize the data",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "default_embed_init",
        "kind": 5,
        "importPath": "src.fjformer.linen.linen",
        "description": "src.fjformer.linen.linen",
        "peekOfCode": "default_embed_init = initializers.variance_scaling(\n    1.0, \"fan_in\", \"normal\", out_axis=0\n)\nclass Embed(Module):\n    \"\"\"Embedding Module.\n    A parameterized function from integers [0, n) to d-dimensional vectors.\n    Attributes:\n      num_embeddings: number of embeddings.\n      features: number of feature dimensions for each embedding.\n      dtype: the dtype of the embedding vectors (default: same as embedding).",
        "detail": "src.fjformer.linen.linen",
        "documentation": {}
    },
    {
        "label": "LoraError",
        "kind": 6,
        "importPath": "src.fjformer.lora.lora_core",
        "description": "src.fjformer.lora.lora_core",
        "peekOfCode": "class LoraError(Exception):\n    \"\"\"Base exception for LoRA-related errors.\"\"\"\n    pass\nclass UnsupportedOperationError(LoraError):\n    \"\"\"Raised when an unsupported operation is encountered.\"\"\"\n    pass\ndef lora(f: Any) -> Any:\n    \"\"\"Decorator for LoRA-compatible functions.\"\"\"\n    return cr.use_implicit_args(f)\n@dataclass",
        "detail": "src.fjformer.lora.lora_core",
        "documentation": {}
    },
    {
        "label": "UnsupportedOperationError",
        "kind": 6,
        "importPath": "src.fjformer.lora.lora_core",
        "description": "src.fjformer.lora.lora_core",
        "peekOfCode": "class UnsupportedOperationError(LoraError):\n    \"\"\"Raised when an unsupported operation is encountered.\"\"\"\n    pass\ndef lora(f: Any) -> Any:\n    \"\"\"Decorator for LoRA-compatible functions.\"\"\"\n    return cr.use_implicit_args(f)\n@dataclass\nclass LoraWeight(cr.ImplicitArray):\n    \"\"\"Represents a LoRA (Low-Rank Adaptation) weight.\"\"\"\n    w: cr.ArrayValue  # M x N (2D)",
        "detail": "src.fjformer.lora.lora_core",
        "documentation": {}
    },
    {
        "label": "LoraWeight",
        "kind": 6,
        "importPath": "src.fjformer.lora.lora_core",
        "description": "src.fjformer.lora.lora_core",
        "peekOfCode": "class LoraWeight(cr.ImplicitArray):\n    \"\"\"Represents a LoRA (Low-Rank Adaptation) weight.\"\"\"\n    w: cr.ArrayValue  # M x N (2D)\n    a: cr.ArrayValue  # k x N (2D)\n    b: cr.ArrayValue  # M x k (2D)\n    alpha: float = cr.aux_field(default=1.00)\n    def __post_init__(self) -> None:\n        super().__post_init__()\n        if not (\n            self.a.shape[-2] == self.b.shape[-1]",
        "detail": "src.fjformer.lora.lora_core",
        "documentation": {}
    },
    {
        "label": "lora",
        "kind": 2,
        "importPath": "src.fjformer.lora.lora_core",
        "description": "src.fjformer.lora.lora_core",
        "peekOfCode": "def lora(f: Any) -> Any:\n    \"\"\"Decorator for LoRA-compatible functions.\"\"\"\n    return cr.use_implicit_args(f)\n@dataclass\nclass LoraWeight(cr.ImplicitArray):\n    \"\"\"Represents a LoRA (Low-Rank Adaptation) weight.\"\"\"\n    w: cr.ArrayValue  # M x N (2D)\n    a: cr.ArrayValue  # k x N (2D)\n    b: cr.ArrayValue  # M x k (2D)\n    alpha: float = cr.aux_field(default=1.00)",
        "detail": "src.fjformer.lora.lora_core",
        "documentation": {}
    },
    {
        "label": "handle_dot_lhs",
        "kind": 2,
        "importPath": "src.fjformer.lora.lora_core",
        "description": "src.fjformer.lora.lora_core",
        "peekOfCode": "def handle_dot_lhs(\n    primitive: Any,\n    lora: LoraWeight,\n    rhs: cr.ArrayValue,\n    *,\n    dimension_numbers: Any,\n    **kwargs: Any,\n) -> Union[jnp.ndarray, Any]:\n    \"\"\"Handle dot product when LoraWeight is on the left-hand side.\"\"\"\n    if not _check_dot_dimension_numbers(dimension_numbers):",
        "detail": "src.fjformer.lora.lora_core",
        "documentation": {}
    },
    {
        "label": "handle_dot_rhs",
        "kind": 2,
        "importPath": "src.fjformer.lora.lora_core",
        "description": "src.fjformer.lora.lora_core",
        "peekOfCode": "def handle_dot_rhs(\n    primitive: Any,\n    lhs: jax.Array,\n    lora: LoraWeight,\n    *,\n    dimension_numbers: Any,\n    **kwargs: Any,\n) -> Union[jnp.ndarray, Any]:\n    \"\"\"Handle dot product when LoraWeight is on the right-hand side.\"\"\"\n    if not _check_dot_dimension_numbers(dimension_numbers):",
        "detail": "src.fjformer.lora.lora_core",
        "documentation": {}
    },
    {
        "label": "handle_conv",
        "kind": 2,
        "importPath": "src.fjformer.lora.lora_core",
        "description": "src.fjformer.lora.lora_core",
        "peekOfCode": "def handle_conv(\n    primitive: Any,\n    inp: cr.ArrayValue,\n    lora: LoraWeight,\n    *,\n    dimension_numbers: Any,\n    **params: Any,\n) -> jnp.ndarray:\n    \"\"\"Handle convolution with LoraWeight.\"\"\"\n    if isinstance(inp, LoraWeight):",
        "detail": "src.fjformer.lora.lora_core",
        "documentation": {}
    },
    {
        "label": "handle_gather",
        "kind": 2,
        "importPath": "src.fjformer.lora.lora_core",
        "description": "src.fjformer.lora.lora_core",
        "peekOfCode": "def handle_gather(\n    primitive: Any,\n    lora: LoraWeight,\n    indices: jax.Array,\n    *,\n    dimension_numbers: Any,\n    slice_sizes: Tuple[int, ...],\n    **params: Any,\n) -> Union[jnp.ndarray, Any]:\n    \"\"\"Handle gather operation with LoraWeight.\"\"\"",
        "detail": "src.fjformer.lora.lora_core",
        "documentation": {}
    },
    {
        "label": "eval_lora_transpose",
        "kind": 2,
        "importPath": "src.fjformer.lora.lora_core",
        "description": "src.fjformer.lora.lora_core",
        "peekOfCode": "def eval_lora_transpose(\n    primitive: Any, arg: LoraWeight, *, permutation: Tuple[int, ...]\n) -> Union[LoraWeight, Any]:\n    \"\"\"Handle transpose operation for LoraWeight.\"\"\"\n    if not (len(arg.shape) == 2 and permutation == (1, 0)):\n        return NotImplemented\n    return LoraWeight(\n        w=arg.w.T,\n        a=arg.b.T,\n        b=arg.a.T,",
        "detail": "src.fjformer.lora.lora_core",
        "documentation": {}
    },
    {
        "label": "eval_lora_convert_element_type",
        "kind": 2,
        "importPath": "src.fjformer.lora.lora_core",
        "description": "src.fjformer.lora.lora_core",
        "peekOfCode": "def eval_lora_convert_element_type(\n    primitive: Any, arg: LoraWeight, **params: Any\n) -> LoraWeight:\n    \"\"\"Handle element type conversion for LoraWeight.\"\"\"\n    result = jax.tree_map(partial(cr.default_handler, primitive, **params), arg)\n    result.dtype = params[\"new_dtype\"]\n    return result",
        "detail": "src.fjformer.lora.lora_core",
        "documentation": {}
    },
    {
        "label": "RaptureConfig",
        "kind": 6,
        "importPath": "src.fjformer.lora.rapture",
        "description": "src.fjformer.lora.rapture",
        "peekOfCode": "class RaptureConfig:\n    \"\"\"Configuration for the RaptureConfig fine-tuning method.\n    Attributes:\n        lora_dim: Dimensionality of LoRA adapters.\n        fully_fine_tune_parameters: Optional list of parameter names to be fully fine-tuned.\n        lora_fine_tune_parameters: Optional list of parameter names to be fine-tuned with LoRA.\n        tune_vectors: Whether to tune vector parameters (length 1).\n        verbose: If True, logs information about parameter tuning decisions.\n        dtype: The dtype of the LoRA parameters.\n    \"\"\"",
        "detail": "src.fjformer.lora.rapture",
        "documentation": {}
    },
    {
        "label": "RaptureModule",
        "kind": 6,
        "importPath": "src.fjformer.lora.rapture",
        "description": "src.fjformer.lora.rapture",
        "peekOfCode": "class RaptureModule:\n    \"\"\"Container for data related to a Flax module adapted with RaptureConfig.\n    Attributes:\n        lora_opt_state: The optimizer state for the LoRA parameters.\n        lora_parameters: The PyTree of LoRA parameters.\n        lora_module: The original Flax module adapted for use with LoRA parameters.\n        lora_specs: A PyTree mirroring the parameter structure, indicating the tuning strategy for each parameter.\n        lora_tx: The Optax gradient transformation wrapped for LoRA.\n    \"\"\"\n    lora_opt_state: Union[jax.tree_util.PyTreeDef, dict]",
        "detail": "src.fjformer.lora.rapture",
        "documentation": {}
    },
    {
        "label": "LoraRapture",
        "kind": 6,
        "importPath": "src.fjformer.lora.rapture",
        "description": "src.fjformer.lora.rapture",
        "peekOfCode": "class LoraRapture:\n    \"\"\"Implements the RaptureConfig fine-tuning method.\n    RaptureConfig combines LoRA (Low-Rank Adaptation) with full fine-tuning,\n    allowing for efficient adaptation of large language models.\n    \"\"\"\n    def __init__(self, config: RaptureConfig):\n        \"\"\"Initializes RaptureConfig with the provided configuration.\n        Args:\n            config: An `RaptureConfig` object specifying the fine-tuning setup.\n        \"\"\"",
        "detail": "src.fjformer.lora.rapture",
        "documentation": {}
    },
    {
        "label": "split_lora_params",
        "kind": 2,
        "importPath": "src.fjformer.lora.rapture",
        "description": "src.fjformer.lora.rapture",
        "peekOfCode": "def split_lora_params(\n    params: Union[dict, jax.tree_util.PyTreeDef],\n    lora_spec: Union[dict, jax.tree_util.PyTreeDef],\n) -> Union[dict, jax.tree_util.PyTreeDef]:\n    \"\"\"Replaces specific parameters in a PyTree with `EmptyNode`.\n    This function processes a PyTree of parameters, replacing `LoraWeight.w`\n    values and parameters marked with `LORA_FREEZE` in the `lora_spec` with\n    `EmptyNode` instances. This is particularly useful for checkpointing,\n    allowing the saving of only trainable parameters.\n    Args:",
        "detail": "src.fjformer.lora.rapture",
        "documentation": {}
    },
    {
        "label": "LORA_FREEZE",
        "kind": 5,
        "importPath": "src.fjformer.lora.rapture",
        "description": "src.fjformer.lora.rapture",
        "peekOfCode": "LORA_FREEZE = 0\nLORA_FULL = -1\nlogger = get_logger(__name__)\ndef split_lora_params(\n    params: Union[dict, jax.tree_util.PyTreeDef],\n    lora_spec: Union[dict, jax.tree_util.PyTreeDef],\n) -> Union[dict, jax.tree_util.PyTreeDef]:\n    \"\"\"Replaces specific parameters in a PyTree with `EmptyNode`.\n    This function processes a PyTree of parameters, replacing `LoraWeight.w`\n    values and parameters marked with `LORA_FREEZE` in the `lora_spec` with",
        "detail": "src.fjformer.lora.rapture",
        "documentation": {}
    },
    {
        "label": "LORA_FULL",
        "kind": 5,
        "importPath": "src.fjformer.lora.rapture",
        "description": "src.fjformer.lora.rapture",
        "peekOfCode": "LORA_FULL = -1\nlogger = get_logger(__name__)\ndef split_lora_params(\n    params: Union[dict, jax.tree_util.PyTreeDef],\n    lora_spec: Union[dict, jax.tree_util.PyTreeDef],\n) -> Union[dict, jax.tree_util.PyTreeDef]:\n    \"\"\"Replaces specific parameters in a PyTree with `EmptyNode`.\n    This function processes a PyTree of parameters, replacing `LoraWeight.w`\n    values and parameters marked with `LORA_FREEZE` in the `lora_spec` with\n    `EmptyNode` instances. This is particularly useful for checkpointing,",
        "detail": "src.fjformer.lora.rapture",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "src.fjformer.lora.rapture",
        "description": "src.fjformer.lora.rapture",
        "peekOfCode": "logger = get_logger(__name__)\ndef split_lora_params(\n    params: Union[dict, jax.tree_util.PyTreeDef],\n    lora_spec: Union[dict, jax.tree_util.PyTreeDef],\n) -> Union[dict, jax.tree_util.PyTreeDef]:\n    \"\"\"Replaces specific parameters in a PyTree with `EmptyNode`.\n    This function processes a PyTree of parameters, replacing `LoraWeight.w`\n    values and parameters marked with `LORA_FREEZE` in the `lora_spec` with\n    `EmptyNode` instances. This is particularly useful for checkpointing,\n    allowing the saving of only trainable parameters.",
        "detail": "src.fjformer.lora.rapture",
        "documentation": {}
    },
    {
        "label": "is_notebook",
        "kind": 2,
        "importPath": "src.fjformer.monitor.tracker",
        "description": "src.fjformer.monitor.tracker",
        "peekOfCode": "def is_notebook() -> bool:\n    \"\"\"Returns True if the code is being run in a notebook, False otherwise.\"\"\"\n    return os.environ.get(\"IPYTHON\") is not None\ndef run(\n    note_book: Optional[bool] = None,\n    interval: float = 1,\n    dir_prefix: str = \"/dev/shm\",\n    display_output: bool = True,\n) -> None:\n    \"\"\"Periodically prints JAX memory usage information.",
        "detail": "src.fjformer.monitor.tracker",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "src.fjformer.monitor.tracker",
        "description": "src.fjformer.monitor.tracker",
        "peekOfCode": "def run(\n    note_book: Optional[bool] = None,\n    interval: float = 1,\n    dir_prefix: str = \"/dev/shm\",\n    display_output: bool = True,\n) -> None:\n    \"\"\"Periodically prints JAX memory usage information.\n    Runs the `go tool pprof` command in a loop, capturing its output and either\n    printing it to stdout or displaying it in a Jupyter notebook cell.\n    Args:",
        "detail": "src.fjformer.monitor.tracker",
        "documentation": {}
    },
    {
        "label": "get_memory_information",
        "kind": 2,
        "importPath": "src.fjformer.monitor.tracker",
        "description": "src.fjformer.monitor.tracker",
        "peekOfCode": "def get_memory_information(dir_prefix: str = \"/dev/shm\") -> str:\n    \"\"\"Retrieves JAX memory usage information using `go tool pprof`.\n    Args:\n        dir_prefix: The directory where the memory profile is stored.\n    Returns:\n        The output of the `go tool pprof` command as a string.\n    \"\"\"\n    return subprocess.run(\n        args=[\"go\", \"tool\", \"pprof\", \"-tags\", f\"{dir_prefix}/memory.prof\"],\n        stdout=subprocess.PIPE,",
        "detail": "src.fjformer.monitor.tracker",
        "documentation": {}
    },
    {
        "label": "initialise_tracking",
        "kind": 2,
        "importPath": "src.fjformer.monitor.tracker",
        "description": "src.fjformer.monitor.tracker",
        "peekOfCode": "def initialise_tracking(interval: float = 1.0, dir_prefix: str = \"/dev/shm\") -> None:\n    \"\"\"Starts a daemon thread to periodically save the memory profile to disk.\n    This function starts a background thread that continuously saves the JAX\n    memory profile to a file. It ensures atomicity of the file update using\n    `posix.rename` if available, otherwise falls back to a simple rename.\n    Args:\n        interval: The time interval (in seconds) between memory profile saves.\n        dir_prefix: The directory to store the memory profile.\n    \"\"\"\n    def _save_profile():",
        "detail": "src.fjformer.monitor.tracker",
        "documentation": {}
    },
    {
        "label": "threaded_log",
        "kind": 2,
        "importPath": "src.fjformer.monitor.tracker",
        "description": "src.fjformer.monitor.tracker",
        "peekOfCode": "def threaded_log(\n    interval: float = 1.0, dir_prefix: str = \"/dev/shm\", save_mem_json: bool = False\n) -> threading.Thread:\n    \"\"\"Starts a thread to periodically log memory information.\n    This function launches a background thread that continuously monitors and\n    either prints or displays the JAX memory usage information. Optionally,\n    it can also save the memory information to a JSON file.\n    Args:\n        interval: The time interval (in seconds) between memory logs.\n        dir_prefix: The directory to save the memory information JSON file.",
        "detail": "src.fjformer.monitor.tracker",
        "documentation": {}
    },
    {
        "label": "get_capacity_matrix",
        "kind": 2,
        "importPath": "src.fjformer.monitor.tracker",
        "description": "src.fjformer.monitor.tracker",
        "peekOfCode": "def get_capacity_matrix(dir_prefix: str = \"/dev/shm\") -> Dict[str, Dict[str, Any]]:\n    \"\"\"Parses memory information and returns a dictionary with capacity details.\n    This function extracts memory usage information from the output of the\n    `go tool pprof` command and structures it into a dictionary. The dictionary\n    maps memory pool names to their usage details, including used memory,\n    usage percentage, process information, and full capacity.\n    Args:\n        dir_prefix: The directory where the memory profile is stored.\n    Returns:\n        A dictionary containing memory capacity information.",
        "detail": "src.fjformer.monitor.tracker",
        "documentation": {}
    },
    {
        "label": "get_adafactor_with_linear_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.adafactor",
        "description": "src.fjformer.optimizers.adafactor",
        "peekOfCode": "def get_adafactor_with_linear_scheduler(\n    steps: int,\n    learning_rate_start: float = 5e-5,\n    learning_rate_end: float = 1e-5,\n    weight_decay: float = 1e-1,\n    min_dim_size_to_factor: int = 128,\n    decay_rate: float = 0.8,\n    decay_offset: int = 0,\n    multiply_by_parameter_scale: bool = True,\n    clipping_threshold: Optional[float] = 1.0,",
        "detail": "src.fjformer.optimizers.adafactor",
        "documentation": {}
    },
    {
        "label": "get_adafactor_with_warmup_linear_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.adafactor",
        "description": "src.fjformer.optimizers.adafactor",
        "peekOfCode": "def get_adafactor_with_warmup_linear_scheduler(\n    steps: int,\n    learning_rate_start: float = 5e-5,\n    learning_rate_end: float = 1e-5,\n    warmup_steps: int = 500,\n    min_dim_size_to_factor: int = 128,\n    decay_rate: float = 0.8,\n    decay_offset: int = 0,\n    multiply_by_parameter_scale: bool = True,\n    clipping_threshold: Optional[float] = 1.0,",
        "detail": "src.fjformer.optimizers.adafactor",
        "documentation": {}
    },
    {
        "label": "get_adafactor_with_cosine_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.adafactor",
        "description": "src.fjformer.optimizers.adafactor",
        "peekOfCode": "def get_adafactor_with_cosine_scheduler(\n    steps: int,\n    learning_rate: float = 5e-5,\n    min_dim_size_to_factor: int = 128,\n    decay_rate: float = 0.8,\n    decay_offset: int = 0,\n    multiply_by_parameter_scale: bool = True,\n    clipping_threshold: Optional[float] = 1.0,\n    momentum: Optional[float] = None,\n    dtype_momentum: chex.ArrayDType = jnp.float32,",
        "detail": "src.fjformer.optimizers.adafactor",
        "documentation": {}
    },
    {
        "label": "get_adafactor_with_warmup_cosine_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.adafactor",
        "description": "src.fjformer.optimizers.adafactor",
        "peekOfCode": "def get_adafactor_with_warmup_cosine_scheduler(\n    steps: int,\n    learning_rate: float = 5e-5,\n    learning_rate_end: float = 1e-5,\n    weight_decay: float = 1e-1,\n    min_dim_size_to_factor: int = 128,\n    decay_rate: float = 0.8,\n    decay_offset: int = 0,\n    multiply_by_parameter_scale: bool = True,\n    clipping_threshold: Optional[float] = 1.0,",
        "detail": "src.fjformer.optimizers.adafactor",
        "documentation": {}
    },
    {
        "label": "get_adamw_with_cosine_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.adamw",
        "description": "src.fjformer.optimizers.adamw",
        "peekOfCode": "def get_adamw_with_cosine_scheduler(\n    steps: int,\n    learning_rate: float = 5e-5,\n    b1: float = 0.9,\n    b2: float = 0.999,\n    eps: float = 1e-8,\n    eps_root: float = 0.0,\n    weight_decay: float = 1e-1,\n    gradient_accumulation_steps: int = 1,\n    mu_dtype: Optional[chex.ArrayDType] = None,",
        "detail": "src.fjformer.optimizers.adamw",
        "documentation": {}
    },
    {
        "label": "get_adamw_with_linear_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.adamw",
        "description": "src.fjformer.optimizers.adamw",
        "peekOfCode": "def get_adamw_with_linear_scheduler(\n    steps: int,\n    learning_rate_start: float = 5e-5,\n    learning_rate_end: float = 1e-5,\n    b1: float = 0.9,\n    b2: float = 0.999,\n    eps: float = 1e-8,\n    eps_root: float = 0.0,\n    weight_decay: float = 1e-1,\n    gradient_accumulation_steps: int = 1,",
        "detail": "src.fjformer.optimizers.adamw",
        "documentation": {}
    },
    {
        "label": "get_adamw_with_warmup_linear_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.adamw",
        "description": "src.fjformer.optimizers.adamw",
        "peekOfCode": "def get_adamw_with_warmup_linear_scheduler(\n    steps: int,\n    learning_rate_start: float = 5e-5,\n    learning_rate_end: float = 1e-5,\n    b1: float = 0.9,\n    b2: float = 0.999,\n    eps: float = 1e-8,\n    eps_root: float = 0.0,\n    weight_decay: float = 1e-1,\n    gradient_accumulation_steps: int = 1,",
        "detail": "src.fjformer.optimizers.adamw",
        "documentation": {}
    },
    {
        "label": "get_adamw_with_warmup_cosine_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.adamw",
        "description": "src.fjformer.optimizers.adamw",
        "peekOfCode": "def get_adamw_with_warmup_cosine_scheduler(\n    steps: int,\n    learning_rate: float = 5e-5,\n    learning_rate_end: float = 1e-5,\n    b1: float = 0.9,\n    b2: float = 0.999,\n    eps: float = 1e-8,\n    eps_root: float = 0.0,\n    weight_decay: float = 1e-1,\n    exponent: float = 1.0,",
        "detail": "src.fjformer.optimizers.adamw",
        "documentation": {}
    },
    {
        "label": "get_lion_with_linear_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.lion",
        "description": "src.fjformer.optimizers.lion",
        "peekOfCode": "def get_lion_with_linear_scheduler(\n    steps: int,\n    learning_rate_start: float = 5e-5,\n    learning_rate_end: float = 1e-5,\n    b1: float = 0.9,\n    b2: float = 0.99,\n    gradient_accumulation_steps: int = 1,\n    mu_dtype: Optional[chex.ArrayDType] = None,\n) -> Tuple[optax.GradientTransformation, optax.Schedule]:\n    \"\"\"",
        "detail": "src.fjformer.optimizers.lion",
        "documentation": {}
    },
    {
        "label": "get_lion_with_warmup_linear_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.lion",
        "description": "src.fjformer.optimizers.lion",
        "peekOfCode": "def get_lion_with_warmup_linear_scheduler(\n    steps: int,\n    learning_rate_start: float = 5e-5,\n    learning_rate_end: float = 1e-5,\n    b1: float = 0.9,\n    b2: float = 0.99,\n    gradient_accumulation_steps: int = 1,\n    mu_dtype: Optional[chex.ArrayDType] = None,\n    warmup_steps: int = 500,\n    warmup_init_value: float = 5e-8,",
        "detail": "src.fjformer.optimizers.lion",
        "documentation": {}
    },
    {
        "label": "get_lion_with_cosine_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.lion",
        "description": "src.fjformer.optimizers.lion",
        "peekOfCode": "def get_lion_with_cosine_scheduler(\n    steps: int,\n    learning_rate: float = 5e-5,\n    alpha: float = 0.0,\n    exponent: float = 1.0,\n    b1: float = 0.9,\n    b2: float = 0.99,\n    gradient_accumulation_steps: int = 1,\n    mu_dtype: Optional[chex.ArrayDType] = None,\n) -> Tuple[optax.GradientTransformation, optax.Schedule]:",
        "detail": "src.fjformer.optimizers.lion",
        "documentation": {}
    },
    {
        "label": "get_lion_with_warmup_cosine_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.lion",
        "description": "src.fjformer.optimizers.lion",
        "peekOfCode": "def get_lion_with_warmup_cosine_scheduler(\n    steps: int,\n    learning_rate: float = 5e-5,\n    learning_rate_end: float = 1e-5,\n    exponent: float = 1.0,\n    b1: float = 0.9,\n    b2: float = 0.99,\n    gradient_accumulation_steps: int = 1,\n    warmup_steps: int = 500,\n    mu_dtype: Optional[chex.ArrayDType] = None,",
        "detail": "src.fjformer.optimizers.lion",
        "documentation": {}
    },
    {
        "label": "OptaxScheduledWeightDecayState",
        "kind": 6,
        "importPath": "src.fjformer.optimizers.optimizer_utils",
        "description": "src.fjformer.optimizers.optimizer_utils",
        "peekOfCode": "class OptaxScheduledWeightDecayState(NamedTuple):\n    \"\"\"\n    State for the scheduled weight decay optimizer.\n    \"\"\"\n    count: chex.Array  # Step count\ndef optax_add_scheduled_weight_decay(\n    schedule_fn: Callable[[chex.Array], chex.Array],\n    mask: Optional[chex.ArrayTree] = None,\n) -> optax.GradientTransformation:\n    \"\"\"",
        "detail": "src.fjformer.optimizers.optimizer_utils",
        "documentation": {}
    },
    {
        "label": "optax_add_scheduled_weight_decay",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.optimizer_utils",
        "description": "src.fjformer.optimizers.optimizer_utils",
        "peekOfCode": "def optax_add_scheduled_weight_decay(\n    schedule_fn: Callable[[chex.Array], chex.Array],\n    mask: Optional[chex.ArrayTree] = None,\n) -> optax.GradientTransformation:\n    \"\"\"\n    Create an optax optimizer that applies weight decay on a schedule.\n    This function is similar to `optax.add_decayed_weights`, but it allows for\n    the weight decay rate to be scheduled over training steps.\n    Args:\n        schedule_fn: A function that takes the current step count as input",
        "detail": "src.fjformer.optimizers.optimizer_utils",
        "documentation": {}
    },
    {
        "label": "get_rmsprop_with_cosine_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.rmsprop",
        "description": "src.fjformer.optimizers.rmsprop",
        "peekOfCode": "def get_rmsprop_with_cosine_scheduler(\n    steps: int,\n    learning_rate: float = 5e-5,\n    decay: float = 0.9,\n    initial_scale: float = 0.0,\n    momentum: Optional[float] = None,\n    nesterov: bool = False,\n    eps: float = 1e-8,\n    weight_decay: float = 1e-1,\n    gradient_accumulation_steps: int = 1,",
        "detail": "src.fjformer.optimizers.rmsprop",
        "documentation": {}
    },
    {
        "label": "get_rmsprop_with_linear_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.rmsprop",
        "description": "src.fjformer.optimizers.rmsprop",
        "peekOfCode": "def get_rmsprop_with_linear_scheduler(\n    steps: int,\n    learning_rate_start: float = 5e-5,\n    learning_rate_end: float = 1e-5,\n    decay: float = 0.9,\n    initial_scale: float = 0.0,\n    momentum: Optional[float] = None,\n    nesterov: bool = False,\n    eps: float = 1e-8,\n    weight_decay: float = 1e-1,",
        "detail": "src.fjformer.optimizers.rmsprop",
        "documentation": {}
    },
    {
        "label": "get_rmsprop_with_warmup_linear_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.rmsprop",
        "description": "src.fjformer.optimizers.rmsprop",
        "peekOfCode": "def get_rmsprop_with_warmup_linear_scheduler(\n    steps: int,\n    learning_rate_start: float = 5e-5,\n    learning_rate_end: float = 1e-5,\n    decay: float = 0.9,\n    eps: float = 1e-8,\n    initial_scale: float = 0.0,\n    momentum: Optional[float] = None,\n    nesterov: bool = False,\n    weight_decay: float = 1e-1,",
        "detail": "src.fjformer.optimizers.rmsprop",
        "documentation": {}
    },
    {
        "label": "get_rmsprop_with_warmup_cosine_scheduler",
        "kind": 2,
        "importPath": "src.fjformer.optimizers.rmsprop",
        "description": "src.fjformer.optimizers.rmsprop",
        "peekOfCode": "def get_rmsprop_with_warmup_cosine_scheduler(\n    steps: int,\n    learning_rate: float = 5e-5,\n    learning_rate_end: float = 1e-5,\n    decay: float = 0.9,\n    initial_scale: float = 0.0,\n    momentum: Optional[float] = None,\n    nesterov: bool = False,\n    eps: float = 1e-8,\n    weight_decay: float = 1e-1,",
        "detail": "src.fjformer.optimizers.rmsprop",
        "documentation": {}
    },
    {
        "label": "Carry",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.efficient_attention.efficient_attention",
        "description": "src.fjformer.pallas_operations.efficient_attention.efficient_attention",
        "peekOfCode": "class Carry(NamedTuple):\n    numerator: chex.Array\n    denominator: chex.Array\n    max_so_far: chex.Array\ndef efficient_attention(\n        query: chex.Array,\n        key: chex.Array,\n        value: chex.Array,\n        bias: chex.Array = None,\n        deterministic: bool = True,",
        "detail": "src.fjformer.pallas_operations.efficient_attention.efficient_attention",
        "documentation": {}
    },
    {
        "label": "efficient_attention",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.efficient_attention.efficient_attention",
        "description": "src.fjformer.pallas_operations.efficient_attention.efficient_attention",
        "peekOfCode": "def efficient_attention(\n        query: chex.Array,\n        key: chex.Array,\n        value: chex.Array,\n        bias: chex.Array = None,\n        deterministic: bool = True,\n        dropout_rng: chex.PRNGKey = None,\n        attention_drop_rate: float = 0.0,\n        causal: bool = True,\n        query_chunk_size: int = 1024,",
        "detail": "src.fjformer.pallas_operations.efficient_attention.efficient_attention",
        "documentation": {}
    },
    {
        "label": "mha_forward_kernel",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "description": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "peekOfCode": "def mha_forward_kernel(\n        q_ref,\n        k_ref,\n        v_ref,  # Input arrays\n        segment_ids_ref: jax.Array | None,  # segment_id arrays\n        o_ref: Any,  # Output\n        *residual_refs: Any,  # Residual outputs\n        num_heads: int,\n        sm_scale: float,\n        causal: bool,",
        "detail": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "documentation": {}
    },
    {
        "label": "segment_mask",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "description": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "peekOfCode": "def segment_mask(\n        q_segment_ids: jax.Array,\n        kv_segment_ids: jax.Array,\n):\n    # [B, T, 1] or [T, 1]\n    q_segment_ids = jnp.expand_dims(q_segment_ids, axis=-1)\n    # [B, 1, S] or [1, S]\n    if kv_segment_ids.ndim == 1:\n        kv_segment_ids = jnp.expand_dims(kv_segment_ids, axis=0)\n    else:",
        "detail": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "documentation": {}
    },
    {
        "label": "mha",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "description": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "peekOfCode": "def mha(\n        q,\n        k,\n        v,\n        segment_ids: jnp.ndarray | None,\n        sm_scale: float = 1.0,\n        causal: bool = False,\n        block_q: int = 128,\n        block_k: int = 128,\n        backward_pass_impl: str = \"triton\",",
        "detail": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "documentation": {}
    },
    {
        "label": "mha_backward_kernel",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "description": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "peekOfCode": "def mha_backward_kernel(\n        # Inputs\n        q_ref,\n        k_ref,\n        v_ref,\n        segment_ids_ref: jax.Array | None,\n        out_ref,\n        do_scaled_ref,\n        l_ref,\n        m_ref,",
        "detail": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "documentation": {}
    },
    {
        "label": "mha_reference",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "description": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "peekOfCode": "def mha_reference(\n        q,\n        k,\n        v,\n        segment_ids: jnp.ndarray | None,\n        sm_scale=1.0,\n        causal: bool = False,\n):\n    q_seq_len = q.shape[1]\n    kv_seq_len = k.shape[1]",
        "detail": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MASK_VALUE",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "description": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "peekOfCode": "DEFAULT_MASK_VALUE = -0.7 * float(np.finfo(np.dtype(\"float32\")).max)\ndef mha_forward_kernel(\n        q_ref,\n        k_ref,\n        v_ref,  # Input arrays\n        segment_ids_ref: jax.Array | None,  # segment_id arrays\n        o_ref: Any,  # Output\n        *residual_refs: Any,  # Residual outputs\n        num_heads: int,\n        sm_scale: float,",
        "detail": "src.fjformer.pallas_operations.gpu.flash_attention.mha",
        "documentation": {}
    },
    {
        "label": "layer_norm_forward_kernel",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "description": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "peekOfCode": "def layer_norm_forward_kernel(\n        x_ref, weight_ref, bias_ref,  # Input arrays\n        o_ref, mean_ref=None, rstd_ref=None,  # Output arrays\n        *, eps: float, block_size: int):\n    n_col = x_ref.shape[0]\n    def mean_body(i, acc_ref):\n        col_idx = i * block_size + jnp.arange(block_size)\n        mask = col_idx < n_col\n        a = pl.load(x_ref, (col_idx,), mask=mask, other=0.,\n                    eviction_policy=\"evict_last\").astype(jnp.float32)",
        "detail": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "documentation": {}
    },
    {
        "label": "layer_norm_forward",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "description": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "peekOfCode": "def layer_norm_forward(\n        x, weight, bias,\n        num_warps: int | None = None,\n        num_stages: int | None = 3,\n        eps: float = 1e-5,\n        backward_pass_impl: str = 'triton',\n        interpret: bool = False):\n    del num_stages\n    del backward_pass_impl\n    n = x.shape[-1]",
        "detail": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "documentation": {}
    },
    {
        "label": "layer_norm_backward_kernel_dx",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "description": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "peekOfCode": "def layer_norm_backward_kernel_dx(\n        # Inputs\n        x_ref, weight_ref, bias_ref, do_ref,\n        mean_ref, rstd_ref,\n        # Outputs\n        dx_ref,\n        *, eps: float, block_size: int):\n    n_col = x_ref.shape[0]\n    def mean_body(i, acc_ref):\n        col_idx = i * block_size + jnp.arange(block_size)",
        "detail": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "documentation": {}
    },
    {
        "label": "layer_norm_backward_kernel_dw_db",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "description": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "peekOfCode": "def layer_norm_backward_kernel_dw_db(\n        # Inputs\n        x_ref, weight_ref, bias_ref, do_ref,\n        mean_ref, rstd_ref,\n        # Outputs\n        dw_ref, db_ref,\n        *, eps: float, block_m: int, block_n: int):\n    m, n_col = x_ref.shape\n    j = pl.program_id(0)\n    col_idx = j * block_n + jnp.arange(block_n)",
        "detail": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "documentation": {}
    },
    {
        "label": "layer_norm_backward",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "description": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "peekOfCode": "def layer_norm_backward(\n        num_warps: int | None,\n        num_stages: int | None,\n        eps: float,\n        backward_pass_impl: str,\n        interpret: bool,\n        res, do):\n    del num_stages\n    x, weight, bias, mean, rstd = res\n    if backward_pass_impl == 'xla':",
        "detail": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "documentation": {}
    },
    {
        "label": "layer_norm",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "description": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "peekOfCode": "def layer_norm(\n        x, weight, bias,\n        num_warps: int | None = None,\n        num_stages: int | None = 3,\n        eps: float = 1e-5,\n        backward_pass_impl: str = 'triton',\n        interpret: bool = False):\n    n = x.shape[-1]\n    # Triton heuristics\n    # Less than 64KB per feature: enqueue fused kernel",
        "detail": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "documentation": {}
    },
    {
        "label": "layer_norm_reference",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "description": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "peekOfCode": "def layer_norm_reference(x, weight, bias, *, eps: float = 1e-5):\n    mean = jnp.mean(x, axis=1)\n    mean2 = jnp.mean(jnp.square(x), axis=1)\n    var = jnp.maximum(0., mean2 - jnp.square(mean))\n    y = x - mean[:, None]\n    mul = lax.rsqrt(var + eps)\n    return y * mul[:, None] * weight[None] + bias[None]",
        "detail": "src.fjformer.pallas_operations.gpu.layer_norm.layer_norm",
        "documentation": {}
    },
    {
        "label": "rms_norm_forward_kernel",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "description": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "peekOfCode": "def rms_norm_forward_kernel(\n        x_ref, weight_ref, bias_ref,  # Input arrays\n        o_ref, rstd_ref=None,  # Output arrays\n        *, eps: float, block_size: int):\n    n_col = x_ref.shape[0]\n    def var_body(i, acc_ref):\n        col_idx = i * block_size + jnp.arange(block_size)\n        mask = col_idx < n_col\n        a = pl.load(x_ref, (col_idx,), mask=mask, other=0.,\n                    eviction_policy=\"evict_last\").astype(jnp.float32)",
        "detail": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "documentation": {}
    },
    {
        "label": "rms_norm_forward",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "description": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "peekOfCode": "def rms_norm_forward(\n        x, weight, bias,\n        num_warps: int | None = None,\n        num_stages: int | None = 3,\n        eps: float = 1e-5,\n        backward_pass_impl: str = 'triton',\n        interpret: bool = False):\n    del num_stages\n    del backward_pass_impl\n    n = x.shape[-1]",
        "detail": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "documentation": {}
    },
    {
        "label": "rms_norm_backward_kernel_dx",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "description": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "peekOfCode": "def rms_norm_backward_kernel_dx(\n        # Inputs\n        x_ref, weight_ref, bias_ref, do_ref,\n        rstd_ref,\n        # Outputs\n        dx_ref,\n        *, eps: float, block_size: int):\n    n_col = x_ref.shape[0]\n    def mean_body(i, c1_acc_ref):\n        col_idx = i * block_size + jnp.arange(block_size)",
        "detail": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "documentation": {}
    },
    {
        "label": "rms_norm_backward_kernel_dw_db",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "description": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "peekOfCode": "def rms_norm_backward_kernel_dw_db(\n        # Inputs\n        x_ref, weight_ref, bias_ref, do_ref,\n        rstd_ref,\n        # Outputs\n        dw_ref, db_ref,\n        *, eps: float, block_m: int, block_n: int):\n    m, n_col = x_ref.shape\n    j = pl.program_id(0)\n    col_idx = j * block_n + jnp.arange(block_n)",
        "detail": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "documentation": {}
    },
    {
        "label": "rms_norm_backward",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "description": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "peekOfCode": "def rms_norm_backward(\n        num_warps: int | None,\n        num_stages: int | None,\n        eps: float,\n        backward_pass_impl: str,\n        interpret: bool,\n        res, do):\n    del num_stages\n    x, weight, bias, rstd = res\n    if backward_pass_impl == 'xla':",
        "detail": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "documentation": {}
    },
    {
        "label": "rms_norm",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "description": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "peekOfCode": "def rms_norm(\n        x, weight, bias,\n        num_warps: int | None = None,\n        num_stages: int | None = 3,\n        eps: float = 1e-5,\n        backward_pass_impl: str = 'triton',\n        interpret: bool = False):\n    n = x.shape[-1]\n    # Triton heuristics\n    # Less than 64KB per feature: enqueue fused kernel",
        "detail": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "documentation": {}
    },
    {
        "label": "rms_norm_reference",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "description": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "peekOfCode": "def rms_norm_reference(x, weight, bias, *, eps: float = 1e-5):\n    var = jnp.mean(jnp.square(x), axis=1)\n    mul = lax.rsqrt(var + eps)\n    return x * mul[:, None] * weight[None] + bias[None]",
        "detail": "src.fjformer.pallas_operations.gpu.rms_norm.rms_norm",
        "documentation": {}
    },
    {
        "label": "softmax",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.gpu.softmax.softmax",
        "description": "src.fjformer.pallas_operations.gpu.softmax.softmax",
        "peekOfCode": "def softmax(\n        x: jax.Array, *, axis: int = -1, num_warps: int = 4,\n        interpret: bool = False, debug: bool = False\n) -> jax.Array:\n    \"\"\"Computes the softmax of the input array along the specified axis.\n    Args:\n      x: input array\n      axis: the axis along which to perform the computation\n      num_warps: the number of warps to use for executing the Triton kernel\n      interpret: whether to interpret the kernel using pallas",
        "detail": "src.fjformer.pallas_operations.gpu.softmax.softmax",
        "documentation": {}
    },
    {
        "label": "flash_attention_forward_kernel",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.pallas_attention.attention",
        "description": "src.fjformer.pallas_operations.pallas_attention.attention",
        "peekOfCode": "def flash_attention_forward_kernel(\n    q_ref,\n    k_ref,\n    v_ref,  # Input arrays\n    b_ref: jax.Array | None,  # bias\n    o_ref: Any,  # Output\n    *residual_refs: Any,  # Residual outputs\n    num_heads: int,\n    sm_scale: float,\n    block_q: int,",
        "detail": "src.fjformer.pallas_operations.pallas_attention.attention",
        "documentation": {}
    },
    {
        "label": "flash_attention",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.pallas_attention.attention",
        "description": "src.fjformer.pallas_operations.pallas_attention.attention",
        "peekOfCode": "def flash_attention(\n    query,\n    key,\n    value,\n    bias: Optional[jnp.ndarray] = None,\n    sm_scale: Optional[float] = None,\n    block_q: int = 128,\n    block_k: int = 128,\n    backward_pass_impl: str = \"triton\",\n    num_warps: Optional[int] = None,",
        "detail": "src.fjformer.pallas_operations.pallas_attention.attention",
        "documentation": {}
    },
    {
        "label": "flash_attention_backward_kernel",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.pallas_attention.attention",
        "description": "src.fjformer.pallas_operations.pallas_attention.attention",
        "peekOfCode": "def flash_attention_backward_kernel(\n    # Inputs\n    q_ref,\n    k_ref,\n    v_ref,\n    b_ref: jax.Array | None,\n    out_ref,\n    do_scaled_ref,\n    l_ref,\n    m_ref,",
        "detail": "src.fjformer.pallas_operations.pallas_attention.attention",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MASK_VALUE",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.pallas_attention.attention",
        "description": "src.fjformer.pallas_operations.pallas_attention.attention",
        "peekOfCode": "DEFAULT_MASK_VALUE = -0.7 * float(np.finfo(np.dtype(\"float32\")).max)\ndef flash_attention_forward_kernel(\n    q_ref,\n    k_ref,\n    v_ref,  # Input arrays\n    b_ref: jax.Array | None,  # bias\n    o_ref: Any,  # Output\n    *residual_refs: Any,  # Residual outputs\n    num_heads: int,\n    sm_scale: float,",
        "detail": "src.fjformer.pallas_operations.pallas_attention.attention",
        "documentation": {}
    },
    {
        "label": "SegmentIds",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "description": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "peekOfCode": "class SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n    SegmentIds are used to generate segment mask, which prevents attention between\n    different segments in the input sequence. Each array is a list of ids\n    (integers).\n    Only the token with the same id can attend to each other.\n    Attributes:\n      q: segment ids along the Q sequence.\n      kv: segment ids along the KV sequence.\n    \"\"\"",
        "detail": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "documentation": {}
    },
    {
        "label": "BlockSizes",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "description": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "peekOfCode": "class BlockSizes:\n    \"\"\"Tile sizes parameterizing FlashAttention kernels.\n    Those parameters have negligible effect on numerics, but affect performance\n    greatly.\n    \"\"\"\n    block_q: int\n    block_k_major: int\n    block_k: int\n    block_b: int\n    block_q_major_dkv: int | None = None",
        "detail": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "documentation": {}
    },
    {
        "label": "flash_attention",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "description": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "peekOfCode": "def flash_attention(\n        q,  # [batch_size, num_heads, q_seq_len, d_model]\n        k,  # [batch_size, num_heads, kv_seq_len, d_model]\n        v,  # [batch_size, num_heads, kv_seq_len, d_model]\n        ab=None,  # [batch_size, num_heads, q_seq_len, kv_seq_len]\n        segment_ids=None,  # q of [batch_size, q_seq_len] and kv of [batch_size, kv_seq_len]\n        *,\n        causal: bool = False,\n        sm_scale: float = 1.0,\n        block_sizes: BlockSizes | None = None,",
        "detail": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "documentation": {}
    },
    {
        "label": "below_or_on_diag",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "description": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "peekOfCode": "def below_or_on_diag(r, r_blk_size, c, c_blk_size):\n    # A block is considered below or on diagonal as long as the bottom left\n    # corner of the block is below or on diagonal.\n    return ((r + 1) * r_blk_size - 1) > (c * c_blk_size)\ndef _flash_attention_kernel(q_tile_ref, *args, **kwargs):\n    block_b = q_tile_ref.shape[0]\n    # If we're not going to tile the softmax, then we can avoid a bunch of VPU ops.\n    if kwargs[\"block_k\"] == kwargs[\"kv_seq_len\"]:\n        kernel = _flash_attention_kernel_single_batch_single_step\n    else:",
        "detail": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "documentation": {}
    },
    {
        "label": "mha_reference_no_custom_vjp",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "description": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "peekOfCode": "def mha_reference_no_custom_vjp(\n        q,\n        k,\n        v,\n        ab: jax.Array | None = None,\n        segment_ids: SegmentIds | None = None,\n        *,\n        causal: bool = False,\n        mask_value: float = DEFAULT_MASK_VALUE,\n        sm_scale: float = 1.0,",
        "detail": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "documentation": {}
    },
    {
        "label": "mha_reference",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "description": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "peekOfCode": "def mha_reference(\n        q,\n        k,\n        v,\n        ab,\n        segment_ids: SegmentIds | None = None,\n        causal: bool = False,\n        mask_value: float = DEFAULT_MASK_VALUE,\n        sm_scale=1.0,\n):",
        "detail": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "documentation": {}
    },
    {
        "label": "mha_reference_bwd",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "description": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "peekOfCode": "def mha_reference_bwd(\n        q,\n        k,\n        v,\n        ab,\n        segment_ids: SegmentIds | None,\n        o,\n        l,\n        m,\n        do,",
        "detail": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MASK_VALUE",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "description": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "peekOfCode": "DEFAULT_MASK_VALUE = -0.7 * float(jnp.finfo(jnp.dtype(\"float32\")).max)\nNUM_LANES = 128\nNUM_SUBLANES = 8\nclass SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n    SegmentIds are used to generate segment mask, which prevents attention between\n    different segments in the input sequence. Each array is a list of ids\n    (integers).\n    Only the token with the same id can attend to each other.\n    Attributes:",
        "detail": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "documentation": {}
    },
    {
        "label": "NUM_LANES",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "description": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "peekOfCode": "NUM_LANES = 128\nNUM_SUBLANES = 8\nclass SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n    SegmentIds are used to generate segment mask, which prevents attention between\n    different segments in the input sequence. Each array is a list of ids\n    (integers).\n    Only the token with the same id can attend to each other.\n    Attributes:\n      q: segment ids along the Q sequence.",
        "detail": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "documentation": {}
    },
    {
        "label": "NUM_SUBLANES",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "description": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "peekOfCode": "NUM_SUBLANES = 8\nclass SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n    SegmentIds are used to generate segment mask, which prevents attention between\n    different segments in the input sequence. Each array is a list of ids\n    (integers).\n    Only the token with the same id can attend to each other.\n    Attributes:\n      q: segment ids along the Q sequence.\n      kv: segment ids along the KV sequence.",
        "detail": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "documentation": {}
    },
    {
        "label": "MIN_BLOCK_SIZE",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "description": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "peekOfCode": "MIN_BLOCK_SIZE = 128\nTRANS_B_DIM_NUMBERS = (((1,), (1,)), ((), ()))\ndef below_or_on_diag(r, r_blk_size, c, c_blk_size):\n    # A block is considered below or on diagonal as long as the bottom left\n    # corner of the block is below or on diagonal.\n    return ((r + 1) * r_blk_size - 1) > (c * c_blk_size)\ndef _flash_attention_kernel(q_tile_ref, *args, **kwargs):\n    block_b = q_tile_ref.shape[0]\n    # If we're not going to tile the softmax, then we can avoid a bunch of VPU ops.\n    if kwargs[\"block_k\"] == kwargs[\"kv_seq_len\"]:",
        "detail": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "documentation": {}
    },
    {
        "label": "TRANS_B_DIM_NUMBERS",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "description": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "peekOfCode": "TRANS_B_DIM_NUMBERS = (((1,), (1,)), ((), ()))\ndef below_or_on_diag(r, r_blk_size, c, c_blk_size):\n    # A block is considered below or on diagonal as long as the bottom left\n    # corner of the block is below or on diagonal.\n    return ((r + 1) * r_blk_size - 1) > (c * c_blk_size)\ndef _flash_attention_kernel(q_tile_ref, *args, **kwargs):\n    block_b = q_tile_ref.shape[0]\n    # If we're not going to tile the softmax, then we can avoid a bunch of VPU ops.\n    if kwargs[\"block_k\"] == kwargs[\"kv_seq_len\"]:\n        kernel = _flash_attention_kernel_single_batch_single_step",
        "detail": "src.fjformer.pallas_operations.tpu.flash_attention.flash_attention",
        "documentation": {}
    },
    {
        "label": "QuantizedTensor",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "description": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "peekOfCode": "class QuantizedTensor(NamedTuple):\n    \"\"\"A tensor which has been quantized to int8 and its scales.\n    Attributes:\n      weight: Weight\n      scales: Scales\n    \"\"\"\n    weight: jnp.ndarray\n    scales: jnp.ndarray\ndef to_int8(x: jnp.ndarray, h: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Converts a float array to an int8 array with a scale.",
        "detail": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "documentation": {}
    },
    {
        "label": "MultiPageAsyncCopyDescriptor",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "description": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "peekOfCode": "class MultiPageAsyncCopyDescriptor:\n    \"\"\"Descriptor for async copy of multiple K/V pages from HBM.\"\"\"\n    def __init__(\n            self,\n            pages_hbm_ref,\n            scales_pages_hbm_ref,\n            vmem_buffer,\n            scales_vmem_buffer,\n            sem,\n            page_indices,",
        "detail": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "documentation": {}
    },
    {
        "label": "to_int8",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "description": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "peekOfCode": "def to_int8(x: jnp.ndarray, h: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Converts a float array to an int8 array with a scale.\n    Args:\n      x: Float array.\n      h: Quantization scale.\n    Returns:\n      Int8 array.\n    \"\"\"\n    return jnp.int8(jnp.rint(x * (MAX_INT8 / h)))\ndef from_int8(",
        "detail": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "documentation": {}
    },
    {
        "label": "from_int8",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "description": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "peekOfCode": "def from_int8(\n        x: jnp.ndarray, h: jnp.ndarray, dtype: jnp.dtype = jnp.bfloat16\n) -> jnp.ndarray:\n    \"\"\"Converts an int8 array to a float array with a scale.\n    Args:\n      x: Int8 array.\n      h: Quantization scale.\n      dtype: Float dtype to convert to.\n    Returns:\n      Float array.",
        "detail": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "documentation": {}
    },
    {
        "label": "get_quantization_scales",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "description": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "peekOfCode": "def get_quantization_scales(x: jnp.ndarray) -> jnp.ndarray:\n    \"\"\"Computes the quantization scales for a float array.\n    These are the maximum values of the trailing dimension.\n    Args:\n      x: Float array to quantize.\n    Returns:\n      Array of the same shape as input but with the trailing dimension reduced to\n      a size 1 absolute max value.\n    \"\"\"\n    return jnp.max(jnp.abs(x), axis=-1, keepdims=True)",
        "detail": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "documentation": {}
    },
    {
        "label": "quantize_to_int8",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "description": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "peekOfCode": "def quantize_to_int8(\n        x: jnp.ndarray,\n) -> QuantizedTensor:\n    \"\"\"Quantizes a float array to an int8 QuantizedTensor.\n    Args:\n      x: Float array to quantize.\n    Returns:\n      Int8 QuantizedTensor.\n    \"\"\"\n    x_scales = get_quantization_scales(x)",
        "detail": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "documentation": {}
    },
    {
        "label": "unquantize_from_int8",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "description": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "peekOfCode": "def unquantize_from_int8(\n        x: QuantizedTensor,\n        dtype: jnp.dtype = jnp.bfloat16,\n) -> jnp.ndarray:\n    \"\"\"Unquantizes an int8 QuantizedTensor to a float array.\n    Args:\n      x: Int8 QuantizedTensor to unquantize.\n      dtype: Float dtype to unquantize to.\n    Returns:\n      Float array.",
        "detail": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "documentation": {}
    },
    {
        "label": "paged_flash_attention_kernel",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "description": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "peekOfCode": "def paged_flash_attention_kernel(\n        lengths_ref,\n        page_indices_ref,\n        buffer_index_ref,\n        step_ref,\n        q_ref,\n        k_pages_hbm_ref,\n        k_scales_pages_hbm_ref,\n        v_pages_hbm_ref,\n        v_scales_pages_hbm_ref,",
        "detail": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "documentation": {}
    },
    {
        "label": "paged_flash_attention_kernel_inline_seq_dim",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "description": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "peekOfCode": "def paged_flash_attention_kernel_inline_seq_dim(\n        lengths_ref,\n        page_indices_ref,\n        buffer_index_ref,\n        step_ref,\n        q_ref,\n        k_pages_hbm_ref,\n        k_scales_pages_hbm_ref,\n        v_pages_hbm_ref,\n        v_scales_pages_hbm_ref,",
        "detail": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "documentation": {}
    },
    {
        "label": "paged_attention",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "description": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "peekOfCode": "def paged_attention(\n        q: jax.Array,\n        k_pages: Union[jax.Array, QuantizedTensor],\n        v_pages: Union[jax.Array, QuantizedTensor],\n        lengths: jax.Array,\n        page_indices: jax.Array,\n        *,\n        mask_value: float = DEFAULT_MASK_VALUE,\n        pages_per_compute_block: int,\n        megacore_mode: Optional[str] = None,",
        "detail": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MASK_VALUE",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "description": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "peekOfCode": "DEFAULT_MASK_VALUE = -0.7 * float(np.finfo(np.dtype(\"float32\")).max)\nP = jax.sharding.PartitionSpec\nMAX_INT8 = 127.5\nclass QuantizedTensor(NamedTuple):\n    \"\"\"A tensor which has been quantized to int8 and its scales.\n    Attributes:\n      weight: Weight\n      scales: Scales\n    \"\"\"\n    weight: jnp.ndarray",
        "detail": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "documentation": {}
    },
    {
        "label": "P",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "description": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "peekOfCode": "P = jax.sharding.PartitionSpec\nMAX_INT8 = 127.5\nclass QuantizedTensor(NamedTuple):\n    \"\"\"A tensor which has been quantized to int8 and its scales.\n    Attributes:\n      weight: Weight\n      scales: Scales\n    \"\"\"\n    weight: jnp.ndarray\n    scales: jnp.ndarray",
        "detail": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "documentation": {}
    },
    {
        "label": "MAX_INT8",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "description": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "peekOfCode": "MAX_INT8 = 127.5\nclass QuantizedTensor(NamedTuple):\n    \"\"\"A tensor which has been quantized to int8 and its scales.\n    Attributes:\n      weight: Weight\n      scales: Scales\n    \"\"\"\n    weight: jnp.ndarray\n    scales: jnp.ndarray\ndef to_int8(x: jnp.ndarray, h: jnp.ndarray) -> jnp.ndarray:",
        "detail": "src.fjformer.pallas_operations.tpu.paged_attention.paged_attention",
        "documentation": {}
    },
    {
        "label": "SegmentIds",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "description": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "peekOfCode": "class SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n    SegmentIds are used to generate segment mask, which prevents attention between\n    different segments in the input sequence. Each array is a list of ids\n    (integers).\n    Only the token with the same id can attend to each other.\n    Attributes:\n      q: segment ids along the Q sequence.\n      kv: segment ids along the KV sequence.\n    \"\"\"",
        "detail": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "documentation": {}
    },
    {
        "label": "BlockSizes",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "description": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "peekOfCode": "class BlockSizes:\n    block_q: int\n    block_k_major: int\n    block_k: int\n    block_b: int\n    block_q_major_dkv: Optional[int] = None\n    block_k_major_dkv: Optional[int] = None\n    block_k_dkv: Optional[int] = None\n    block_q_dkv: Optional[int] = None\n    block_k_major_dq: Optional[int] = None",
        "detail": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "documentation": {}
    },
    {
        "label": "ring_attention",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "description": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "peekOfCode": "def ring_attention(\n    q, k, v, attn_bias, segment_ids, axis_name, float32_logits, blockwise_kwargs\n):\n    y, _ = _ring_attention_fwd(\n        q, k, v, attn_bias, segment_ids, axis_name, float32_logits, blockwise_kwargs\n    )\n    return y\nring_attention.defvjp(_ring_attention_fwd, _ring_attention_bwd)\ndef _ring_attention_standard_fwd(q, k, v, attn_mask, axis_name, float32_logits):\n    if float32_logits:",
        "detail": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "documentation": {}
    },
    {
        "label": "ring_attention_standard",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "description": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "peekOfCode": "def ring_attention_standard(q, k, v, attn_mask, axis_name, float32_logits=True):\n    y, _ = _ring_attention_standard_fwd(q, k, v, attn_mask, axis_name, float32_logits)\n    return y\nring_attention_standard.defvjp(\n    _ring_attention_standard_fwd, _ring_attention_standard_bwd\n)\ndef _blockwise_attention_fwd(\n    q,\n    k,\n    v,",
        "detail": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "documentation": {}
    },
    {
        "label": "ring_flash_attention_tpu",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "description": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "peekOfCode": "def ring_flash_attention_tpu(\n    q, k, v, attn_bias, segment_ids, axis_name, float32_logits, blockwise_kwargs\n):\n    y, _ = _ring_flash_attention_fwd_tpu(\n        q, k, v, attn_bias, segment_ids, axis_name, float32_logits, blockwise_kwargs\n    )\n    return y\nring_flash_attention_tpu.defvjp(\n    _ring_flash_attention_fwd_tpu, _ring_flash_attention_bwd_tpu\n)",
        "detail": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "documentation": {}
    },
    {
        "label": "below_or_on_diag",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "description": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "peekOfCode": "def below_or_on_diag(r, r_blk_size, c, c_blk_size):\n    # A block is considered below or on diagonal as long as the bottom left\n    # corner of the block is below or on diagonal.\n    return ((r + 1) * r_blk_size - 1) > (c * c_blk_size)\ndef _flash_attention_kernel(\n    q_idx_chunk_start, k_idx_chunk_start, q_tile_ref, *args, **kwargs\n):\n    block_b = q_tile_ref.shape[0]\n    # If we're not going to tile the softmax, then we can avoid a bunch of VPU ops.\n    if kwargs[\"block_k\"] == kwargs[\"kv_seq_len\"]:",
        "detail": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MASK_VALUE",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "description": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "peekOfCode": "DEFAULT_MASK_VALUE = -0.7 * float(jnp.finfo(jnp.dtype(\"float32\")).max)\nNUM_LANES = 128\nNUM_SUBLANES = 8\nclass SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n    SegmentIds are used to generate segment mask, which prevents attention between\n    different segments in the input sequence. Each array is a list of ids\n    (integers).\n    Only the token with the same id can attend to each other.\n    Attributes:",
        "detail": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "documentation": {}
    },
    {
        "label": "NUM_LANES",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "description": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "peekOfCode": "NUM_LANES = 128\nNUM_SUBLANES = 8\nclass SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n    SegmentIds are used to generate segment mask, which prevents attention between\n    different segments in the input sequence. Each array is a list of ids\n    (integers).\n    Only the token with the same id can attend to each other.\n    Attributes:\n      q: segment ids along the Q sequence.",
        "detail": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "documentation": {}
    },
    {
        "label": "NUM_SUBLANES",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "description": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "peekOfCode": "NUM_SUBLANES = 8\nclass SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n    SegmentIds are used to generate segment mask, which prevents attention between\n    different segments in the input sequence. Each array is a list of ids\n    (integers).\n    Only the token with the same id can attend to each other.\n    Attributes:\n      q: segment ids along the Q sequence.\n      kv: segment ids along the KV sequence.",
        "detail": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "documentation": {}
    },
    {
        "label": "MIN_BLOCK_SIZE",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "description": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "peekOfCode": "MIN_BLOCK_SIZE = 128\nTRANS_B_DIM_NUMBERS = (((1,), (1,)), ((), ()))\ndef below_or_on_diag(r, r_blk_size, c, c_blk_size):\n    # A block is considered below or on diagonal as long as the bottom left\n    # corner of the block is below or on diagonal.\n    return ((r + 1) * r_blk_size - 1) > (c * c_blk_size)\ndef _flash_attention_kernel(\n    q_idx_chunk_start, k_idx_chunk_start, q_tile_ref, *args, **kwargs\n):\n    block_b = q_tile_ref.shape[0]",
        "detail": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "documentation": {}
    },
    {
        "label": "TRANS_B_DIM_NUMBERS",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "description": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "peekOfCode": "TRANS_B_DIM_NUMBERS = (((1,), (1,)), ((), ()))\ndef below_or_on_diag(r, r_blk_size, c, c_blk_size):\n    # A block is considered below or on diagonal as long as the bottom left\n    # corner of the block is below or on diagonal.\n    return ((r + 1) * r_blk_size - 1) > (c * c_blk_size)\ndef _flash_attention_kernel(\n    q_idx_chunk_start, k_idx_chunk_start, q_tile_ref, *args, **kwargs\n):\n    block_b = q_tile_ref.shape[0]\n    # If we're not going to tile the softmax, then we can avoid a bunch of VPU ops.",
        "detail": "src.fjformer.pallas_operations.tpu.ring_attention.ring_attention",
        "documentation": {}
    },
    {
        "label": "SegmentIds",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "class SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n    SegmentIds are a mechanims to ensure that there is no cross-attention between\n    segments (fraction of a sequence) that have been concatenated together into a\n    sequence. Each array is a list of ids (integers). Only tokens with the same\n    id are allowed to attend to each other.\n    The static mask (e.g. causal) is \"and-ed\" with the segment id mask to form\n    the actual attention mask. It is important that the latter does not have any\n    all-zero rows (along dimension kv). Otherwise it would result in a invalid\n    softmax (the denominator would be 0).",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "QKVLayout",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "class QKVLayout(enum.IntEnum):\n    HEAD_DIM_MINOR = enum.auto()  # [..., seq_len, head_dim]\n    SEQ_MINOR = enum.auto()  # [..., head_dim, seq_len]\ndef from_head_minor(vals: tuple[Any, ...], layout: QKVLayout):\n    if layout == QKVLayout.HEAD_DIM_MINOR:\n        return vals\n    return (*vals[:-2], vals[-1], vals[-2])\n@dataclasses.dataclass(unsafe_hash=True)\nclass BlockSizes:\n    \"\"\"Tile sizes parameterizing SplashAttention kernels.",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "BlockSizes",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "class BlockSizes:\n    \"\"\"Tile sizes parameterizing SplashAttention kernels.\n    Those parameters have negligible effect on numerics, but affect performance\n    greatly.\n    Note that changing the layouts only influences the physical layout that the\n    kernel will enforce. The logical interface to splash attention always takes\n    the head dimension as the minormost one.\n    \"\"\"\n    block_q: int\n    block_kv: int",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "SplashAttentionKernel",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "class SplashAttentionKernel:\n    def __init__(\n            self,\n            fwd_mask_info: mask_info_lib.MaskInfo,\n            dq_mask_info: mask_info_lib.MaskInfo | None,\n            dkv_mask_info: mask_info_lib.MaskInfo | None,\n            **kwargs,\n    ):\n        self.kwargs = kwargs\n        self.fwd_mask_info = fwd_mask_info",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "get_kernel_name",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "def get_kernel_name(\n        block_metadata: Mapping[str, Any],\n        is_mqa: bool,\n        save_residuals: bool,\n        is_segmented: bool,\n        phase: str,\n) -> str:\n    \"\"\"Returns a unique name for all SplashAttention kernel variants.\"\"\"\n    assert phase == \"dq\" or phase == \"dkv\" or phase == \"fwd\"\n    # Saving residuals is supported only for the fwd phase.",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "attention_reference",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "def attention_reference(\n        mask: jax.Array,  # [q_seq_len, kv_seq_len]\n        q: jax.Array,  # [q_seq_len, head_dim]\n        k: jax.Array,  # [kv_seq_len, head_dim]\n        v: jax.Array,  # [kv_seq_len, head_dim]\n        segment_ids: SegmentIds | None,\n        *,\n        mask_value: float = DEFAULT_MASK_VALUE,\n        save_residuals: bool = False,\n        custom_type: str = \"flash\",",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "attention_reference_custom",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "def attention_reference_custom(\n        mask: jax.Array,  # [q_seq_len, kv_seq_len]\n        q: jax.Array,  # [q_seq_len, head_dim]\n        k: jax.Array,  # [kv_seq_len, head_dim]\n        v: jax.Array,  # [kv_seq_len, head_dim]\n        segment_ids: SegmentIds | None,\n        *,\n        mask_value: float = DEFAULT_MASK_VALUE,\n        save_residuals: bool = False,\n        custom_type: str = \"flash\",",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "make_attention_reference",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "def make_attention_reference(\n        mask: mask_lib.Mask | np.ndarray,\n        is_mqa: bool,\n        backward_impl: str = \"vanilla\",\n        **params: Any,\n) -> Callable:\n    @partial(\n        jax.jit,\n        static_argnames=[\n            \"mask_value\",",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "from_head_minor",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "def from_head_minor(vals: tuple[Any, ...], layout: QKVLayout):\n    if layout == QKVLayout.HEAD_DIM_MINOR:\n        return vals\n    return (*vals[:-2], vals[-1], vals[-2])\n@dataclasses.dataclass(unsafe_hash=True)\nclass BlockSizes:\n    \"\"\"Tile sizes parameterizing SplashAttention kernels.\n    Those parameters have negligible effect on numerics, but affect performance\n    greatly.\n    Note that changing the layouts only influences the physical layout that the",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "flash_attention_kernel",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "def flash_attention_kernel(\n        # Prefetched inputs\n        data_next_ref,\n        block_mask_ref,\n        mask_next_ref,\n        # Inputs\n        q_ref,\n        k_ref,\n        v_ref,\n        q_segment_ids_ref,",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "partial",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "partial = functools.partial\nDEFAULT_MASK_VALUE = -0.7 * float(np.finfo(np.dtype(\"float32\")).max)\nNUM_LANES = 128\nNUM_SUBLANES = 8\n# We predefine some useful dimension numbers for dot_general\nNN_DIM_NUMBERS = (((1,), (0,)), ((), ()))  # standard matmul\nNT_DIM_NUMBERS = (((1,), (1,)), ((), ()))  # RHS transposed\n# mypy: ignore-errors\nclass SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MASK_VALUE",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "DEFAULT_MASK_VALUE = -0.7 * float(np.finfo(np.dtype(\"float32\")).max)\nNUM_LANES = 128\nNUM_SUBLANES = 8\n# We predefine some useful dimension numbers for dot_general\nNN_DIM_NUMBERS = (((1,), (0,)), ((), ()))  # standard matmul\nNT_DIM_NUMBERS = (((1,), (1,)), ((), ()))  # RHS transposed\n# mypy: ignore-errors\nclass SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n    SegmentIds are a mechanims to ensure that there is no cross-attention between",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "NUM_LANES",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "NUM_LANES = 128\nNUM_SUBLANES = 8\n# We predefine some useful dimension numbers for dot_general\nNN_DIM_NUMBERS = (((1,), (0,)), ((), ()))  # standard matmul\nNT_DIM_NUMBERS = (((1,), (1,)), ((), ()))  # RHS transposed\n# mypy: ignore-errors\nclass SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n    SegmentIds are a mechanims to ensure that there is no cross-attention between\n    segments (fraction of a sequence) that have been concatenated together into a",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "NUM_SUBLANES",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "NUM_SUBLANES = 8\n# We predefine some useful dimension numbers for dot_general\nNN_DIM_NUMBERS = (((1,), (0,)), ((), ()))  # standard matmul\nNT_DIM_NUMBERS = (((1,), (1,)), ((), ()))  # RHS transposed\n# mypy: ignore-errors\nclass SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n    SegmentIds are a mechanims to ensure that there is no cross-attention between\n    segments (fraction of a sequence) that have been concatenated together into a\n    sequence. Each array is a list of ids (integers). Only tokens with the same",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "NN_DIM_NUMBERS",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "NN_DIM_NUMBERS = (((1,), (0,)), ((), ()))  # standard matmul\nNT_DIM_NUMBERS = (((1,), (1,)), ((), ()))  # RHS transposed\n# mypy: ignore-errors\nclass SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n    SegmentIds are a mechanims to ensure that there is no cross-attention between\n    segments (fraction of a sequence) that have been concatenated together into a\n    sequence. Each array is a list of ids (integers). Only tokens with the same\n    id are allowed to attend to each other.\n    The static mask (e.g. causal) is \"and-ed\" with the segment id mask to form",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "NT_DIM_NUMBERS",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "NT_DIM_NUMBERS = (((1,), (1,)), ((), ()))  # RHS transposed\n# mypy: ignore-errors\nclass SegmentIds(NamedTuple):\n    \"\"\"SegmentIds for Q and KV sequences.\n    SegmentIds are a mechanims to ensure that there is no cross-attention between\n    segments (fraction of a sequence) that have been concatenated together into a\n    sequence. Each array is a list of ids (integers). Only tokens with the same\n    id are allowed to attend to each other.\n    The static mask (e.g. causal) is \"and-ed\" with the segment id mask to form\n    the actual attention mask. It is important that the latter does not have any",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "SplashCustomReturnType",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "SplashCustomReturnType = Union[\n    # out, no residuals\n    jax.Array,\n        # out, residuals:\n    tuple[jax.Array, tuple[jax.Array,]]\n]\nSplashResidualsType = tuple[\n    jax.Array,  # q\n    jax.Array,  # k\n    jax.Array,  # v",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "SplashResidualsType",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "SplashResidualsType = tuple[\n    jax.Array,  # q\n    jax.Array,  # k\n    jax.Array,  # v\n    Optional[SegmentIds],  # segment_ids\n    jax.Array,  # out\n    jax.Array,  # logsumexp\n    Optional[mask_info_lib.MaskInfo],  # dq_mask_info\n    Optional[mask_info_lib.MaskInfo],  # dkv_mask_info\n]",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "MaskFunctionType",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "MaskFunctionType = Callable[..., jax.Array]\ndef get_kernel_name(\n        block_metadata: Mapping[str, Any],\n        is_mqa: bool,\n        save_residuals: bool,\n        is_segmented: bool,\n        phase: str,\n) -> str:\n    \"\"\"Returns a unique name for all SplashAttention kernel variants.\"\"\"\n    assert phase == \"dq\" or phase == \"dkv\" or phase == \"fwd\"",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "_attention_reference_custom",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "_attention_reference_custom = jax.custom_vjp(\n    _attention_reference, nondiff_argnums=(5, 6, 7, 8)\n)\n_attention_reference_custom.defvjp(_attention_reference_custom_fwd,\n                                   _attention_reference_custom_bwd)\ndef attention_reference_custom(\n        mask: jax.Array,  # [q_seq_len, kv_seq_len]\n        q: jax.Array,  # [q_seq_len, head_dim]\n        k: jax.Array,  # [kv_seq_len, head_dim]\n        v: jax.Array,  # [kv_seq_len, head_dim]",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "make_masked_mha_reference",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "make_masked_mha_reference = partial(make_attention_reference, is_mqa=False)\nmake_masked_mqa_reference = partial(make_attention_reference, is_mqa=True)\n# Splash attention implementation\n# We use an IntEnum to make it JSON serializable as regen metadata.\nclass QKVLayout(enum.IntEnum):\n    HEAD_DIM_MINOR = enum.auto()  # [..., seq_len, head_dim]\n    SEQ_MINOR = enum.auto()  # [..., head_dim, seq_len]\ndef from_head_minor(vals: tuple[Any, ...], layout: QKVLayout):\n    if layout == QKVLayout.HEAD_DIM_MINOR:\n        return vals",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "make_masked_mqa_reference",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "make_masked_mqa_reference = partial(make_attention_reference, is_mqa=True)\n# Splash attention implementation\n# We use an IntEnum to make it JSON serializable as regen metadata.\nclass QKVLayout(enum.IntEnum):\n    HEAD_DIM_MINOR = enum.auto()  # [..., seq_len, head_dim]\n    SEQ_MINOR = enum.auto()  # [..., head_dim, seq_len]\ndef from_head_minor(vals: tuple[Any, ...], layout: QKVLayout):\n    if layout == QKVLayout.HEAD_DIM_MINOR:\n        return vals\n    return (*vals[:-2], vals[-1], vals[-2])",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "make_splash_mha",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "make_splash_mha = partial(_make_splash_attention, is_mqa=False)\nmake_splash_mqa = partial(_make_splash_attention, is_mqa=True)\nmake_splash_mha_single_device = partial(\n    make_splash_mha, is_mqa=False, head_shards=1, q_seq_shards=1\n)\nmake_splash_mqa_single_device = partial(\n    make_splash_mha, is_mqa=True, head_shards=1, q_seq_shards=1\n)",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "make_splash_mqa",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "make_splash_mqa = partial(_make_splash_attention, is_mqa=True)\nmake_splash_mha_single_device = partial(\n    make_splash_mha, is_mqa=False, head_shards=1, q_seq_shards=1\n)\nmake_splash_mqa_single_device = partial(\n    make_splash_mha, is_mqa=True, head_shards=1, q_seq_shards=1\n)",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "make_splash_mha_single_device",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "make_splash_mha_single_device = partial(\n    make_splash_mha, is_mqa=False, head_shards=1, q_seq_shards=1\n)\nmake_splash_mqa_single_device = partial(\n    make_splash_mha, is_mqa=True, head_shards=1, q_seq_shards=1\n)",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "make_splash_mqa_single_device",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "peekOfCode": "make_splash_mqa_single_device = partial(\n    make_splash_mha, is_mqa=True, head_shards=1, q_seq_shards=1\n)",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_kernel",
        "documentation": {}
    },
    {
        "label": "Mask",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "peekOfCode": "class Mask:\n    \"\"\"A base class for splash attention masks.\"\"\"\n    @property\n    def shape(self) -> Tuple[int, ...]:\n        raise NotImplementedError\n    def __getitem__(self, idx) -> np.ndarray:\n        raise NotImplementedError\n    def __bool__(self) -> bool:\n        raise NotImplementedError(\n            'Conversion to bool is unsupported. Could be caused by using logical'",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "documentation": {}
    },
    {
        "label": "LogicalOr",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "peekOfCode": "class LogicalOr(Mask):\n    left: Mask\n    right: Mask\n    def __init__(self, left: Mask, right: Mask):\n        if left.shape != right.shape:\n            raise ValueError('Masks must have the same shape')\n        self.left = left\n        self.right = right\n    @property\n    def shape(self) -> Tuple[int, ...]:",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "documentation": {}
    },
    {
        "label": "LogicalAnd",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "peekOfCode": "class LogicalAnd(Mask):\n    left: Mask\n    right: Mask\n    def __init__(self, left: Mask, right: Mask):\n        if left.shape != right.shape:\n            raise ValueError('Masks must have the same shape')\n        self.left = left\n        self.right = right\n    @property\n    def shape(self) -> Tuple[int, ...]:",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "documentation": {}
    },
    {
        "label": "MultiHeadMask",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "peekOfCode": "class MultiHeadMask(Mask):\n    \"\"\"Lazy multihead mask, combines multiple lazy masks one per head.\"\"\"\n    masks: Sequence[Mask]\n    def __post_init__(self):\n        if not self.masks:\n            raise ValueError('Unsupported empty tuple of masks')\n        shape = self.masks[0].shape\n        for mask in self.masks[1:]:\n            if shape != mask.shape:\n                raise ValueError(",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "documentation": {}
    },
    {
        "label": "_ComputableMask",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "peekOfCode": "class _ComputableMask(Mask):\n    \"\"\"Superclass for all masks that can be computed inside the kernel using a callable object.\n    Attributes:\n      _shape: Shape of the 2-dim mask: (q_seq_len, kv_seq_len).\n      offset: Offset of q start wrt kv. A positive offset shifts the bottom\n        triangle upward, a negative one shifts it downward. A negative offset\n        makes the first 'offset' rows of the attention matrix all 0s which leads\n        to undefined softmax.\n      q_sequence: Indices of Q sequence.\n        q_sequence is reused across __getitem__ calls which is important for",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "documentation": {}
    },
    {
        "label": "CausalMask",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "peekOfCode": "class CausalMask(_ComputableMask):\n    \"\"\"Lazy causal mask, prevents the model from attending to future tokens.\n    Attributes:\n      offset: Offset of q start wrt kv. A positive offset shifts the bottom\n        triangle upward, a negative one shifts it downward. A negative offset\n        makes the first 'offset' rows of the attention matrix all 0s which leads\n        to undefined softmax.\n    \"\"\"\n    offset: int\n    def __init__(",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "documentation": {}
    },
    {
        "label": "LocalMask",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "peekOfCode": "class LocalMask(Mask):\n    \"\"\"Lazy local mask, prevents model from attending to tokens outside window.\n    Attributes:\n      _shape: Shape of the 2-dim mask: (q_seq_len, kv_seq_len).\n      window_size: Size of the two sides of the local window (None identifes no\n        limit for the given side).\n      offset: Offset of q start wrt kv. A positive offset shifts the bottom\n        triangle upward, a negative one shifts it downward. A negative offset\n        makes the first 'offset' rows of the attention matrix all 0s which leads\n        to undefined softmax.",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "documentation": {}
    },
    {
        "label": "NumpyMask",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "peekOfCode": "class NumpyMask(Mask):\n    \"\"\"A mask backed by a dense numpy array.\"\"\"\n    array: np.ndarray\n    def __post_init__(self):\n        if self.array.ndim != 2:\n            raise ValueError('Expected a 2-dim array')\n        if self.array.dtype != np.bool_:\n            raise ValueError('Mask must be a boolean array')\n    @property\n    def shape(self) -> Tuple[int, ...]:",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "documentation": {}
    },
    {
        "label": "FullMask",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "peekOfCode": "class FullMask(Mask):\n    \"\"\"Lazy full mask, allows all tokens to attend to all other tokens.\"\"\"\n    _shape: tuple[int, int]\n    def __post_init__(self):\n        if not isinstance(self.shape, tuple):\n            raise ValueError(f'Unsupported shape type: {type(self.shape)}')\n    @property\n    def shape(self) -> Tuple[int, ...]:\n        return self._shape\n    def __getitem__(self, idx) -> np.ndarray:",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "documentation": {}
    },
    {
        "label": "make_causal_mask",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "peekOfCode": "def make_causal_mask(shape: Tuple[int, int], offset: int = 0) -> np.ndarray:\n    \"\"\"Makes a causal attention mask.\n    Args:\n      shape: Shape of the 2-dim mask: (q_seq_len, kv_seq_len).\n      offset: Offset of q start wrt kv. A positive offset shifts the bottom\n        triangle upward, a negative one shifts it downward. A negative offset\n        makes the first 'offset' rows of the attention matrix all 0s which leads\n        to undefined softmax.\n    Returns:\n      The causal mask.",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "documentation": {}
    },
    {
        "label": "make_local_attention_mask",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "peekOfCode": "def make_local_attention_mask(\n        shape: Tuple[int, int],\n        window_size: Tuple[int | None, int | None],\n        *,\n        offset: int = 0,\n) -> np.ndarray:\n    \"\"\"Makes a local attention mask.\"\"\"\n    q_seq_len, kv_seq_len = shape\n    q_idx = np.arange(q_seq_len, dtype=np.int32)\n    kv_idx = np.arange(kv_seq_len, dtype=np.int32)",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "documentation": {}
    },
    {
        "label": "make_random_mask",
        "kind": 2,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "peekOfCode": "def make_random_mask(\n        shape: Tuple[int, int], sparsity: float, seed: int\n) -> np.ndarray:\n    \"\"\"Makes a random attention mask.\"\"\"\n    np.random.seed(seed)\n    return np.random.binomial(n=1, p=1.0 - sparsity, size=shape).astype(np.bool_)\n@dataclasses.dataclass\nclass LogicalOr(Mask):\n    left: Mask\n    right: Mask",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask",
        "documentation": {}
    },
    {
        "label": "MaskInfo",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask_info",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask_info",
        "peekOfCode": "class MaskInfo(NamedTuple):\n    \"\"\"Contains runtime masking information for the Splash attention kernel.\n    The arrays data_next, mask_next and block_mask are placed in TPU\n    scalar-memory. This is a scarse resource so the mask creation logic attempts\n    to shrink the data-type of these arrays to the smallest possible one.\n    This can be: np.int32, np.int16 or np.int8.\n    For the arrays data_next, mask_next and block_mask the size of the first\n    dimension can be one of the two following values: num_head or\n    num_head_shards.\n    The first dimension has size:",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask_info",
        "documentation": {}
    },
    {
        "label": "_HashableNDArray",
        "kind": 6,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask_info",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask_info",
        "peekOfCode": "class _HashableNDArray:\n    \"\"\"Helper to make a numpy array hashable: can be added associative containers.\n    Attributes:\n      array: The underlying numpy array.\n    \"\"\"\n    array: np.ndarray\n    def __init__(self, array: np.ndarray):\n        self.array = array\n    def __hash__(self):\n        return hash(self.array.tobytes())",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask_info",
        "documentation": {}
    },
    {
        "label": "process_mask",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask_info",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask_info",
        "peekOfCode": "process_mask = functools.partial(_process_mask, is_dkv=False)\nprocess_mask_dkv = functools.partial(_process_mask, is_dkv=True)",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask_info",
        "documentation": {}
    },
    {
        "label": "process_mask_dkv",
        "kind": 5,
        "importPath": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask_info",
        "description": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask_info",
        "peekOfCode": "process_mask_dkv = functools.partial(_process_mask, is_dkv=True)",
        "detail": "src.fjformer.pallas_operations.tpu.splash_attention.splash_attention_mask_info",
        "documentation": {}
    },
    {
        "label": "make_shard_and_gather_fns",
        "kind": 2,
        "importPath": "src.fjformer.sharding.sharding",
        "description": "src.fjformer.sharding.sharding",
        "peekOfCode": "def make_shard_and_gather_fns(\n        partition_specs: dict[str, PartitionSpec],\n        mesh: Optional[jax.sharding.Mesh] = None\n) -> Tuple[dict[Callable], dict[Callable]]:\n    \"\"\"\n    Create shard and gather functions based on given partition specs and mesh.\n    This function generates dictionaries of shard and gather functions that can be used\n    to distribute and collect arrays across a JAX mesh. The functions are specifically\n    designed for use with Flax's `jax.tree_map`.\n    Args:",
        "detail": "src.fjformer.sharding.sharding",
        "documentation": {}
    },
    {
        "label": "get_jax_mesh",
        "kind": 2,
        "importPath": "src.fjformer.sharding.sharding",
        "description": "src.fjformer.sharding.sharding",
        "peekOfCode": "def get_jax_mesh(axis_dims: str, names: Sequence[str]) -> jax.sharding.Mesh:\n    \"\"\"\n    Create a JAX mesh based on axis dimensions and names.\n    Args:\n        axis_dims: A comma-separated string specifying the dimensions of the mesh, e.g., \"1,2,4\".\n                   A \"!\" prefix indicates mesh axis splitting.\n        names: A sequence of names for the mesh axes, e.g., [\"data\", \"model\"].\n    Returns:\n        A JAX mesh object.\n    \"\"\"",
        "detail": "src.fjformer.sharding.sharding",
        "documentation": {}
    },
    {
        "label": "names_in_current_mesh",
        "kind": 2,
        "importPath": "src.fjformer.sharding.sharding",
        "description": "src.fjformer.sharding.sharding",
        "peekOfCode": "def names_in_current_mesh(*names: str) -> bool:\n    \"\"\"\n    Check if the given names are present in the current JAX mesh.\n    Args:\n        *names: Variable number of axis names to check.\n    Returns:\n        True if all given names are present in the current mesh, False otherwise.\n    \"\"\"\n    mesh_axis_names = pxla.thread_resources.env.physical_mesh.axis_names\n    return set(names) <= set(mesh_axis_names)",
        "detail": "src.fjformer.sharding.sharding",
        "documentation": {}
    },
    {
        "label": "get_names_from_partition_spec",
        "kind": 2,
        "importPath": "src.fjformer.sharding.sharding",
        "description": "src.fjformer.sharding.sharding",
        "peekOfCode": "def get_names_from_partition_spec(partition_specs: dict[str, PartitionSpec]) -> list[str]:\n    \"\"\"\n    Extract axis names from a partition specification.\n    This function recursively iterates through the provided `partition_specs`\n    dictionary and extracts all unique axis names used in the sharding specifications.\n    Args:\n        partition_specs: A dictionary mapping parameter names to their respective `PartitionSpec`.\n    Returns:\n        A list of unique axis names used in the partition specs.\n    \"\"\"",
        "detail": "src.fjformer.sharding.sharding",
        "documentation": {}
    },
    {
        "label": "with_sharding_constraint",
        "kind": 2,
        "importPath": "src.fjformer.sharding.sharding",
        "description": "src.fjformer.sharding.sharding",
        "peekOfCode": "def with_sharding_constraint(x: jnp.ndarray, partition_specs: dict[str, PartitionSpec]) -> jnp.ndarray:\n    \"\"\"\n    Apply sharding constraints if axis names are present in the current mesh.\n    This is a smarter version of `jax.lax.with_sharding_constraint`. It only applies the\n    sharding constraint if all the axis names specified in the `partition_specs` are\n    present in the current JAX mesh.\n    Args:\n        x: The JAX array to apply sharding constraints to.\n        partition_specs: A dictionary mapping parameter names to their respective `PartitionSpec`.\n    Returns:",
        "detail": "src.fjformer.sharding.sharding",
        "documentation": {}
    },
    {
        "label": "wrap_function_with_rng",
        "kind": 2,
        "importPath": "src.fjformer.sharding.sharding",
        "description": "src.fjformer.sharding.sharding",
        "peekOfCode": "def wrap_function_with_rng(rng: jnp.ndarray) -> Callable:\n    \"\"\"\n    Wrap a function to automatically manage RNG splitting.\n    This decorator simplifies the use of RNGs within functions by handling the\n    splitting of the RNG key. When the wrapped function is called, it splits\n    the provided RNG key, passes the split key to the original function, and\n    updates the RNG state.\n    Args:\n        rng: The initial JAX RNG key.\n    Returns:",
        "detail": "src.fjformer.sharding.sharding",
        "documentation": {}
    },
    {
        "label": "get_metrics",
        "kind": 2,
        "importPath": "src.fjformer.sharding.sharding",
        "description": "src.fjformer.sharding.sharding",
        "peekOfCode": "def get_metrics(metrics: dict, unreplicate: bool = False, stack: bool = False) -> dict:\n    \"\"\"\n    Process and aggregate metrics.\n    Args:\n        metrics: A dictionary of metrics, potentially replicated across devices.\n        unreplicate: If True, unreplicate the metrics before processing.\n        stack: If True, stack the metrics along a new axis.\n    Returns:\n        A dictionary of processed metrics.\n    \"\"\"",
        "detail": "src.fjformer.sharding.sharding",
        "documentation": {}
    },
    {
        "label": "tree_path_to_string",
        "kind": 2,
        "importPath": "src.fjformer.sharding.sharding",
        "description": "src.fjformer.sharding.sharding",
        "peekOfCode": "def tree_path_to_string(path: tuple, sep: Optional[str] = None) -> str:\n    \"\"\"\n    Convert a JAX tree path to a string representation.\n    Args:\n        path: The JAX tree path tuple.\n        sep: Separator to use when joining path elements.\n    Returns:\n        The string representation of the path.\n    \"\"\"\n    keys = []",
        "detail": "src.fjformer.sharding.sharding",
        "documentation": {}
    },
    {
        "label": "flatten_tree",
        "kind": 2,
        "importPath": "src.fjformer.sharding.sharding",
        "description": "src.fjformer.sharding.sharding",
        "peekOfCode": "def flatten_tree(xs: dict, is_leaf: Optional[Callable] = None, sep: Optional[str] = None) -> dict:\n    \"\"\"\n    Flatten a JAX tree and convert paths to strings.\n    Args:\n        xs: The JAX tree to flatten.\n        is_leaf: Optional function to determine leaf nodes.\n        sep: Separator to use when joining path elements.\n    Returns:\n        A flattened dictionary with string keys representing the tree paths.\n    \"\"\"",
        "detail": "src.fjformer.sharding.sharding",
        "documentation": {}
    },
    {
        "label": "named_tree_map",
        "kind": 2,
        "importPath": "src.fjformer.sharding.sharding",
        "description": "src.fjformer.sharding.sharding",
        "peekOfCode": "def named_tree_map(\n        f: Callable,\n        tree: dict,\n        *rest,\n        is_leaf: Optional[Callable] = None,\n        sep: Optional[str] = None\n):\n    \"\"\"\n    An extended version of `jax.tree_util.tree_map`.\n    This function extends `jax.tree_util.tree_map` by providing the path",
        "detail": "src.fjformer.sharding.sharding",
        "documentation": {}
    },
    {
        "label": "match_partition_rules",
        "kind": 2,
        "importPath": "src.fjformer.sharding.sharding",
        "description": "src.fjformer.sharding.sharding",
        "peekOfCode": "def match_partition_rules(rules: list[Tuple[str, PartitionSpec]], params: dict) -> dict:\n    \"\"\"\n    Match partition rules to parameters based on their names.\n    This function takes a list of partition rules (regular expressions and\n    corresponding `PartitionSpec`) and applies them to a dictionary of parameters\n    based on their names. It's useful for automatically defining sharding strategies.\n    Args:\n        rules: A list of tuples, where each tuple contains:\n               - A regular expression to match parameter names.\n               - A `PartitionSpec` to apply if the name matches.",
        "detail": "src.fjformer.sharding.sharding",
        "documentation": {}
    },
    {
        "label": "get_weight_decay_mask",
        "kind": 2,
        "importPath": "src.fjformer.sharding.sharding",
        "description": "src.fjformer.sharding.sharding",
        "peekOfCode": "def get_weight_decay_mask(exclusions: list[str]) -> Callable:\n    \"\"\"\n    Create a weight decay mask function based on exclusion rules.\n    Args:\n        exclusions: A list of regular expressions defining parameter names\n                   to exclude from weight decay.\n    Returns:\n        A function that takes a parameter dictionary and returns a mask\n        (a PyTree with the same structure as the parameters) indicating\n        which parameters should be subject to weight decay.",
        "detail": "src.fjformer.sharding.sharding",
        "documentation": {}
    },
    {
        "label": "tree_apply",
        "kind": 2,
        "importPath": "src.fjformer.sharding.sharding",
        "description": "src.fjformer.sharding.sharding",
        "peekOfCode": "def tree_apply(fns: dict, tree: dict):\n    \"\"\"\n    Apply a dictionary of functions to a corresponding PyTree.\n    Args:\n        fns: A dictionary where keys match the PyTree structure and values are functions.\n        tree: The PyTree to apply functions to.\n    Returns:\n        A new PyTree with the same structure as `tree`, but with values modified by the functions in `fns`.\n    \"\"\"\n    return jax.tree_util.tree_map(lambda fn, x: fn(x), fns, tree)",
        "detail": "src.fjformer.sharding.sharding",
        "documentation": {}
    },
    {
        "label": "create_mesh",
        "kind": 2,
        "importPath": "src.fjformer.sharding.sharding",
        "description": "src.fjformer.sharding.sharding",
        "peekOfCode": "def create_mesh(\n        axis_dims: Sequence[int] = (1, -1, 1, 1),\n        axis_names: Sequence[str] = (\"dp\", \"fsdp\", \"tp\", \"sp\"),\n        backend: str = \"\"\n) -> jax.sharding.Mesh:\n    \"\"\"\n    Create a JAX mesh with specified dimensions and names.\n    Args:\n        axis_dims: A sequence of integers representing the size of each mesh dimension.\n                   A dimension of -1 indicates that it should be inferred automatically.",
        "detail": "src.fjformer.sharding.sharding",
        "documentation": {}
    },
    {
        "label": "AxisNames",
        "kind": 6,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "class AxisNames(tuple):\n    \"\"\"Tuple of strings specifying name for each axis.\n    We create a separate class for this so JAX's pytree utilities can distinguish\n    it from a tuple that should be treated as a pytree, instead treating it as a\n    leaf.\n    \"\"\"\n    def __new__(cls, *names):\n        return tuple.__new__(AxisNames, names)\n    def __repr__(self):\n        return \"AxisNames%s\" % tuple.__repr__(self)",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "LocalChunkInfo",
        "kind": 6,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "class LocalChunkInfo:\n    # The logical slice of an array located on this host's local devices.\n    slice: Tuple[slice, ...]\n    # A unique index for this host/local chunk among chunks with the same slice.\n    replica_id: int\nclass LocalChunker:\n    \"\"\"Utility class to aid chunking of sharded arrays in multihost settings.\"\"\"\n    def __init__(self, global_mesh: Mesh):\n        self.global_mesh = global_mesh\n        local_mesh = global_mesh.local_mesh",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "LocalChunker",
        "kind": 6,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "class LocalChunker:\n    \"\"\"Utility class to aid chunking of sharded arrays in multihost settings.\"\"\"\n    def __init__(self, global_mesh: Mesh):\n        self.global_mesh = global_mesh\n        local_mesh = global_mesh.local_mesh\n        first_local_device = local_mesh.devices.reshape(-1)[0]\n        host_location = collections.OrderedDict(\n            zip(\n                global_mesh.shape.keys(),\n                list(zip(*np.nonzero(global_mesh.devices == first_local_device)))[0],",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "DataLayout",
        "kind": 6,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "class DataLayout:\n    \"\"\"Represents data layout for the partitioned model.\"\"\"\n    batch_size: int\n    shard_id: int\n    num_shards: int\n    is_first_host_in_replica_set: bool\nPartitionedCallable = Callable[..., Any]\nCompiledPartitionedCallable = Callable[..., Any]\nclass BasePartitioner(metaclass=abc.ABCMeta):\n    \"\"\"Interface for partitioning computations across hardware devices.\"\"\"",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "BasePartitioner",
        "kind": 6,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "class BasePartitioner(metaclass=abc.ABCMeta):\n    \"\"\"Interface for partitioning computations across hardware devices.\"\"\"\n    def __init__(\n        self,\n        num_partitions: Optional[int] = None,\n        model_parallel_submesh: Optional[HardwareMesh] = None,\n        params_on_devices: bool = True,\n        backend: Optional[str] = None,\n        ici_mesh_shape: Optional[HardwareMesh] = None,\n        dcn_mesh_shape: Optional[HardwareMesh] = None,",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "PjittedFnWithContext",
        "kind": 6,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "class PjittedFnWithContext(PartitionedCallable):\n    \"\"\"Wraps pjitted function to apply the appropriate contexts.\"\"\"\n    def __init__(\n        self,\n        pjitted_fn,\n        partition_mesh: Mesh,\n        logical_axis_rules: flax_partitioning.LogicalRules = (),\n    ):\n        self._pjitted_fn = pjitted_fn\n        self._mesh = partition_mesh",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "BasePjitPartitioner",
        "kind": 6,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "class BasePjitPartitioner(BasePartitioner):\n    \"\"\"Partitioner that uses T5X version of jax.pjit.\"\"\"\n    @cached_property\n    def _local_chunker(self) -> LocalChunker:\n        return LocalChunker(self.mesh)\n    @cached_property\n    def mesh(self) -> Mesh:\n        return default_mesh(\n            self._num_partitions,\n            self._model_parallel_submesh,",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "PjitPartitioner",
        "kind": 6,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "class PjitPartitioner(BasePjitPartitioner):\n    \"\"\"Partitioner that uses named axes and jax.pjit.\"\"\"\n    def __init__(\n        self,\n        num_partitions: Optional[int] = None,\n        model_parallel_submesh: Optional[HardwareMesh] = None,\n        params_on_devices: bool = True,\n        backend: Optional[str] = None,\n        ici_mesh_shape: Optional[HardwareMesh] = None,\n        dcn_mesh_shape: Optional[HardwareMesh] = None,",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "with_sharding_constraint",
        "kind": 2,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "def with_sharding_constraint(x, axis_resources):\n    \"\"\"Wrapper for lax.with_sharding_constraint, no-op on cpu or outside pjit.\"\"\"\n    if jax.devices()[0].platform == \"cpu\" or not global_mesh_defined():\n        return x\n    else:\n        return jax.lax.with_sharding_constraint(x, axis_resources)\ndef bounds_from_last_device(last_device: jax.Device) -> HardwareMesh:\n    \"\"\"Get the bound from the given last device.\"\"\"\n    # Must be passed the device at the highest-coordinate corner of the\n    # relevant mesh, which is a requirement we know is satisfied by the last",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "bounds_from_last_device",
        "kind": 2,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "def bounds_from_last_device(last_device: jax.Device) -> HardwareMesh:\n    \"\"\"Get the bound from the given last device.\"\"\"\n    # Must be passed the device at the highest-coordinate corner of the\n    # relevant mesh, which is a requirement we know is satisfied by the last\n    # device in jax.devices().\n    if hasattr(last_device, \"coords\"):\n        x, y, z = last_device.coords\n        return x + 1, y + 1, z + 1, last_device.core_on_chip + 1\n    else:\n        # On non-TPU platforms, the \"mesh\" is hosts x devices per host in order",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "get_coords",
        "kind": 2,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "def get_coords(device: jax.Device) -> HardwareMesh:\n    \"\"\"Returns the coordinates of the given device.\"\"\"\n    if hasattr(device, \"coords\"):\n        return *device.coords, device.core_on_chip\n    return device.process_index, device.id % jax.local_device_count()\ndef global_mesh_defined():\n    \"\"\"Checks if global xmap/pjit mesh resource environment is defined.\"\"\"\n    maps_env = jax.experimental.maps.thread_resources.env\n    return (\n        maps_env.physical_mesh.devices.shape != ()",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "global_mesh_defined",
        "kind": 2,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "def global_mesh_defined():\n    \"\"\"Checks if global xmap/pjit mesh resource environment is defined.\"\"\"\n    maps_env = jax.experimental.maps.thread_resources.env\n    return (\n        maps_env.physical_mesh.devices.shape != ()\n    )  # pylint: disable=g-explicit-bool-comparison\ndef get_mesh(\n    model_parallel_submesh: HardwareMesh,\n    input_devices: Sequence[JaxDevice] = (),\n    input_local_devices: Sequence[JaxDevice] = (),",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "get_mesh",
        "kind": 2,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "def get_mesh(\n    model_parallel_submesh: HardwareMesh,\n    input_devices: Sequence[JaxDevice] = (),\n    input_local_devices: Sequence[JaxDevice] = (),\n    tile_by_host_if_needed: bool = True,\n    backend: Optional[str] = None,\n) -> Mesh:\n    \"\"\"Construct an xmap/pjit Mesh for the given model-parallel submesh.\n    The resulting mesh has two resource axes: 'model', with the provided submesh\n    shape, and 'data', which covers the rest of the mesh.",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "get_cpu_mesh",
        "kind": 2,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "def get_cpu_mesh() -> Mesh:\n    \"\"\"Trivial mesh for CPU Testing.\"\"\"\n    devices = np.empty((jax.process_count(), jax.local_device_count()), dtype=object)\n    for device in jax.devices():\n        devices[device.process_index, device.id % jax.local_device_count()] = device\n    return Mesh(devices, [\"data\", \"model\"])\ndef get_gpu_mesh(num_partitions: int) -> Mesh:\n    \"\"\"Mesh for GPUs that preferentially places 'model' on NVLink.\"\"\"\n    nvlink_size = jax.local_device_count()\n    dcn_size = jax.process_count()",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "get_gpu_mesh",
        "kind": 2,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "def get_gpu_mesh(num_partitions: int) -> Mesh:\n    \"\"\"Mesh for GPUs that preferentially places 'model' on NVLink.\"\"\"\n    nvlink_size = jax.local_device_count()\n    dcn_size = jax.process_count()\n    nvlink_mp = min(num_partitions, nvlink_size)\n    nvlink_dp, extra1 = divmod(nvlink_size, nvlink_mp)\n    dcn_mp, extra2 = divmod(num_partitions, nvlink_mp)\n    assert not (extra1 or extra2), (\n        \"number of partitions on GPU must be a factor\"\n        \" or multiple of the number of local devices\"",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "default_mesh",
        "kind": 2,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "def default_mesh(\n    num_partitions: int,\n    model_parallel_submesh: Optional[HardwareMesh] = None,\n    backend: Optional[str] = None,\n    ici_mesh_shape: Optional[HardwareMesh] = None,\n    dcn_mesh_shape: Optional[HardwareMesh] = None,\n) -> Mesh:\n    \"\"\"Attempt to return a default mesh for simple cases.\n    Args:\n      num_partitions: number of partitions to use, will be ignored if",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "standard_logical_axis_rules",
        "kind": 2,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "def standard_logical_axis_rules(\n    activation_partitioning_dims: int = 1,\n    parameter_partitioning_dims: int = 1,\n    additional_rules: Optional[LogicalAxisRules] = None,\n) -> LogicalAxisRules:\n    \"\"\"Default sharding rules for T5X model in terms of logical axis names.\n    Args:\n      activation_partitioning_dims: enables 2-D activation sharding when set to 2.\n      parameter_partitioning_dims: enables 2-D parameter sharding when set to 2.\n      additional_rules: additional rules (a sequence of tuples) that will be",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "host_local_array_to_global_array",
        "kind": 2,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "def host_local_array_to_global_array(arr_tree, mesh: jax.sharding.Mesh, pspecs):\n    pspecs = jax.tree_map(\n        lambda x: PartitionSpec() if x is None else x,\n        pspecs,\n        is_leaf=lambda x: x is None,\n    )\n    return multihost_utils.host_local_array_to_global_array(arr_tree, mesh, pspecs)",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "JaxDevice",
        "kind": 5,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "JaxDevice = jax.Device\nTpuMesh = Tuple[int, int, int, int]  # (x, y, z, num_cores).\nOtherMesh = Tuple[int, int]\nHardwareMesh = Union[TpuMesh, OtherMesh]\nLogicalAxisRules = Sequence[Tuple[str, Optional[str]]]\ncached_property = property\nclass AxisNames(tuple):\n    \"\"\"Tuple of strings specifying name for each axis.\n    We create a separate class for this so JAX's pytree utilities can distinguish\n    it from a tuple that should be treated as a pytree, instead treating it as a",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "TpuMesh",
        "kind": 5,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "TpuMesh = Tuple[int, int, int, int]  # (x, y, z, num_cores).\nOtherMesh = Tuple[int, int]\nHardwareMesh = Union[TpuMesh, OtherMesh]\nLogicalAxisRules = Sequence[Tuple[str, Optional[str]]]\ncached_property = property\nclass AxisNames(tuple):\n    \"\"\"Tuple of strings specifying name for each axis.\n    We create a separate class for this so JAX's pytree utilities can distinguish\n    it from a tuple that should be treated as a pytree, instead treating it as a\n    leaf.",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "OtherMesh",
        "kind": 5,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "OtherMesh = Tuple[int, int]\nHardwareMesh = Union[TpuMesh, OtherMesh]\nLogicalAxisRules = Sequence[Tuple[str, Optional[str]]]\ncached_property = property\nclass AxisNames(tuple):\n    \"\"\"Tuple of strings specifying name for each axis.\n    We create a separate class for this so JAX's pytree utilities can distinguish\n    it from a tuple that should be treated as a pytree, instead treating it as a\n    leaf.\n    \"\"\"",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "HardwareMesh",
        "kind": 5,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "HardwareMesh = Union[TpuMesh, OtherMesh]\nLogicalAxisRules = Sequence[Tuple[str, Optional[str]]]\ncached_property = property\nclass AxisNames(tuple):\n    \"\"\"Tuple of strings specifying name for each axis.\n    We create a separate class for this so JAX's pytree utilities can distinguish\n    it from a tuple that should be treated as a pytree, instead treating it as a\n    leaf.\n    \"\"\"\n    def __new__(cls, *names):",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "LogicalAxisRules",
        "kind": 5,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "LogicalAxisRules = Sequence[Tuple[str, Optional[str]]]\ncached_property = property\nclass AxisNames(tuple):\n    \"\"\"Tuple of strings specifying name for each axis.\n    We create a separate class for this so JAX's pytree utilities can distinguish\n    it from a tuple that should be treated as a pytree, instead treating it as a\n    leaf.\n    \"\"\"\n    def __new__(cls, *names):\n        return tuple.__new__(AxisNames, names)",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "cached_property",
        "kind": 5,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "cached_property = property\nclass AxisNames(tuple):\n    \"\"\"Tuple of strings specifying name for each axis.\n    We create a separate class for this so JAX's pytree utilities can distinguish\n    it from a tuple that should be treated as a pytree, instead treating it as a\n    leaf.\n    \"\"\"\n    def __new__(cls, *names):\n        return tuple.__new__(AxisNames, names)\n    def __repr__(self):",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "PartitionedCallable",
        "kind": 5,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "PartitionedCallable = Callable[..., Any]\nCompiledPartitionedCallable = Callable[..., Any]\nclass BasePartitioner(metaclass=abc.ABCMeta):\n    \"\"\"Interface for partitioning computations across hardware devices.\"\"\"\n    def __init__(\n        self,\n        num_partitions: Optional[int] = None,\n        model_parallel_submesh: Optional[HardwareMesh] = None,\n        params_on_devices: bool = True,\n        backend: Optional[str] = None,",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "CompiledPartitionedCallable",
        "kind": 5,
        "importPath": "src.fjformer.sharding.t5x_partitioning",
        "description": "src.fjformer.sharding.t5x_partitioning",
        "peekOfCode": "CompiledPartitionedCallable = Callable[..., Any]\nclass BasePartitioner(metaclass=abc.ABCMeta):\n    \"\"\"Interface for partitioning computations across hardware devices.\"\"\"\n    def __init__(\n        self,\n        num_partitions: Optional[int] = None,\n        model_parallel_submesh: Optional[HardwareMesh] = None,\n        params_on_devices: bool = True,\n        backend: Optional[str] = None,\n        ici_mesh_shape: Optional[HardwareMesh] = None,",
        "detail": "src.fjformer.sharding.t5x_partitioning",
        "documentation": {}
    },
    {
        "label": "JaxRNG",
        "kind": 6,
        "importPath": "src.fjformer.utils",
        "description": "src.fjformer.utils",
        "peekOfCode": "class JaxRNG:\n    \"\"\"A wrapper around JAX's PRNGKey that simplifies key splitting.\"\"\"\n    def __init__(self, rng: jrandom.PRNGKey):\n        \"\"\"Initializes the JaxRNG with a PRNGKey.\n        Args:\n            rng: A JAX PRNGKey.\n        \"\"\"\n        self.rng = rng\n    @classmethod\n    def from_seed(cls, seed: int) -> \"JaxRNG\":",
        "detail": "src.fjformer.utils",
        "documentation": {}
    },
    {
        "label": "GenerateRNG",
        "kind": 6,
        "importPath": "src.fjformer.utils",
        "description": "src.fjformer.utils",
        "peekOfCode": "class GenerateRNG:\n    \"\"\"An infinite generator of JAX PRNGKeys, useful for iterating over seeds.\"\"\"\n    def __init__(self, seed: int = 0):\n        \"\"\"Initializes the generator with a starting seed.\n        Args:\n            seed: The seed to use for the initial PRNGKey.\n        \"\"\"\n        self.seed = seed\n        self._rng = jrandom.PRNGKey(seed)\n    def __next__(self) -> jrandom.PRNGKey:",
        "detail": "src.fjformer.utils",
        "documentation": {}
    },
    {
        "label": "is_torch_available",
        "kind": 2,
        "importPath": "src.fjformer.utils",
        "description": "src.fjformer.utils",
        "peekOfCode": "def is_torch_available() -> bool:\n    \"\"\"Checks if PyTorch is installed.\n    Returns:\n        True if the torch module is installed, False otherwise.\n    \"\"\"\n    return importlib.util.find_spec(\"torch\") is not None\ndef count_num_params(params: flax.core.frozen_dict.FrozenDict) -> int:\n    \"\"\"Counts the number of parameters in a model.\n    Args:\n        params: A Flax FrozenDict containing the model parameters.",
        "detail": "src.fjformer.utils",
        "documentation": {}
    },
    {
        "label": "count_num_params",
        "kind": 2,
        "importPath": "src.fjformer.utils",
        "description": "src.fjformer.utils",
        "peekOfCode": "def count_num_params(params: flax.core.frozen_dict.FrozenDict) -> int:\n    \"\"\"Counts the number of parameters in a model.\n    Args:\n        params: A Flax FrozenDict containing the model parameters.\n    Returns:\n        The total number of parameters in the model.\n    \"\"\"\n    return sum(\n        i.size for i in jax.tree_util.tree_flatten(flax.core.unfreeze(params))[0]\n    )",
        "detail": "src.fjformer.utils",
        "documentation": {}
    },
    {
        "label": "count_params",
        "kind": 2,
        "importPath": "src.fjformer.utils",
        "description": "src.fjformer.utils",
        "peekOfCode": "def count_params(params: flax.core.frozen_dict.FrozenDict) -> None:\n    \"\"\"Prints the number of parameters in a Flax model.\n    Args:\n        params: A Flax FrozenDict containing the model parameters.\n    \"\"\"\n    print(\n        f\"\\033[1;31mModel Contains : {count_num_params(params) / 1e9:.2f} Billion Parameters\"\n    )\nclass JaxRNG:\n    \"\"\"A wrapper around JAX's PRNGKey that simplifies key splitting.\"\"\"",
        "detail": "src.fjformer.utils",
        "documentation": {}
    },
    {
        "label": "init_rng",
        "kind": 2,
        "importPath": "src.fjformer.utils",
        "description": "src.fjformer.utils",
        "peekOfCode": "def init_rng(seed: int) -> None:\n    \"\"\"Initializes the global JaxRNG with a seed.\n    Args:\n        seed: The seed to use for the random number generator.\n    \"\"\"\n    global jax_utils_rng\n    jax_utils_rng = JaxRNG.from_seed(seed)\ndef next_rng(\n    *args, **kwargs\n) -> Union[jrandom.PRNGKey, Tuple[jrandom.PRNGKey, ...], dict]:",
        "detail": "src.fjformer.utils",
        "documentation": {}
    },
    {
        "label": "next_rng",
        "kind": 2,
        "importPath": "src.fjformer.utils",
        "description": "src.fjformer.utils",
        "peekOfCode": "def next_rng(\n    *args, **kwargs\n) -> Union[jrandom.PRNGKey, Tuple[jrandom.PRNGKey, ...], dict]:\n    \"\"\"Provides access to the global JaxRNG and splits the key based on arguments.\n    This function wraps the global `jax_utils_rng` instance and calls its `__call__` method,\n    passing through any arguments provided. This provides a convenient way to access and\n    split the global random number generator key.\n    Args:\n        *args: Positional arguments passed to the `jax_utils_rng` instance's `__call__` method.\n        **kwargs: Keyword arguments passed to the `jax_utils_rng` instance's `__call__` method.",
        "detail": "src.fjformer.utils",
        "documentation": {}
    },
    {
        "label": "get_logger",
        "kind": 2,
        "importPath": "src.fjformer.utils",
        "description": "src.fjformer.utils",
        "peekOfCode": "def get_logger(name, level: int = logging.INFO) -> logging.Logger:\n    \"\"\"\n    Function to create and configure a logger.\n    Args:\n        name: str: The name of the logger.\n        level: int: The logging level. Defaults to logging.INFO.\n    Returns:\n        logging.Logger: The configured logger instance.\n    \"\"\"\n    logger = logging.getLogger(name)",
        "detail": "src.fjformer.utils",
        "documentation": {}
    },
    {
        "label": "set_loggers_level",
        "kind": 2,
        "importPath": "src.fjformer.utils",
        "description": "src.fjformer.utils",
        "peekOfCode": "def set_loggers_level(level: int = logging.WARNING):\n    \"\"\"Function to set the logging level of all loggers to the specified level.\n    Args:\n        level: int: The logging level to set. Defaults to\n            logging.WARNING.\n    \"\"\"\n    logging.root.setLevel(level)\n    for handler in logging.root.handlers:\n        handler.setLevel(level)",
        "detail": "src.fjformer.utils",
        "documentation": {}
    },
    {
        "label": "jax_utils_rng",
        "kind": 5,
        "importPath": "src.fjformer.utils",
        "description": "src.fjformer.utils",
        "peekOfCode": "jax_utils_rng = None\ndef init_rng(seed: int) -> None:\n    \"\"\"Initializes the global JaxRNG with a seed.\n    Args:\n        seed: The seed to use for the random number generator.\n    \"\"\"\n    global jax_utils_rng\n    jax_utils_rng = JaxRNG.from_seed(seed)\ndef next_rng(\n    *args, **kwargs",
        "detail": "src.fjformer.utils",
        "documentation": {}
    },
    {
        "label": "combine_causal_and_attention_mask",
        "kind": 2,
        "importPath": "test.attention_test",
        "description": "test.attention_test",
        "peekOfCode": "def combine_causal_and_attention_mask(q_attention_mask, kv_attention_mask):\n    assert q_attention_mask.ndim == 2\n    assert kv_attention_mask.ndim == 2\n    return jnp.bitwise_and(\n        jnp.bitwise_and(q_attention_mask.astype(\"bool\"), kv_attention_mask.astype(\"bool\")),\n        jnp.tril(jnp.ones(\n            (q_attention_mask.shape[-1], kv_attention_mask.shape[-1]), dtype=\"bool\")\n        )[None, None, :, :]\n    ).astype(\"bool\")\ndef main():",
        "detail": "test.attention_test",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test.attention_test",
        "description": "test.attention_test",
        "peekOfCode": "def main():\n    k1, k2, k3 = random.split(random.PRNGKey(3), num=3)\n    q = random.normal(k1, (batch, seq, heads, hd), \"float32\")\n    k = random.normal(k2, (batch, seq, heads, hd), \"float32\")\n    v = random.normal(k3, (batch, seq, heads, hd), \"float32\")\n    a = jnp.ones((batch, seq), dtype=\"bool\")\n    a = a.at[:, :seq // 2].set(0)\n    # a = a.at[:, seq // 2:].set(0)\n    # print(a)\n    csm = make_causal_mask(jnp.ones((batch, seq)))",
        "detail": "test.attention_test",
        "documentation": {}
    },
    {
        "label": "os.environ['JAX_TRACEBACK_FILTERING']",
        "kind": 5,
        "importPath": "test.attention_test",
        "description": "test.attention_test",
        "peekOfCode": "os.environ['JAX_TRACEBACK_FILTERING'] = 'off'\nfrom flax.linen.attention import dot_product_attention, make_attention_mask, make_causal_mask, combine_masks\nfrom src.fjformer.pallas_operations.pallas_attention import flash_attention\nfrom jax import random, numpy as jnp\nbatch = 1\nseq = 6\nheads = 32\nhd = 128\ndef combine_causal_and_attention_mask(q_attention_mask, kv_attention_mask):\n    assert q_attention_mask.ndim == 2",
        "detail": "test.attention_test",
        "documentation": {}
    },
    {
        "label": "batch",
        "kind": 5,
        "importPath": "test.attention_test",
        "description": "test.attention_test",
        "peekOfCode": "batch = 1\nseq = 6\nheads = 32\nhd = 128\ndef combine_causal_and_attention_mask(q_attention_mask, kv_attention_mask):\n    assert q_attention_mask.ndim == 2\n    assert kv_attention_mask.ndim == 2\n    return jnp.bitwise_and(\n        jnp.bitwise_and(q_attention_mask.astype(\"bool\"), kv_attention_mask.astype(\"bool\")),\n        jnp.tril(jnp.ones(",
        "detail": "test.attention_test",
        "documentation": {}
    },
    {
        "label": "seq",
        "kind": 5,
        "importPath": "test.attention_test",
        "description": "test.attention_test",
        "peekOfCode": "seq = 6\nheads = 32\nhd = 128\ndef combine_causal_and_attention_mask(q_attention_mask, kv_attention_mask):\n    assert q_attention_mask.ndim == 2\n    assert kv_attention_mask.ndim == 2\n    return jnp.bitwise_and(\n        jnp.bitwise_and(q_attention_mask.astype(\"bool\"), kv_attention_mask.astype(\"bool\")),\n        jnp.tril(jnp.ones(\n            (q_attention_mask.shape[-1], kv_attention_mask.shape[-1]), dtype=\"bool\")",
        "detail": "test.attention_test",
        "documentation": {}
    },
    {
        "label": "heads",
        "kind": 5,
        "importPath": "test.attention_test",
        "description": "test.attention_test",
        "peekOfCode": "heads = 32\nhd = 128\ndef combine_causal_and_attention_mask(q_attention_mask, kv_attention_mask):\n    assert q_attention_mask.ndim == 2\n    assert kv_attention_mask.ndim == 2\n    return jnp.bitwise_and(\n        jnp.bitwise_and(q_attention_mask.astype(\"bool\"), kv_attention_mask.astype(\"bool\")),\n        jnp.tril(jnp.ones(\n            (q_attention_mask.shape[-1], kv_attention_mask.shape[-1]), dtype=\"bool\")\n        )[None, None, :, :]",
        "detail": "test.attention_test",
        "documentation": {}
    },
    {
        "label": "hd",
        "kind": 5,
        "importPath": "test.attention_test",
        "description": "test.attention_test",
        "peekOfCode": "hd = 128\ndef combine_causal_and_attention_mask(q_attention_mask, kv_attention_mask):\n    assert q_attention_mask.ndim == 2\n    assert kv_attention_mask.ndim == 2\n    return jnp.bitwise_and(\n        jnp.bitwise_and(q_attention_mask.astype(\"bool\"), kv_attention_mask.astype(\"bool\")),\n        jnp.tril(jnp.ones(\n            (q_attention_mask.shape[-1], kv_attention_mask.shape[-1]), dtype=\"bool\")\n        )[None, None, :, :]\n    ).astype(\"bool\")",
        "detail": "test.attention_test",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test.import_check_test",
        "description": "test.import_check_test",
        "peekOfCode": "def main():\n    start_time = time.time()\n    from src import fjformer\n    from src.fjformer import checkpoint\n    from src.fjformer import functions\n    from src.fjformer import linen\n    from src.fjformer import monitor\n    from src.fjformer import optimizers\n    from src.fjformer import pallas_operations\n    from src.fjformer import sharding",
        "detail": "test.import_check_test",
        "documentation": {}
    },
    {
        "label": "DummyNet",
        "kind": 6,
        "importPath": "test.linear_bit_kernel_test",
        "description": "test.linear_bit_kernel_test",
        "peekOfCode": "class DummyNet(nn.Module):\n    @nn.compact\n    def __call__(self, x):\n        return nn.LayerNorm(\n        )(\n            nn.Dense(\n                64\n            )(\n                nn.Embed(\n                    512, 1024",
        "detail": "test.linear_bit_kernel_test",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "test.linear_bit_kernel_test",
        "description": "test.linear_bit_kernel_test",
        "peekOfCode": "def main():\n    rng_gen = GenerateRNG(42)\n    net = DummyNet()\n    params = net.init(\n        rng_gen.rng,\n        jax.random.randint(rng_gen.rng, (1, 68), minval=0, maxval=512)\n    )\n    quantized_params = nn.quantize_int8_parameters([\"kernel\", \"embedding\"], params)\n    inputs = jax.random.randint(rng_gen.rng, (1, 128), minval=0, maxval=512)\n    print(quantized_params)",
        "detail": "test.linear_bit_kernel_test",
        "documentation": {}
    },
    {
        "label": "test_compute_weighted_cross_entropy_1",
        "kind": 2,
        "importPath": "test.test_cross_ent_loss_and_acc",
        "description": "test.test_cross_ent_loss_and_acc",
        "peekOfCode": "def test_compute_weighted_cross_entropy_1():\n    \"\"\"\n    batch_size = 2\n    length = 4\n    vocab_size = 8\n    :return:\n    \"\"\"\n    label_smoothing = 0.0\n    z_loss_coeff = 0.0001\n    logits = jnp.array(",
        "detail": "test.test_cross_ent_loss_and_acc",
        "documentation": {}
    },
    {
        "label": "test_compute_weighted_cross_entropy_ignore_token_minus100",
        "kind": 2,
        "importPath": "test.test_cross_ent_loss_and_acc",
        "description": "test.test_cross_ent_loss_and_acc",
        "peekOfCode": "def test_compute_weighted_cross_entropy_ignore_token_minus100():\n    \"\"\"\n    batch_size = 2\n    length = 4\n    vocab_size = 8\n    :return:\n    \"\"\"\n    label_smoothing = 0.0\n    z_loss_coeff = 0.0000\n    logits = jnp.array(",
        "detail": "test.test_cross_ent_loss_and_acc",
        "documentation": {}
    },
    {
        "label": "test_multiple",
        "kind": 2,
        "importPath": "test.test_cross_ent_loss_and_acc",
        "description": "test.test_cross_ent_loss_and_acc",
        "peekOfCode": "def test_multiple():\n    # Logits: Unnormalized log probabilities from a model, shaped as [batch_size, num_classes].\n    # Here, batch_size=2 for two examples, and num_classes=3 for three possible classes.\n    logits = jnp.array(\n        [\n            [2.0, 1.0, 0.1],  # Logits for the first example\n            [0.1, 2.0, 1.0],  # Logits for the second example\n        ]\n    )\n    # Labels: One-hot encoded true class labels, with the same shape as logits [batch_size, num_classes].",
        "detail": "test.test_cross_ent_loss_and_acc",
        "documentation": {}
    },
    {
        "label": "QuantizedArray",
        "kind": 6,
        "importPath": "env",
        "description": "env",
        "peekOfCode": "class QuantizedArray(ImplicitArray):\n    array_quant: ArrayValue\n    scale: ArrayValue\n    min_vals: ArrayValue\n    def materialize(self):\n        return self.dequantize(\n            array_quant=self.array_quant,\n            scale=self.scale,\n            min_vals=self.min_vals,\n            float_dtype=self.dtype,",
        "detail": "env",
        "documentation": {}
    },
    {
        "label": "quantize",
        "kind": 2,
        "importPath": "env",
        "description": "env",
        "peekOfCode": "def quantize(array: Array, axis: int = -1) -> Tuple[Array, Array, Array]:\n    min_vals = jnp.min(array, axis=axis, keepdims=True)\n    max_vals = jnp.max(array, axis=axis, keepdims=True)\n    # Compute the scaling factors\n    scale = (max_vals - min_vals) / (2**7 - 1)\n    # Quantize the data\n    quantized_data = jnp.round((array - min_vals) / scale)\n    # Clip the quantized values to ensure they lie within the representable range\n    quantized_data = jnp.clip(quantized_data, 0, 2**7 - 1).astype(jnp.uint8)\n    return quantized_data, scale, min_vals",
        "detail": "env",
        "documentation": {}
    },
    {
        "label": "dequantize",
        "kind": 2,
        "importPath": "env",
        "description": "env",
        "peekOfCode": "def dequantize(\n    array_quant: Array,\n    scale: Array,\n    min_vals: Array,\n    float_dtype: jnp.dtype = jnp.float16,\n):\n    return (array_quant * scale + min_vals).astype(float_dtype)\n@dataclass\nclass QuantizedArray(ImplicitArray):\n    array_quant: ArrayValue",
        "detail": "env",
        "documentation": {}
    },
    {
        "label": "get_binop_result_shape_dtype",
        "kind": 2,
        "importPath": "env",
        "description": "env",
        "peekOfCode": "def get_binop_result_shape_dtype(a, b):\n    out_shape = jnp.broadcast_shapes(jnp.shape(a), jnp.shape(b))\n    out_dtype = jnp.result_type(a.dtype, b.dtype)\n    return out_shape, out_dtype\n@jax.jit\n@use_implicit_args\ndef f(x, y):\n    return (x + y)[0, 0]\ndef main():\n    orginal_array = jax.random.normal(jax.random.PRNGKey(0), (512, 64))",
        "detail": "env",
        "documentation": {}
    },
    {
        "label": "f",
        "kind": 2,
        "importPath": "env",
        "description": "env",
        "peekOfCode": "def f(x, y):\n    return (x + y)[0, 0]\ndef main():\n    orginal_array = jax.random.normal(jax.random.PRNGKey(0), (512, 64))\n    quantized_array = QuantizedArray.quantize(orginal_array)\n    print(f(quantized_array, jnp.ones(64)))\n    print((orginal_array + jnp.ones(64))[0, 0])\nif __name__ == \"__main__\":\n    main()",
        "detail": "env",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "env",
        "description": "env",
        "peekOfCode": "def main():\n    orginal_array = jax.random.normal(jax.random.PRNGKey(0), (512, 64))\n    quantized_array = QuantizedArray.quantize(orginal_array)\n    print(f(quantized_array, jnp.ones(64)))\n    print((orginal_array + jnp.ones(64))[0, 0])\nif __name__ == \"__main__\":\n    main()",
        "detail": "env",
        "documentation": {}
    },
    {
        "label": "flatten_dict",
        "kind": 2,
        "importPath": "generate_documents",
        "description": "generate_documents",
        "peekOfCode": "def flatten_dict(d, parent_key=\"\", sep=\"-\"):\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\ndef unflatten_dict(xs, sep=None):",
        "detail": "generate_documents",
        "documentation": {}
    },
    {
        "label": "unflatten_dict",
        "kind": 2,
        "importPath": "generate_documents",
        "description": "generate_documents",
        "peekOfCode": "def unflatten_dict(xs, sep=None):\n    assert isinstance(xs, dict), f\"input is not a dict; it is a {type(xs)}\"\n    result = {}\n    for path, value in xs.items():\n        if sep is not None:\n            path = path.split(sep)\n        cursor = result\n        for key in path[:-1]:\n            if key not in cursor:\n                cursor[key] = {}",
        "detail": "generate_documents",
        "documentation": {}
    },
    {
        "label": "get_inner",
        "kind": 2,
        "importPath": "generate_documents",
        "description": "generate_documents",
        "peekOfCode": "def get_inner(path: str):\n    return [\n        os.path.join(path, o)\n        for o in os.listdir(path)\n        if os.path.exists(os.path.join(path, o))\n    ]\ndef get_dirs(path: str):\n    return [\n        os.path.join(path, o)\n        for o in os.listdir(path)",
        "detail": "generate_documents",
        "documentation": {}
    },
    {
        "label": "get_dirs",
        "kind": 2,
        "importPath": "generate_documents",
        "description": "generate_documents",
        "peekOfCode": "def get_dirs(path: str):\n    return [\n        os.path.join(path, o)\n        for o in os.listdir(path)\n        if os.path.exists(os.path.join(path, o))\n        and os.path.isdir(os.path.join(path, o))\n    ]\ndef get_files(path: str):\n    return [\n        os.path.join(path, o)",
        "detail": "generate_documents",
        "documentation": {}
    },
    {
        "label": "get_files",
        "kind": 2,
        "importPath": "generate_documents",
        "description": "generate_documents",
        "peekOfCode": "def get_files(path: str):\n    return [\n        os.path.join(path, o)\n        for o in os.listdir(path)\n        if os.path.exists(os.path.join(path, o))\n        and not os.path.isdir(os.path.join(path, o))\n    ]\ndef run(\n    project_locations=\"src/fjformer/\",\n    start_head=\"fjformer\",",
        "detail": "generate_documents",
        "documentation": {}
    },
    {
        "label": "run",
        "kind": 2,
        "importPath": "generate_documents",
        "description": "generate_documents",
        "peekOfCode": "def run(\n    project_locations=\"src/fjformer/\",\n    start_head=\"fjformer\",\n):\n    global cache\n    try:\n        for current_file in get_inner(project_locations):\n            if (\n                not current_file.endswith(\"__init__.py\")\n                and not os.path.isdir(current_file)",
        "detail": "generate_documents",
        "documentation": {}
    },
    {
        "label": "create_rst",
        "kind": 2,
        "importPath": "generate_documents",
        "description": "generate_documents",
        "peekOfCode": "def create_rst(name, children, output_dir):\n    \"\"\"Create an .rst file with a toctree linking its children.\"\"\"\n    rst_path = os.path.join(output_dir, f\"{name}.rst\")\n    # print(rst_path)\n    if isinstance(children, dict):\n        with open(rst_path, \"w\") as rst_file:\n            rst_file.write(f\"{name.replace('_', ' ')}\\n{'=' * len(name)}\\n\\n\")\n            rst_file.write(\".. toctree::\\n   :maxdepth: 2\\n\\n\")\n            for child_name, child_value in children.items():\n                if isinstance(child_value, str):",
        "detail": "generate_documents",
        "documentation": {}
    },
    {
        "label": "generate_api_docs",
        "kind": 2,
        "importPath": "generate_documents",
        "description": "generate_documents",
        "peekOfCode": "def generate_api_docs(structure, output_dir):\n    \"\"\"Recursively generate .rst files based on the given dictionary structure.\"\"\"\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    for root_name, children in structure.items():\n        create_rst(root_name.replace(\" \", \"_\"), children, output_dir)\ndef main():\n    global cache\n    for current_file in get_inner(\"docs/api_docs/\"):\n        if current_file.startswith(\"docs/api_docs/\"):",
        "detail": "generate_documents",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "generate_documents",
        "description": "generate_documents",
        "peekOfCode": "def main():\n    global cache\n    for current_file in get_inner(\"docs/api_docs/\"):\n        if current_file.startswith(\"docs/api_docs/\"):\n            os.remove(current_file)\n            # print(\"Removed Past generated file: \" + current_file)\n    run()\n    cache = {(\"APIs\",) + k: v for k, v in cache.items()}\n    # print(cache)\n    pages = unflatten_dict(cache)",
        "detail": "generate_documents",
        "documentation": {}
    },
    {
        "label": "dirname",
        "kind": 5,
        "importPath": "generate_documents",
        "description": "generate_documents",
        "peekOfCode": "dirname = os.path.dirname(os.path.basename(__file__))\nsys.path.append(dirname)\nsys.path.append(os.path.join(dirname, \"src\"))\nstatic_joins = \"\\n\\t:members:\\n\\t:undoc-members:\\n\\t:show-inheritance:\"\ncache = {}\ndef flatten_dict(d, parent_key=\"\", sep=\"-\"):\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, dict):",
        "detail": "generate_documents",
        "documentation": {}
    },
    {
        "label": "static_joins",
        "kind": 5,
        "importPath": "generate_documents",
        "description": "generate_documents",
        "peekOfCode": "static_joins = \"\\n\\t:members:\\n\\t:undoc-members:\\n\\t:show-inheritance:\"\ncache = {}\ndef flatten_dict(d, parent_key=\"\", sep=\"-\"):\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))",
        "detail": "generate_documents",
        "documentation": {}
    },
    {
        "label": "cache",
        "kind": 5,
        "importPath": "generate_documents",
        "description": "generate_documents",
        "peekOfCode": "cache = {}\ndef flatten_dict(d, parent_key=\"\", sep=\"-\"):\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, dict):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)",
        "detail": "generate_documents",
        "documentation": {}
    }
]